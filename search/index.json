[{"content":"本文将对QAT的一些基本算法做一些介绍，然后介绍两种较为直观也较为常用的QAT算法\nQAT 首先写出原始形式的量化式： $$ s \\cdot {\\rm clamp}(\\lfloor \\frac{x}{s} \\rceil;n,p) $$ 或： $$ s \\cdot [{\\rm clamp}(\\lfloor \\frac{x}{s} \\rceil + \\lfloor z \\rceil;n,p) - \\lfloor z \\rceil] $$ STE（直通估计器）最早在Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation1中提出。由于round运算的梯度为0或未定义，STE将舍入算子的梯度近似为1：\n所以舍入值（权重和激活）相对于原值的导数可以定义为：\nLSQ 本文主要提出了两种方法：\n 提供了一种简单的方法来近似量化器步长的梯度，它对量化的状态转换很敏感，可以说在学习步长作为一个模型参数时提供了更精细的优化。 提出了一个简单的启发式方法，使步长更新的幅度与权重更新达到更好的平衡，以改善收敛性。  LSQ2这一工作引入了可学习的步长$s$:\nLSQ的巧妙之处在于，它模拟了量化过程中的一个现象：一个给定的$x_i$离量化过渡点越近，它就越有可能由于对$s$的更新改变其量化值$\\hat x_i$，导致$\\hat x_i$出现大幅度跳跃——梯度随着$x_i$到过渡点的距离减少而增加。\n在更新之前，激活和权重的初始步长值为： $$ s_{init} = \\frac{2\\bar x}{\\sqrt{p}} $$ 在训练过程中，我们期望步长参数随着精度的提高而变小（因为数据被更精细地量化），而步长更新随着量化项目的增加而变大（因为在计算其梯度时，更多的项目被加在一起）。对此我们需要将总损失与一个调节参数相乘：$g=1/\\sqrt{n p}$（$n$为每一层权重/激活的参数量）。同时将所有矩阵乘法层的输入激活和权重设置为2位、3位、4位或8位，但第一层和最后一层始终使用8位。\nLSQ+ LSQ方法大多基于以ReLU为激活函数的模型。但对于GeLU这样的函数，正负范围分布不均，无论是使用无符号量化范围量化（将所有负值钳制为0）还是使用有符号量化范围量化（对激活函数的负和正部分给予同等重视）都会造成精度损失。\nLSQ+3这一工作引入了可学习的偏置$z$:\n这样，我们就可以在激活处执行非对称量化，提高精度。\n对于权重，我们使用对称量化： $$ s_{init} = \\max(|\\mu - 3\\sigma|,|\\mu + 3\\sigma|) / 2^{b-1} $$ 对于激活，我们通过校准最小化下式： $$ s_{init}, \\beta_{init} = \\arg \\min_{s,\\beta} ||\\hat x - x||_F^2 $$\nCode 首先给出PyTorch中自定义求导算子的写法官方文档：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  class Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) return result @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output * result # Use it by calling the apply method: output = Exp.apply(input)   借助此，我们可以自定义函数的正向传播和反向传播过程。以下代码参考自45。\nSTE 首先我们给出BNN中STE（效果如下图所示）的表达式：\n1 2 3 4 5 6 7 8  class STEFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): return (input \u0026gt; 0).float() @staticmethod def backward(ctx, grad_output): return F.hardtanh(grad_output)   在真正作为一个module使用时，需要进行封装：\n1 2 3 4 5 6 7  class StraightThroughEstimator(nn.Module): def __init__(self): super(StraightThroughEstimator, self).__init__() def forward(self, x): x = STEFunction.apply(x) return x   对于一般的round函数，其STE可以做如下构造：\n1 2 3 4 5 6 7 8  class STE(torch.autograd.Function): @staticmethod def forward(ctx, x): return torch.round(x) @staticmethod def backward(ctx, grad_output): return grad_output.clone()   我们可以看下这种写法和直接使用torch.round()函数的区别：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # original print(\u0026#39;version 1\u0026#39;) x = torch.tensor([0., 100., 200.], requires_grad = True) out = x.round() loss = out.mean() loss.backward() print(x.grad) # modified print(\u0026#39;version 2\u0026#39;) x = torch.tensor([0., 100., 200.], requires_grad = True) out = STE.apply(x) loss = out.mean() loss.backward() print(x.grad)   1 2 3 4  version 1 tensor([0., 0., 0.]) version 2 tensor([0.3333, 0.3333, 0.3333])   可以看到，原生的round函数不产生任何梯度。\n此外，还有一种更精妙的写法：\n1 2 3 4  def round_pass(x): y = x.round() y_grad = x return y.detach() - y_grad.detach() + y_grad   此函数的意味是：函数的返回值为y - y_grad + y_grad = x.round()，但反向传播的梯度仍然按照y_grad = x的梯度去算。\n 按照此种写法还可以实现“梯度倍增”的效果：\n1 2 3 4  def grad_scale(x, scale): y = x y_grad = x * scale return y.detach() - y_grad.detach() + y_grad    LSQ LSQ的正向传播和反向传播在STE的基础上实现，同时还有初始化规则：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  class LsqQuan(Quantizer): def __init__(self, bit, all_positive=False, symmetric=False, per_channel=True): super().__init__(bit) if all_positive: assert not symmetric, \u0026#34;Positive quantization cannot be symmetric\u0026#34; # unsigned activation is quantized to [0, 2^b-1] self.thd_neg = 0 self.thd_pos = 2 ** bit - 1 else: if symmetric: # signed weight/activation is quantized to [-2^(b-1)+1, 2^(b-1)-1] self.thd_neg = - 2 ** (bit - 1) + 1 self.thd_pos = 2 ** (bit - 1) - 1 else: # signed weight/activation is quantized to [-2^(b-1), 2^(b-1)-1] self.thd_neg = - 2 ** (bit - 1) self.thd_pos = 2 ** (bit - 1) - 1 self.per_channel = per_channel self.s = torch.nn.Parameter(torch.ones(1)) def init_from(self, x, *args, **kwargs): if self.per_channel: self.s = torch.nn.Parameter( x.detach().abs().mean(dim=list(range(1, x.dim())), keepdim=True) * 2 / (self.thd_pos ** 0.5)) else: self.s = torch.nn.Parameter(x.detach().abs().mean() * 2 / (self.thd_pos ** 0.5)) def forward(self, x): if self.per_channel: s_grad_scale = 1.0 / ((self.thd_pos * x.numel()) ** 0.5) else: s_grad_scale = 1.0 / ((self.thd_pos * x.numel()) ** 0.5) s_scale = grad_scale(self.s, s_grad_scale) x = x / s_scale x = torch.clamp(x, self.thd_neg, self.thd_pos) x = round_pass(x) x = x * s_scale return   LSQ+同理，只不过在原有基础上引入偏移量z，并使用不同的初始化规则，此处不再详细说明。\nReference   http://arxiv.org/abs/1308.3432\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n http://arxiv.org/abs/2004.09576\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n http://arxiv.org/abs/2110.01900\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://zhuanlan.zhihu.com/p/72681647\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://github.com/zhutmost/lsq-net\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2023-01-25T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/hardware-machine-learning/qat%E4%B8%B2%E8%AE%B2%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","title":"QAT串讲和代码实现"},{"content":"Mapping systolic array为我们提供了一种很好的并行化方法，然而在实际运行中可能存在一些问题：比如systolic array的尺寸过小，无法载入整个权重等。这时就需要插入一些mapping的技巧。\n我们先贴出原版的systolic array代码：\n1 2 3 4 5 6 7 8 9 10 11  for (m = 0; m \u0026lt; M; m++) { spatial_for(n = 0; n \u0026lt; N; n++) { OA[n, m] = 0; spatial_for(k = 0; k \u0026lt; K; k++) { OA[n, m] += IA[m, k] * W[k, n]; } } }   case 1: systolic array的尺寸小于K or N\n这意味着weight无法一次性载入到整个systolic array中，以下为分块载入的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  for (m = 0; m \u0026lt; M; m++) { for (n1 = 0; n1 \u0026lt; N1; n1++) { OA [n1 * N0:(n1 + 1) * N0, m] = 0; for (k1 = 0; k1 \u0026lt; K1; k1++) { spatial_for(n0 = 0; n0 \u0026lt; N0; n0++) { spatial_for(k0 = 0; k0 \u0026lt; K0; k0++) { OA[n1 * N0 + n0, m] += IA[m, k1 * K0 + k0] * W[k1 * K0 + k0, n1 * N0 + n0]; } } } } }   这里N0*N1=N，K0*K1=K，systolic array的尺寸大小为N0*K0，取块的方式如下：\ncase 2：weight buffer小于systolic array的size，也就是说，weight不能被一次性载入到systolic array，下为代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  for (m = 0; m \u0026lt; M; m++) { // IA buffer stores: 1*K  mvin(IA [m:m + 1, 0:K]); for (n2 = 0; n2 \u0026lt; N2; n2++) { // W buffer stores: N1*N0*K \u0026lt; NK  mvin(W [0:K, n2 * N1 * N0:(n2 + 1) * N1 * N0]); OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m:m + 1] = 0; for (n1 = 0; n1 \u0026lt; N1; n1++) { for (k1 = 0; k1 \u0026lt; K1; k1++) { spatial_for(n0 = 0; n0 \u0026lt; N0; n0++) { spatial_for(k0 = 0; k0 \u0026lt; K0; k0++) { OA[n2 * N1 * N0 + n1 * N0 + n0, m] += IA[m, k1 * K1 + k0] * W[k1 * K0 + k0, n2 * N1 * N0 + n1 * N0 + n0]; } } } } mvout(OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m:m + 1]); } }    这里只考虑了weight buffer在N维度上不够的情况，即N=N0*N1*N2。映射方式如下：\n case 3：input buffer小于systolic array的size，也就是说，input不能被一次性载入到systolic array，下为代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  for (m2 = 0; m2 \u0026lt; M2; m2++) { // IA buffer stores: M1*K  mvin(IA [m2 * M1:(m2 + 1) * M1, 0:K]); for (n2 = 0; n2 \u0026lt; N2; n2++) { // W buffer stores: N1*N0*K  mvin(W [0:K, n2 * N1 * N0:(n2 + 1) * N1 * N0]); OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m2 * M1:(m2 + 1) * M1] = 0; for (m1 = 0; m1 \u0026lt; M1; m1++) { for (n1 = 0; n1 \u0026lt; N1; n1++) { for (k1 = 0; k1 \u0026lt; K1; k1++) { spatial_for(n0 = 0; n0 \u0026lt; N0; n0++) { spatial_for(k0 = 0; k0 \u0026lt; K0; k0++) { OA[n2 * N1 * N0 + n1 * N0 + n0, m2 * M1 + m1] += IA[m2 * M1 + m1, k1 * K1 + k0] * W[k1 * K0 + k0, n2 * N1 * N0 + n1 * N0 + n0]; } } } } } mvout(OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m2 * M1:(m2 + 1) * M1]); } }   解决了上述的特殊情况之后，我们还需要解决一个问题：什么样的循环写法是最优的，例如对于case 3的代码，我们可以有如下两种写法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  for (m2 = 0; m2 \u0026lt; M2; m2++) { // IA buffer stores: M1*K  mvin(IA [m2 * M1:(m2 + 1) * M1, 0:K]); for (n2 = 0; n2 \u0026lt; N2; n2++) { // W buffer stores: N1*N0*K  mvin(W [0:K, n2 * N1 * N0:(n2 + 1) * N1 * N0]); compute_matmul(*W, *IA, *OA, N2, N1, N0, M2, M1, K1, K0, m2, n2); mvout(OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m2 * M1:(m2 + 1) * M1]); } } for (n2 = 0; n2 \u0026lt; N2; n2++) { // W buffer stores: N1*N0*K  mvin(W [0:K, n2 * N1 * N0:(n2 + 1) * N1 * N0]); for (m2 = 0; m2 \u0026lt; M2; m2++) { // IA buffer stores: M1*K  mvin(IA [m2 * M1:(m2 + 1) * M1, 0:K]); compute_matmul(*W, *IA, *OA, N2, N1, N0, M2, M1, K1, K0, m2, n2); mvout(OA [n2 * N1 * N0:(n2 + 1) * N1 * N0, m2 * M1:(m2 + 1) * M1]); } }   如何判断两种循环的优劣？我们需要统计据的搬移量：\n针对这两种循环，我们可以设置不同的并行机制（即在不同位置使用spatial_for）。总而言之，设置loop的顺序本质上是一个优化问题，给定：维度数和硬件参数，最优化：能量/延迟。\nSparsity ReLU函数的存在使得activation中存在大量的0，同理也可以通过剪枝使得weight中存在大量的0。\n这为我们存储数值提供了机会：\n  bitmask：使用0/1数组表示是否为0（会造成独立于密度的固定开销，对高密度不友好）\n  run-length encoding：游程编码（同样有固定开销）\n  compressed sprase row：（可以进行快速的行选取）\n这其中的col index直接记录其列号，而bound中的第i个数字记录了前i-1行包含的非0元素的数量。例如我们如果需要选取第三行的数字，由于Bounds[2]=3，Bounds[3]=5，我们可以得知在第三行的数字于value中的下标为[3,5)，据此可以直接取用数字。\n  compressed sprase column：（可以进行快速的列选取）\n  随后稀疏矩阵和密集矩阵的混合计算。按照操作数的稀疏性和对齐方式分为两种：\n  Indirection：使用稀疏矩阵中非0元素的下标去索引密集矩阵中的元素。\n场景：算式y[i] = A[i, j] * x[j]，A的第二维是稀疏的，x是密集的。可以想到循环次数要小于len(x)，为len(A_index[1])。\n  Intersection：寻找权重和激活中均不为0的数值。\n场景：算式y[i] = A[i, j] * x[j]，A的第二维是稀疏的，x是稀疏的（想象两个指针在非0下标数组上同时滑动的场景）。\n  Arbitration：先计算，之后在输出的矩阵中决定这些结果的位置。\n场景：算式A[i, j] = B[i, k] * C[k, j]，B和C第二维均是稀疏的，我们先将两者“浓缩”的矩阵进行相乘，然后在输出中决定其位置。\n  Near-Data Processing 在之前的几节中，我们介绍的主要是Fully Connected和Conv两个深度学习的算子，这两者本质上都可以划归为GEMM（通用矩阵乘）。GEMM中的基本单元是MAC乘加单元，它包括2个operator，load 2个数据，然后将1个结果store进寄存器中。\n深度学习中其实还有一些其他的算子，它们在operator、load、store上都和GEMM有很大的不同。举例说明：\nElement-wise operation：逐元素运算，例如LSTM的Hadama积，ResNet中的残差相加。它包括1个operator，load 2个数据，然后将1个结果store进寄存器中。\n 这里需要特别强调的是残差相加，尽管没有引入额外的参数或者是计算复杂度，但仍然会导致减速，这是由于该操作增加了高速缓存的处理量，对之前数据的储存增加了内存负担。\n Embedding Layer：嵌入运算。这一运算的本质是将id序列（例如词序号等）转换为one-hot向量之后，与Embedding大矩阵相乘，得到对应的词嵌入。不过实际操作中一般不使用这种类似于GEMM的运算，而是基于id序列对Embedding大矩阵进行一种类似于查找表的运算。因此它需要1个operator，load 2个数据，然后将1个结果store进寄存器中。\n实际上，上述的一些算子在深度学习之外的一些领域也有用，如GC（Java中的垃圾回收机制）、LINQ（语言集成查询）等。我们可以用上述算子构建Near-Data Processing的专用硬件（这里的Near-Data Processing指的是具有直接访问DRAM的功能，而不需要通过高速缓存的层次结构的硬件）。\nIn Memory Computing 在介绍IMC之前，我们需要首先回顾一下存储器的分类：\n Random Access Memory(RAM): 随机存取存储器，其“随机存取”指的是当存储器中的消息被读取或写入时，所需要的时间与这段信息所在的位置无关。  SRAM：静态随机存取存储器，其“静态”指的是这种存储器只要保持通电，里面存储的数据就可以恒常保持。多用于高速cache。 DRAM：动态随机存取存储器，其“动态”指的是由于晶体管会有漏电电流的现象，导致电容上所存储的电荷数量并不足以正确的判别数据，进而导致数据毁损，所以DRAM需要经常性刷新。多用于廉价内存。 Flash：闪存，与DRAM和SRAM不同的是掉电后数据不会被清除，但读写速度会慢。   Serial Access Memory(SAM): 顺序存取存储器需要按顺序读取存储数据。 Content Addressable Memory(CAM): 可以根据其内容而不是其名称或位置进行检索。它已被用于固定内容的高速存储和检索。  以下将重点计算在SRAM中进行的存内计算：\n在Deep Learning等数据密集型的运算场景中，数据搬移的消耗将超越数据运算。一个直观的思路是在存储器中进行运算，而不是将数据从存储器中搬移出来后进行运算（例如，对于一个weight stationary的dataflow，权重只需要呆在存储器中，无需进行搬运）。这就是存内计算的基本思想。\n需要指出的是，目前的存内计算大部分是在模拟域进行的。因此需要先将数字化的输入经过DAC之后与权重进行运算，之后通过ADC变换回数字域。\n这样我们就得到了IMC中数据的传输范式：\n input activation：被DAC转换为电压值，位于SRAM的WL（word line）上。 output activation：在SRAM的BL（bit line）上进行累加。 weight：储存于存储单元，无需搬移。  例如在这项工作中，一个5位数模转换器被用来驱动字线（WL）到代表特征向量的模拟电压，而位单元存储二进制权重±1。位单元的电流（IBC）实际上是特征向量的值和存储在位单元中的权重值的乘积；来自一列中的位单元的电流加在一起对位线（VBL）放电。\n存内计算为什么在模拟域中进行？一个好处是其加法可以直接借助基尔霍夫定律中电流的相加，而无需借助额外的硬件，但模拟电路中的不稳定，精度难以控制也造成存内计算难以落地的窘境。\n 例如，在模拟场景下，我们如果想要实现维度更高的阵列，就需要更长的导线，然而这会导致更大的寄生电容。\n 同时，目前的存内计算大多集中于推理阶段，而非训练阶段。这是由于这一技术更擅长于数据的读而非写，例如在更新权重序列的数据时需要更大的电压。\n与此同时，现有的深度学习IMC工作大多数基于BNN（二值网络），位宽的限制使其效果无法与传统数字电路媲美。\n","date":"2023-01-14T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/hardware-machine-learning/ee-290-lab-3/","title":"EE290-2 LAB3 tiling and optimization for accelerator"},{"content":"Dataflow 乘加累计单元 机器学习中的核心计算单元被称为乘積累加单元（Multiply Accumulate, MAC）。它使用一条指令进行以下运算： $$ a \\leftarrow a + b \\times c $$\n为减小其数据搬移所造成的的开销，我们需要关注其数据的重复使用，对此我们采取以下方法：\n Temporal reuse：单个数据被同一个计算单元重复使用多次 Spatial reuse：单个数据被多个不同的计算单元同时使用  OS Dataflow \u0026amp; WS Dataflow 基于这样的思想，我们可以设计两种不同的dataflow（以下的代码以1D conv为例）：\n Output Stationary Dataflow：输出的同一数据在时间上相邻，便于输出数据的重用  1 2 3 4 5 6 7  for (q=0; q\u0026lt;Q; q++) // 输出的循环位于外层 { for (s=0; s\u0026lt;S; s++) { OA[q] += IA[q+s] * W[s]; } }   这一重用可以在output侧加入寄存器实现：  Weight Stationary Dataflow：权重的同一数据在时间上相邻，便于权重数据的重用  1 2 3 4 5 6 7  for (s=0; s\u0026lt;S; s++) // 权重的循环位于外层 { for (q=0; q\u0026lt;Q; q++) { OA[q] += IA[q+s] * W[s]; } }   这一重用可以在weight侧加入寄存器实现： Accelerator 接下来，我们将在乘加单元的基础上探索更加宏观层面的矩阵乘法的高效数据流计算。\n首先我们写出一般矩阵乘法的计算流：\n1 2 3 4 5 6 7 8  for (m=0; m\u0026lt;M; m++) { for (n=0; n\u0026lt;N; n++) { OA[n,m] = 0; for (k=0; k\u0026lt;K; k++) { OA[n,m] += IA[m, k] * W[k, n]; } } }   数据的并行化大致可以分为两种方法：Spatial-K和Spatial-N。\nSpatial-K旨在将IA中一行和W中一列的计算并行化：\n1 2 3 4 5 6 7 8  for (m=0; m\u0026lt;M; m++) { for (n=0; n\u0026lt;N; n++) { OA[n,m] = 0; spatial_for (k=0; k\u0026lt;K; k++) { OA[n,m] += IA[m, k] * W[k, n]; } } }   Spatial-N旨在将W中的一行进行广播：\n1 2 3 4 5 6 7 8  for (m=0; m\u0026lt;M; m++) { spatial_for (n=0; n\u0026lt;N; n++) { OA[n,m] = 0; for (k=0; k\u0026lt;K; k++) { OA[n,m] += IA[m, k] * W[k, n]; } } }   当然，我们也可以将其进行组合：\n1 2 3 4 5 6 7 8  for (m=0; m\u0026lt;M; m++) { spatial_for (n=0; n\u0026lt;N; n++) { OA[n,m] = 0; spatial_for (k=0; k\u0026lt;K; k++) { OA[n,m] += IA[m, k] * W[k, n]; } } }   Adder Tree 加法树（Adder Tree）采用了一种以空间换取时间的策略。为了同时计算多个IA[m, k] * W[k, n]，可以将多个加法器堆叠组成树状结构：\nDirect-Wiring Multicast 将权重的一行进行广播，使得权重的N列可以同时计算：\nSystolic Array 脉动阵列（Systolic Array）既可以实现Spatial-K中的加速（Systolic Accleration）功能，也可以实现Spatial-N中的广播（Systoic Multicast）功能。这里我们直接将两者组合以说明其功能。\n考虑一个基本的矩阵乘法$C=AB$，weight stationary的systolic array计算方式如下：\n1 2 3 4 5 6 7 8 9 10 11 12  // stage 1 c_11 = a_11 * b_11 // stage 2 c_11 += a_12 * b_21 c_12 = a_11 * b_12 c_21 = a_21 * b_11 // stage 3 c_12 += a_12 * b_22 c_21 += a_22 * b_21 c_22 = a_21 * b_12 // stage 4 c_22 += a_22 * b_22   如果考虑到$B$的预装，计算方式改为：\n1 2 3 4 5 6 7 8  // stage 1 move B and do not load // stage 2 load b_11, b_21 // stage 3 load b_12, b_22 c_11 = a_11 * b_11 // stage 4 and after same as systolic-1   这会带来一个问题：在预装B的时候，我们不能做任何有用的工作。所以，每一个处理单元必须要有至少两个寄存器来存储它负责的B上的元素。在任何特定的周期中，一个寄存器将用于执行乘法累加操作，而另一个寄存器将作为预加载过程的一部分向下传播B的元素。\n这仍然遗留了一个问题：如果我们想保留之前预装的矩阵，而不是用一个新的矩阵来覆盖它，怎么办？我们如何将其传达给收缩阵列的处理单元？对此可以引入一个1 bit的信号量。\n如果传播信号为0，那么第1个寄存器（寄存器0）应该用于向下传播权重，而第2个寄存器（寄存器1）应该用于乘法累加。同样，如果传播信号为1，那么第1个寄存器（寄存器0）应该被用来进行乘法累加，而第2个寄存器（寄存器1）应该被用来向下传播权重。\n以下是$\\alpha \\times \\beta = \\gamma$和$A \\times B = C$的例子：由于$\\beta$的标识均为0，所以$\\beta$由寄存器0向下传播；当$\\beta$准备用于计算时，$b$被载入，由于传输信号为1，$b$由寄存器1向下传播，而寄存器0用于乘法的计算。\n同理有$\\alpha \\times \\beta = \\gamma$，$A \\times \\beta = C$, $\\alpha \\times Y = Z$的数据流。\n","date":"2023-01-11T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/hardware-machine-learning/ee-290-lab-2/","title":"EE290-2 LAB2 hardware implementation of machine learning systems"},{"content":"本节用到的公式虽然基础，但想要真正理清楚公式之间的关系还是比较麻烦的（博主就被绕晕过）。新人首秀，多多指教~\nassignment 本实验是UC Berkeley EECS 本科课程Hardware for Machine Learning的第1个lab。本lab要求将一个简单的LeNet以不同的方式进行量化，并在CIFAR 10数据集上进行测评。具体要求见本文档。\n关于量化(quantization)的基本公式本文不再赘述，详见1。本文主要关注量化的代码实现细节。\nmethod 流程 本文主要介绍静态训练后量化（static post training quantization）。其流程如下：\n 在浮点环境下训练模型。 使用少量的校正数据，确定激活（activation，即经过中间层的数据流）的数据分布，并得到其scaling factor和zero point（本文简单起见，在量化时不使用zero point，scaling factor使用最常规的公式得到：$S = \\frac{r_{\\max}-r_{\\min}}{q_{\\max}-q_{\\min}}$），用于后续的推理2。 量化权重（weight）、偏置（bias，可选）和激活，进行全整数推理。  关于weight和activation的量化 假设一个神经网络由$W_1, W_2, W_3\\cdots$若干层组成（此处我们假设它们均为线性层，但其他如卷积层等同理）。\n$$ O_1 = IW_1 $$\n$$ W_{1q} = {\\rm clamp}({\\rm round}(\\frac{W_1}{S_{W_1}}), \\min,\\max) $$\n$$ I_{q} = {\\rm clamp}({\\rm round}(\\frac{I}{S_I}), \\min,\\max) $$\n我们可以得到使用$W_q$和$S_q$重建的$W_r$，$I_r$同理： $$ W_{1r} = S_{W_1}W_{1q} \\approx W_1 $$\n$$ I_r = S_II_q \\approx I $$\n则： $$ O_1 \\approx W_{1r}I_r \\approx S_{W_1}S_IW_{1q}I_q $$\n 为方便起见，我们在代码中将$W_{1q}I_q$记做$R_1$，将$W_{2q}O_{1q}$记作$R_2$，以此类推。\n 注意到此处$W_{1r}$和$I_r$均为整数，$S_{W_1}$和$S_I$均为浮点数，则$O$也为浮点数。但当$O$进入下一个计算模块时，同样需要进行整数量化：\n$$ O_{1q} = {\\rm clamp}({\\rm round}(\\frac{O_1}{S_{O_1}}), \\min,\\max) \\approx \\frac{S_{W_1}S_I}{S_{O_1}}W_{1q}I_q $$\n$$ O_{1r} = S_{O_1}O_{1q} \\approx O_1 $$\n而此处的$\\frac{S_{W_1}S_I}{S_{O_1}}$同样可以化为整数的形式，即$\\frac{S_{W_1}S_I}{S_{O_1}}\\approx2^{-n}$，从而实现真正意义的全整数推理。\n对于第二层参数，我们同样有： $$ O_{2} \\approx W_{2r}O_{1r} \\approx S_{W_2}S_{O_1}W_{2q}O_{1q} $$\n$$ O_{2q} = {\\rm clamp}({\\rm round}(\\frac{O_2}{S_{O_2}}), \\min,\\max) \\approx \\frac{S_{W_2}S_{O_1}}{S_{O_2}}W_{2q}O_{1q} $$\n当计算到最后一层第$n$层时，我们无需获取$S_{O_n}$（因为计算它的唯一目的在于进行后一层的整数推理），于是我们有：\n$$ O_{n} \\approx O_{nr} = O_{nq}S_{O_n} = S_{W_n}S_{O_{n-1}}W_{nq}O_{(n-1)q} $$\n 实际上，此处是否获取$S_{O_n}$无关紧要。例如，若此处进行的是图像分类任务，我们需要求出的实际上只是数量之间的比例关系，而不是数量本身。\n 综上，我们可以得出每一层的scaling factor $S_i$：\n 输入：$S_I$ 中间层：$\\frac{S_{O_1}}{S_{W_1}S_{I}}, \\frac{S_{O_2}}{S_{W_2}S_{O_1}}, \\cdots, \\frac{S_{O_n}}{S_{W_n}S_{O_{n-1}}}$  令$\\frac{S_{O_i}}{S_{W_i}S_{O_{i-1}}}=S_i$（当i=0时，$S_{O_{i-1}}$化为$S_I$），则也可以使用递归的方法写作：\n$$ S_i = \\begin{cases} \\frac{S_{O_1}}{S_{W_1}S_I}, i = 1 \\ \\frac{S_{O_i}}{S_{W_i}S_I\\prod_1^{i-1}(S_kS_{W_k})}, i \u0026gt; 1 \\end{cases} $$\n关于bias的量化 在量化中处理bias有两种较为简单的办法：\n 使用无bias的模型。 不对bias进行quantization，而是直接使用浮点域的bias和反量化之后的weight、activation乘积进行相加。相加完毕之后再对结果进行量化。  但同样地，我们也可以在整数域进行带bias的推理。我们以本题的场景为例（只对最后一层进行量化），最后一层在浮点域的推理可以表示为：\n$$ O_n = W_nO_{n-1} + b_n $$\n量化之后的表示为： $$ O_n \\approx S_{W_n}S_{O_{n-1}}(W_{nq}O_{(n-1)q} + b_n) $$\n据此可以直接得出$b_n$的量化因子为：\n$$ S_{b_n} = S_{W_n}S_{O_{n-1}} = S_{W_n}S_I\\prod_{k=1}^{n-1}(S_kS_{W_k}) $$\ncode 本节只呈现一些关键代码\npoint 1: 量化权重 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  from typing import Tuple def quantized_weights(weights: torch.Tensor, bit: int=8, actual=True) -\u0026gt; Tuple[torch.Tensor, float]: if actual: maxval = torch.max(weights) minval = torch.min(weights) else: mu = torch.mean(weights) sigma = torch.std(weights) maxval = mu + 3 * sigma minval = mu - 3 * sigma maxval = torch.max(-minval, maxval) scale = maxval / (2 ** (bit - 1)) result = torch.round(weights / scale) return torch.clamp(result, min=-2**(bit-1), max=2**(bit-1)-1), scale   本节作者在确定数据范围时尝试了两种方法：min/max法和3-sigma法。由于LeNet的权重分布基本类似于正态分布，所以两种策略差距不是很大。\npoint2: 量化激活 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def quantize_activations(activations: np.ndarray, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]], bits = 8) -\u0026gt; float: maxval = np.max(activations) minval = np.min(activations) n_a = (maxval - minval) / (2 ** bits) if len(ns) == 0: scale = n_a / (n_initial_input * n_w) else: div = 1 for n in ns: div *= n[0] * n[1] scale = n_a / (n_initial_input * n_w * div) return scale   此处的n_w指代$S_{W_i}$，n_a指代$S_{O_i}$，对于第$i(i\\ge2)$层，ns列表中存储的若干对元素为$[S_{W_1},S_1], \\cdots, [S_{W_{i-1}},S_{i-1}]$，套用前文关于$S_i$的公式即得。\npoint3: 正向传播 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  def forward(self, x: torch.Tensor, bits: int = 8) -\u0026gt; torch.Tensor: # You can access the output activation scales like this: input_scale = self.input_scale fc1_output_scale = self.fc1.output_scale fc2_output_scale = self.fc2.output_scale fc3_output_scale = self.fc3.output_scale conv1_output_scale = self.conv1.output_scale conv2_output_scale = self.conv2.output_scale I = x I_q = torch.clamp(torch.round(I / input_scale), min=-2**(bits-1), max=2**(bits-1)-1) R_1 = self.pool(F.relu(self.conv1(I_q))) O_1q = torch.clamp(torch.round(R_1 / conv1_output_scale), min=-2**(bits-1), max=2**(bits-1)-1) R_2 = self.pool(F.relu(self.conv2(O_1q))) O_2q = torch.clamp(torch.round(R_2 / conv2_output_scale), min=-2**(bits-1), max=2**(bits-1)-1) O_2q = O_2q.view(-1, 16 * 5 * 5) R_3 = F.relu(self.fc1(O_2q)) O_3q = torch.clamp(torch.round(R_3 / fc1_output_scale), min=-2**(bits-1), max=2**(bits-1)-1) R_4 = F.relu(self.fc2(O_3q)) O_4q = torch.clamp(torch.round(R_4 / fc2_output_scale), min=-2**(bits-1), max=2**(bits-1)-1) R_5 = self.fc3(O_4q) O_5q = torch.clamp(torch.round(R_5 / fc3_output_scale), min=-2**(bits-1), max=2**(bits-1)-1) return O_5q.to(device)   注意clamp操作和round操作插入的位置。\npoint4: 量化偏置 1 2 3 4 5  def quantized_bias(bias: torch.Tensor, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]], bits: int=32) -\u0026gt; torch.Tensor: scale = n_w * n_initial_input for n in ns: scale *= n[0] * n[1] return torch.clamp((bias / scale).round(), min=MIN_32B_SINT, max=MAX_32B_SINT)   同point 2的处理方法。\nresult 将一些基本结果列入下表：\n   权重bit 激活bit 偏置bit acc     32 32 \\ 55.35%   8 32 \\ 55.33%   4 32 \\ 53.13%   2 32 \\ 31.27%   8 8 \\ 54.80%   32 32 32 55.73%   8 8 32 49.77%   8 8 8(最后一层) 55.64%    这里值得一提的是关于偏置的结论：当一个模型有偏置时，如果只量化权重和激活而无视偏置，将会有较大的精度下降，这是因为量化之后整型的权重和激活之积和浮点型的偏置所在的值域有较大的差异。而对最后一层的偏置进行量化后，两者的值域之间的差异大大减少，偏置真正起到了其作用，最后模型输出的误差有所减小，因而精度有所恢复。\n不过在实际程序中大多采用“不对bias进行quantization，而是直接使用浮点域的bias和反量化之后的weight、activation乘积进行相加，相加完毕之后再对结果进行量化”的方法。考虑到偏置相加的计算开销要远远小于权重相乘，这样的简便做法是可以被接受的。\nreference   Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Nvidia PPT for quantization\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"2022-12-24T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/hardware-machine-learning/ee-290-lab-1/","title":"EE290-2 LAB1 quantization"},{"content":"DCT DCT一般用于图像处理。我们使用L2距离来量化编码误差。为了便于观看，我们倾向于保留低频分量。被截断的系数的能量越小，编码误差越小。\n如果将DFT作用于图像处理有以下缺陷：由于DFT的信号由周期延拓得到，使得包络和相位不连续，引入高频分量。对此，可以将原序列对称扩展到2N个点，然后做2N点周期延拓。\n首先列出DCT的定义： $$ X_{DCT}[k] = \\begin{cases} \\sqrt{\\frac{1}{N}}\\sum_{n=0}^{N-1}x[n],k=0\\ \\sqrt{\\frac{2}{N}}\\sum_{n=0}^{N-1}x[n]{\\rm cos}(\\frac{\\pi}{2N}k(2n+1)), elsewhere \\ \\end{cases} $$ 为什么要有k=0的特例？保范。\n这个公式实际上来源于2N点延拓的DFT： $$ X_2[k]=\\sum_{n=0}^{2N-1}x_2[n]W_{2N}^{kn}=\\sum_{n=0}^{N-1}xn = 2e^{j\\frac{\\pi}{2N}k}\\sum_{n=0}^{N-1}x[n]{\\rm cos}(\\frac{\\pi}{2N}k(2n+1)) $$ 可以看到，这个公式与DCT的公式只有一个系数的区别。\n $e^{j\\frac{\\pi}{2N}k}$这个系数来源于何处？实际上观察2N点延拓后的序列，它并非完全偶对称的，其DFT也并非实序列。这一系数可以看做一个平移系数。\n DCT的快速算法：\n 2N点DFT$X_1[k]$。 取$\\frac{1}{\\sqrt{N}}Re{X_1[k]e^{-j\\pi\\frac{k}{2N}}}$，按照k=0调整系数。  ","date":"2022-10-25T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/digital_signal_processing/dct/","title":"DCT"},{"content":"method 整体上看，Swin的结构类似于PVT，呈现出一种金字塔架构。\n为什么金字塔结构如此重要？FPN理论认为，不同尺寸的特征图拥有不同的感受野，同时还有池化操作，从而能够很好地处理这个物体不同尺寸的这个问题（这点可以参考论文U-Net）。这样的模型更适合使用密集型任务。\ncode Patch Merging 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  def forward(self, x): \u0026#34;\u0026#34;\u0026#34; x: B, H*W, C \u0026#34;\u0026#34;\u0026#34; print(x.shape) H, W = self.input_resolution B, L, C = x.shape assert L == H * W, \u0026#34;input feature has wrong size\u0026#34; assert H % 2 == 0 and W % 2 == 0, f\u0026#34;x size ({H}*{W}) are not even.\u0026#34; x = x.view(B, H, W, C) x0 = x[:, 0::2, 0::2, :] # B H/2 W/2 C x1 = x[:, 1::2, 0::2, :] # B H/2 W/2 C x2 = x[:, 0::2, 1::2, :] # B H/2 W/2 C x3 = x[:, 1::2, 1::2, :] # B H/2 W/2 C x = torch.cat([x0, x1, x2, x3], -1) # B H/2 W/2 4*C x = x.view(B, -1, 4 * C) # B H/2*W/2 4*C x = self.norm(x) x = self.reduction(x) return x   1  self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)   这部分首先将特征图不同位置的信息提取出来，组成4张新的特征图，然后在通道维度上进行拼接，通道数就扩大为了原来的4倍，最后通过投影将通道数缩小为之前的一半。如下图：\n提取方法如下：\nSwin Transformer Block 首先需要说明的是，两个transformer层合在一起才算是swin transformer的一个基本单元。两次attention作用的位置不同：\nWindow Based Attention 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def forward(self, x, mask=None): \u0026#34;\u0026#34;\u0026#34; Args: x: input features with shape of (num_windows*B, N, C) mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \u0026#34;\u0026#34;\u0026#34; # [128, 49, 96] B_, N, C = x.shape qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = (q @ k.transpose(-2, -1)) if mask is not None: nW = mask.shape[0] attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) attn = attn.view(-1, self.num_heads, N, N) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B_, N, C) x = self.proj(x) x = self.proj_drop(x) return x   首先注意到输入的中间维度是49。这是因为swin的window size为7，这意味着一个window中一共包含49个patch。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  def forward(self, x): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, \u0026#34;input feature has wrong size\u0026#34; shortcut = x x = self.norm1(x) x = x.view(B, H, W, C) # [2, 56, 56, 96] # cyclic shift if self.shift_size \u0026gt; 0: # 3 shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) else: shifted_x = x # [2, 56, 56, 96] # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # nW*B, window_size*window_size, C # [128, 49, 96] # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C # [128, 49, 96] # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\t# [128, 7, 7, 96] shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H\u0026#39; W\u0026#39; C # reverse cyclic shift if self.shift_size \u0026gt; 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x x = x.view(B, H * W, C) # [2, 3136, 96] # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) return x   代码通过window_partation操作，将一个batch内的图片切分为若干个长为49的序列，然后在此序列中做attention，从而减小计算量。\nShift Window swin transformer在奇数层的window不shift，但在偶数层shift，这点在层数的设置上也可以看出：\n1 2 3 4 5 6 7 8  SwinTransformerBlock(dim=dim, input_resolution=input_resolution, num_heads=num_heads, window_size=window_size, shift_size=0 if (i % 2 == 0) else window_size // 2, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer)   对于cyclic shift的形象解释：\n而后是mask机制。在window shift之后，为了并行计算，图片仍然被划分为4个大块。但为了让图片避免与不相关的部分作自注意力，我们引入了一下掩码：\n消融实验表明shift window和pos embedding在检测任务上的有效性：\nSwin Transformer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def forward_features(self, x): x = self.patch_embed(x) if self.ape: x = x + self.absolute_pos_embed x = self.pos_drop(x) for layer in self.layers: x = layer(x) x = self.norm(x) # B L C # [2, 49, 768] x = self.avgpool(x.transpose(1, 2)) # B C 1 # [2, 768, 1] x = torch.flatten(x, 1) # [2, 768] return x def forward(self, x): x = self.forward_features(x) x = self.head(x) return x   类似resnet，swin会将最后一层特征图池化，而不是引入cls token。\n","date":"2022-10-22T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/cv/swin/","title":"Swin"},{"content":"FFT [TOC]\n按频率抽取 按频率抽取的情况下，$x[n]$是有序的。序列的分治通过$x[n]$的前后划分来实现。\n对于基$b$的FFT，我们不妨对$N$点序列进行以下划分：$x_1[n] = x[n],\\ x_2[n] = x[n+\\frac{N}{b}],\\ \u0026hellip; \\ x_b[n] = x[n+\\frac{(b-1)N}{b}],\\ n=0,1\u0026hellip;\\frac{N}{b}-1$。\n这样，我们就可以对DFT表达式进行分解: $$ X[k] = \\sum_{n=0}^{\\frac{N}{b}-1} [x_1[n]W_{N}^{nk} + x_2[n]W_{N}^{(n+\\frac{N}{b})k}+\u0026hellip;+x_b[n]W_N^{(n+\\frac{(b-1)N}{b})k}] $$ 使用$bk,bk+1,\u0026hellip;,bk+b-1$对$k$进行替换，就可以将$W_{N}^{nk}$替换为$W_{\\frac{N}{b}}^{nk}$，以实现递归。带入后的表达式如下： $$ X[bk+m] = \\sum_{n=0}^{\\frac{N}{b}-1}{x[n]+x[n+\\frac{N}{b}]W_{b}^{m}+x[n+\\frac{2N}{b}]W_{b}^{2m}\u0026hellip;+x[n+\\frac{(b-1)N}{b}]W_{b}^{(b-1)m}}W_{\\frac{N}{b}}^{nk}W_N^{nm} $$ 对于单位根系数有简单的记忆技巧：\n 基2 $(1,1)\\ (1,-1)$ 基3 $(1,1,1)\\ (1,W_3^1,W_3^2)\\ (1,W_3^2,W_3^4)$ 基4 $(1,1,1,1)\\ (1,-i,-1,i) \\ (1,-1,1,-1) \\ (1,i,-1,-i)$ 基n 可以考虑每一个分量分别是旋转$m$次，每次顺时针旋转$\\frac{2\\pi}{b}$所得到的  按时间抽取 按时间抽取的情况下，$X[k]$是有序的。序列的分治通过$x[n]$的模划分来实现。\n对于基$b$的FFT，其DFT可以写为如下形式： $$ X[k] = \\sum_{n=0}^{\\frac{N}{b}-1}x[bn]W_{\\frac{N}{b}}^{kn} + W_{N}^{k}\\sum_{n=0}^{\\frac{N}{b}-1}x[bn+1]W_{\\frac{N}{b}}^{kn} + \u0026hellip; + W_{N}^{(b-1)k}\\sum_{n=0}^{\\frac{N}{b}-1}x[bn+b-1]W_{\\frac{N}{b}}^{kn} $$ 使用$k,k+\\frac{N}{b},\u0026hellip;,k+\\frac{(b-1)N}{b}$对$k$进行代换，可以得到： $$ X[k+\\frac{mN}{b}] = \\sum_{n=0}^{\\frac{N}{b}-1}{x[bn]+x[bn+1]W_b^mW_N^k+x[bn+2]W_b^{2m}W_N^{2k}+\u0026hellip;+x[bn+b-1]W_b^{(b-1)m}W_N^{(b-1)k}}W_{\\frac{N}{b}}^{kn} $$\n应用 卷积 考虑一个长度为$L$的序列$x_1[n]$和一个长度为$P$的序列$x_2[n]$，卷积后的序列$y[n]$长度为$(L+P-1)$。将$y[n]$按照$N$点混叠得到$N$点循环卷积$\\bar y_N[n]$，当$N\u0026gt;L+P$，则可以将$X_1[k]X_2[k]$的循环卷积变换为$X_1(e^{j\\omega})X_2(e^{j\\omega})$的线性卷积（参考循环卷积的频域理解）。\n如果$P$较小，则需要对$x_2[n]$大量补0，导致计算量增大，对此需要对$x_1[n]$分段切割，表示为长度为$L$的平移有限长序列之和： $$ x[n] = \\sum_{r=0}^{\\infty}x_r[n-rL] \\ x_r[n] = x[n+rL], 0\\le n\\le L-1 $$ 分段计算线性卷积$y_r[n] = x_r[n] * h[n]$，使用$N\\ge L+P-1$点DFT计算线性卷积，得到的序列。由于每一个分割后的序列段长度为$L$，所以上述卷积长度为$L+P-1$，各序列段的卷积将会重叠$P-1$点。\n 重叠相加法：  将$x[n]$拆分成$L$点长的子段$x_r[n]$ 将$h[n]$补零做$L+P-1$点FFT$H_{L+P-1}[k]$ 将$x_r[n]$补零做$L+P-1$点FFT$X_r [k]$ $X_r[k]$和$H_{L+P-1}[k]$逐点相乘后作$L+P-1$点IFFT得$y_r[n]$ 平移$rL$点后重叠相加$y[n] = \\sum_{r=0}^{\\infty}y_r[n-rL]$   重叠保留法：  $h[n]$做$L$点FFT$H_L[k]$ 将$x[n]$分为长度为$L$的序列段，使得每个输人段与先 前的序列段重叠$(P-1)$点，也就是$x_r[n]=x[n+r(L-P+1)-P+1]\\ (0\\le n \\le L-1)$。 做卷积$y_{rp}[n]=h[n]*x_r[n]$（使用FFT和IFFT） 由于各段$x_r[n]$之间有交叠，所以$y_{rp}[n]$的重复段$0\\le n \\le P-2$段被去掉，新的子序列$y_r[n] = y_{rp}[n](P-1\\le n \\le L-1)$ 平移$r(L-P+1)$点后重叠相加$y[n] = \\sum_{r=0}^{\\infty}y_r[n-r(L-P+1)+P-1]$    相关 将一个序列进行反转共轭 ${\\rm FFT}{y^{}[-n]}=Y^{}[k]$ 后计算卷积。\n短时FFT $$ X[k,n+1]=(X[k,n]-x[n-W+1]+x[n+1])e^{j2\\pi\\frac{k}{W}} $$\n","date":"2022-10-18T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/digital_signal_processing/fft/","title":"FFT"},{"content":"method intro 我们首先介绍Transformer用于CV任务的难处：如何把二维的图像变为一维的序列。Transformer的计算复杂度与序列长度成正比。之前的工作为了解决这一问题，或者在图片的一个小区域使用transformer，或者在图片的长宽方向分别使用transformer。 ViT将图片分割为一个一个的patch，并将其linear embedding作为Transformer的输入。\nViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制。\n Transformer相比CNN缺少归纳偏置，即先验知识。CNN的两种先验知识如下：\n 局部性：图片上相邻的区域具有相似的特征 平移不变性：$f(g(x))=g(f(x))$ 所以在小模型时CNN比transformer要好。   struct 可以看到，ViT的架构与bert几乎完全一样，只不过bert的输入是word embedding + positional embedding，ViT的输入是patch的linear projection + position embedding。不过需要指出的是，ViT是有监督的。 ViT的训练步骤如下：\n patch embedding：输入224x224，patch大小16x16，则输入序列长度为196，每个patch维度为16x16x3=768。经过768x768的linear projection之后维度为196x768。由于在前面需要加一个特殊字符[cls]，因此最终的维度是197x768。 positional encoding：与patch embedding相加得到最终结果，有以下三种：  1-D pos emb：只考虑patch flatten之后的相对位置信息 2-D pos emb：同时考虑X轴和y轴的信息 rel pos emb：   MHA  关于位置编码 不管使用哪种位置编码方式，模型的精度都很接近，甚至不适用位置编码，模型的性能损失也没有特别大。原因可能是ViT是作用在image patch上的，而不是image pixel，对网络来说这些patch之间的相对位置信息很容易理解。\n混合模型 在数据量较小时，混合模型比较占优，但较大时会被Transformer超越。\n图像分类方法 在原论文中，为了执行分类任务，作者在编码时引入了NLP界常用的[cls]标签，但也可以使用传统的average pooling，两种方法较为相近。\n数据集 ViT只在较大数据集上占据优势。\ncode MLP ViT中的MLP和Transformer中的没有太大的区别：\n1 2 3 4 5 6 7  MLPBlock( (0): Linear(in_features=768, out_features=3072, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=3072, out_features=768, bias=True) (4): Dropout(p=0.0, inplace=False) )   当然，注意到代码中对MLP层进行了初始化：\n1 2 3 4 5 6 7 8 9 10  class MLPBlock(MLP): \u0026#34;\u0026#34;\u0026#34;Transformer MLP block.\u0026#34;\u0026#34;\u0026#34; def __init__(self, in_dim: int, mlp_dim: int, dropout: float): super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout) for m in self.modules(): if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.normal_(m.bias, std=1e-6)   Encoder Layer 基本结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  EncoderBlock( (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True) (self_attention): MultiheadAttention( (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True) ) (dropout): Dropout(p=0.0, inplace=False) (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True) (mlp): MLPBlock( (0): Linear(in_features=768, out_features=3072, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=3072, out_features=768, bias=True) (4): Dropout(p=0.0, inplace=False) ) )   结构同样模仿transformer，此处不再赘述。\n1 2 3 4 5 6 7 8 9 10  def forward(self, input: torch.Tensor): torch._assert(input.dim() == 3, f\u0026#34;Expected (batch_size, seq_length, hidden_dim) got {input.shape}\u0026#34;) x = self.ln_1(input) x, _ = self.self_attention(query=x, key=x, value=x, need_weights=False) x = self.dropout(x) x = x + input y = self.ln_2(x) y = self.mlp(y) return x + y   Encoder Encoder在堆叠EncoderLayer的基础上，引入了最后一层的norm和embedding：\n1 2 3 4  def forward(self, input: torch.Tensor): torch._assert(input.dim() == 3, f\u0026#34;Expected (batch_size, seq_length, hidden_dim) got {input.shape}\u0026#34;) input = input + self.pos_embedding return self.ln(self.layers(self.dropout(input)))   这里要注意一下embedding。原文中尝试了3种embedding，但结果类似。此处看上去使用的是1d-embedding，形状为[1, 197, 768]。\nVision Transformer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def forward(self, x: torch.Tensor): # Reshape and permute the input tensor # [2, 3, 224, 224] x = self._process_input(x) # [2, 196, 768] n = x.shape[0] # Expand the class token to the full batch batch_class_token = self.class_token.expand(n, -1, -1) x = torch.cat([batch_class_token, x], dim=1) # [2, 197, 768] x = self.encoder(x) # [2, 197, 768] # Classifier \u0026#34;token\u0026#34; as used by standard language architectures x = x[:, 0] # [2, 768] x = self.heads(x) # [2, 1000] return x   首先重点关注图像的预处理部分self._process_input(x)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  def _process_input(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # [2, 3, 224, 224] n, c, h, w = x.shape p = self.patch_size n_h = h // p n_w = w // p # (n, c, h, w) -\u0026gt; (n, hidden_dim, n_h, n_w) x = self.conv_proj(x) # [2, 768, 14, 14] # (n, hidden_dim, n_h, n_w) -\u0026gt; (n, hidden_dim, (n_h * n_w)) x = x.reshape(n, self.hidden_dim, n_h * n_w) # [2, 768, 196] # (n, hidden_dim, (n_h * n_w)) -\u0026gt; (n, (n_h * n_w), hidden_dim) # The self attention layer expects inputs in the format (N, S, E) # where S is the source sequence length, N is the batch size, E is the # embedding dimension x = x.permute(0, 2, 1) # [2, 196, 768] return x   224x224的图像，16x16的patch，所以一共有196个patch。可以把它们看成一个“句子”，之后送入transformer，就和NLP、Speech里做得一样了。这里特别注意一下self.conv_proj(x)：\n1 2 3  self.conv_proj = nn.Conv2d( in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size )   注意到这里获取序列的方法是二维卷积而不是线性层。\n之后，我们将可学习参数self.class_token拼接到序列之前，投影时将分类token取出来即可用于分类。\n1  self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim)) # torch.Size([2, 1, 768])   使用总结：\n1 2 3 4  img = torch.randn((2, 3, 224, 224)) vitimpl = vit_b_16() output = vitimpl(img) print(output.shape)   ","date":"2022-10-17T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/cv/vit/","title":"ViT"},{"content":"DFT 定义\n$$ X[k] = \\sum_{n=0}^{N-1}x[n] e^{-j2\\pi\\frac{nk}{N}} = \\sum_{n=0}^{N-1}x[n]W_{N}^{nk} $$ 单位根表示\n$$ W_{N}^k = e^{-j2\\pi\\frac{k}{N}} $$\n $N$代表将单位圆平均分成的份数，$k$则相当于旋转的次数。整体上可以看成一个绕原点进行顺时针旋转的单位向量。\n DFT vs DTFT\n$$ X(\\omega)|_{\\omega=2\\pi\\frac{k}{N}} = X[k] $$\nIDFT\n$$ x[r]=\\frac{1}{N}\\sum_{k=0}^{N-1}X[k]e^{j2\\pi\\frac{rk}{N}} $$\n性质\n共轭：$ {\\rm DFT}{x^{\\ast}[n]} = X^{\\ast}[N-k] $\n（时域共轭，频域共轭+循环反转）\n反转：${\\rm DFT}{x[((-n))_N]}=X[((-k))_N]R_N[k]$\n（时域循环反转，频域循环反转）\n对称：${\\rm DFT}{\\Re{x[n]}}=X_e[k]$ ${\\rm DFT}{j\\Im{x[n]}}=X_o[k]$ $\\Re{X[k]}={{\\rm DFT} {x_e[n]}}$ $j\\Im{X[k]}={{\\rm DFT} {x_o[n]}}$\n（推论：实序列的DFT共轭偶对称，即 $X[k] = X^{*}[N-k]$ ）\n对偶：${\\rm DFT}{X[k]}=Nx[((-n))_N]R_N[n]$\n（直观理解：DFTMatrix的$N-k$行和IDFTMatrix的$k$行相同，如果不考虑系数）\n循环位移：${\\rm DFT}{x[((n-m))_N]R_N[n]} = W_N^{mk}X[k]$\n（DTFT的位移变成了DFT的循环位移，$m$理论上可以取非整数）\n循环卷积：${\\rm IDFT}{X[k]Y[k]}=\\sum_{m=0}^{N-1}x[m]y[((n-m))_N]$\n（把传统线性卷积的反转+位移变为循环反转+循环位移。时域理解：将有限序列进行周期延拓后，形成以$N$为周期的序列，在主值区间上进行线性卷积 频域理解：线性卷积以$N$为周期进行混叠后，在$[0,N-1]$截断）\n保范：$\\sum_{n=0}^{N-1}|x[n]|^2=\\frac{1}{N}\\sum_{k=0}^{N-1}|X[k]|^2$\n 补充定义\n循环位移：$ x[((n-m))_N]R_N[n] $ (移位+截断，$m$为向右平移的位数)\n循环反转：$x[((-n))_N]R_N[n]$ (0不变，其余前后反转)\n共轭偶/奇对称：$x_{e/o}[n] = \\frac{1}{2}(x[n]\\pm x^{\\ast}[((-n))]R_N[n])$ (注意把原来的取反变成了循环反转)\n ","date":"2022-10-13T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/digital_signal_processing/dft/","title":"DFT"},{"content":"Resnet method Resnet引入了所谓残差连接的思想：对于每一个层，我们要学习的不是$\\mathcal{H}(x)$，而是$\\mathcal{F}(x) = \\mathcal{H}(x) - x$。换句话讲，我们不需要再学之前层学过的东西，而是要学两者的差距。\n注意Resnet在连接时，存在两种不同的连接方式： $$ y = \\mathcal{F}(x,{W_i})+x \\ $$\n$$ y = \\mathcal{F}(x,{W_i})+W_sx $$\n当跨越维度连接时，有两种可选方案来解决维度不适配的问题：1) 补0 2)投影。消融实验表明，这两种方法对结果没有本质影响。\n不同的Resnet配置如下：\ncode BasicBlock 见resnet.py。\nResnet提供了两种原版结构：BasicBlock和BottleNeck。此处不再深究两者的区别，以默认的BasicBlock为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError(\u0026#39;BasicBlock only supports groups=1 and base_width=64\u0026#39;) if dilation \u0026gt; 1: raise NotImplementedError(\u0026#34;Dilation \u0026gt; 1 not supported in BasicBlock\u0026#34;) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(in_planes=inplanes, out_planes=planes, stride=stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) # residual connection out += identity out = self.relu(out) return out   每一个BasicBlock对应的是这样一个模块：\n其__init__参数中的inplanes和planes分别指代input channel和output channel，一个简单示例如下：\n1 2 3 4 5 6 7 8 9  \u0026gt;\u0026gt; basicblockimpl = BasicBlock(64, 128) \u0026gt;\u0026gt; print(basicblockimpl) BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )   其forward部分如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) # residual connection out += identity out = self.relu(out) return out   注意到其中的残差连接部分。值得注意的是，为了进行维度匹配，forward中还存在self.downsample部分，是一个channel维度不匹配的二维卷积。之后在Resnet的解释中会详细说明：\n1 2 3 4  (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) )   Resnet 观察其forward函数，我们逐个分析其中的模块：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  def _forward_impl(self, x): # [2, 3, 224, 224] x = self.conv1(x) # [2, 64, 112, 112] x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) # [2, 64, 56, 56] x = self.layer1(x) # [2, 64, 56, 56] x = self.layer2(x) # [2, 128, 28, 28] x = self.layer3(x) # [2, 256, 14, 14] x = self.layer4(x) # [2, 512, 7, 7] x = self.avgpool(x) # [2, 512, 1, 1] x = torch.flatten(x, 1) # [2, 512] x = self.fc(x) # [2, 1000] return x   self.conv1(x)使用7x7卷积核，将图片的channel从3变为64，同时特征图长宽缩短一半。\n1  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)     self.maxpool(x)将特征图长宽再缩短一半。\n1  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)     之后是模块的主体部分：\n1 2 3 4 5 6 7  self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])   首先看这4个层中共有的特征：64/128/256/512是输出通道数，layers[i]是通道的层数。\n注意到layer1和其它模块参数的不同之处：无stride。这决定了：\n 每一个layer第一个BasicBlock的状态：  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ## layer 1 (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ## layer 2 (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) )   第一个layer用于缩小特征图和增加channel数（stride=1时除外）。其它的layer保持特征图的尺寸不变。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) )   是否含有downsample层：  1 2 3 4  downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), )   当跨层进行残差连接时，可能会出现维度不匹配的问题，这时候需要进行downsampling以使得维度匹配：\n1 2 3  out: torch.Size([2, 128, 28, 28]) in before: torch.Size([2, 64, 56, 56]) in after: torch.Size([2, 128, 28, 28])   self.avgpool所使用的是torch.nn.AdaptiveAvgPool2d，使用时只需要指定最后两维的尺寸(H_0, W_0)即可。\nself.fc将图片分为1000类。\n最后我们对模型的使用做一个总结（以resnet34为例）：\n1 2 3 4  def resnet34(pretrained=False, progress=True, **kwargs): return _resnet(\u0026#39;resnet34\u0026#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs) # layer1~4\u0026#39;s depth is 3,4,6,3   1 2 3 4  resnet34impl = resnet34() img = torch.randn([2, 3, 224, 224]) out = resnet34impl(img) print(out.shape) # [2, 1000]   ","date":"2022-10-09T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/cv/resnet/","title":"ResNet"},{"content":"小黑前传的名字叫作《追忆似火年华》，不过小黑后传似乎和“似火”并不怎么沾边，索性就叫小黑后传好了，以后有想法再更新一个文艺点的标题。\n一 2022.6.19 考完试的第一天。\n“您当日的听歌时间为8h42min，位居好友榜榜首，恭喜！”\n虽然知道自己网抑云的习惯由来已久（确切地来讲，网易云和QQ音乐都听，而且QQ更多一些），但看到这个数字小黑还是震惊了一下的。其他人排解心魔的方式有社交、有读书、也有游戏，小黑则是网抑云，嗯网抑云。\n小黑的听歌方式，比较奇怪。他不怎么关注日推，也不会从500多首红心中随机抽取音乐，而是这样的：在一个特定的时间段里，他会对歌单里的5~10首歌进行单曲循环，如果在这期间搜罗到了什么好音乐，单曲循环的队列就会被更新。\n 从这个角度讲，这种听歌机制其实很像计算机里面cache的原理——平时单曲循环的歌存在cache里，适当的时候从主存里加载歌曲到cache里再单曲循环\u0026hellip;\n 从这个听歌习惯，似乎也能看到小黑的一点习惯——或者更确切的说，叫秉性——念旧、保守\u0026hellip;\u0026hellip;\n小黑摇了摇头，网抑云不是这么抑的。不过说起来，这个习惯是怎么形成的？\n二 2022.5.22 小黑居然在大二下学期中刷完了一部番，不过是老番，也比较短——《新世纪福音战士》。\n男主似乎和小黑有不少共同点——弱不禁风，胆小，喜欢边喝着酒/饮料边瘫坐着听音乐。\n这里要特别说一下“边喝着酒/饮料边瘫坐着听音乐”，小黑在看到这些镜头时，竟然前所未有地产生了共鸣感——比起那种激励人奋进的鸡汤，小黑似乎对这样的状态更加向往。\n男主有一段台词让他十分印象深刻：\n 你总是逃避自己讨厌的事情。\n我已经很努力地再试了！\n这有什么不好！\n逃避讨厌的事情又有什么不对！\n 小黑感觉自己也在逃避，逃避的是什么？也许不只是眼前的困难——就像科研进度十分缓慢，小黑第一反应想的不是调整状态，多看几篇论文和代码，而是打开手机看eva——美其名曰寻找与自己和解的路子。\n从那以后，耳机便真的成了小黑的耳旁常伴的物品了。\n小黑更想逃避的，也许是真实的自己、阴暗的自己。不过，小黑是什么时候开始思考这些问题的呢？\n三 2022.4.12 这场不大不小的折磨终于结束了。\n身体的折磨是暂时结束了。不过从那以后，小黑似乎就变了一个人——头脑中无用的精神内耗开始变多，头脑里就好像有两个不同的政党在打架，最后也给不出什么具体的执行方案来，生活就这样原地踏步地进行着。\n这造成的直接后果就是：睡眠质量严重下降，上课难以集中精神，科研开始摆烂\u0026hellip;\u0026hellip;不过有一点，小黑始终认为，自己并没有失去对学习的兴趣，只是暂时失去了自己的方向。\n说起方向，小黑有过自己真正的方向吗？从某种意义上，小黑在战略决策上似乎从来只是一株墙头草——小黑从小到大，或者更确切地说，是进了大学之后，努力的出发点就只是“我想成为xxx那样的人”——A的笔记，B的钻研，C的个人魅力\u0026hellip;\u0026hellip;这本是一件好事，但小黑发现自己似乎哪一个人都变不成，而当别人告诉自己要“做自己”时，小黑甚至连“自我”的定义都难以下达——自己追寻了太多别人的足迹，却连自己本来的样子都忘记了。\n那么，自己到底想成为什么样的人？这要从一次谈话说起。\n四 2022.1.18 在入学前，小黑给自己的label是：社恐分子、小镇做题家、局外人。\n一年半后，小黑给自己的label是：孤勇者、想要成为文艺青年的理工男、安静、内向，自己将会在学习和科研的路上一路狂奔。\n但，果真如此吗？\n我与老友聊起了关于自身的定位问题，老友说：其实你一直是一个很有人情味的人，并列举种种事例。\n这时小黑才意识到，自己的内心中，本质上有很多感性的成分。自己也渴望社交、渴望沟通、渴望人与人之间微妙的联结——而不是仅仅一味扎进学术中。\n \u0026ldquo;人有种奇怪的虚荣心，想让别人或自己相信他向往的是真理，但其实他有求于这个世界的是爱。\u0026ldquo;加缪一语道破天机。\n 自己的一些label，似乎硬生生束缚住了自己的手脚。本我，似乎与展现在世人面前的我，以及小黑所期望的我，出现了前所未有的参差。它们相互交织，却又在触及人心最柔软的部分互斥。\n小黑原本以为自己的这些label大概是在高中时期形成的。但另一位老友给了他一个惊人的答案：\n 小学。\n 五 2022.8.4 递归似地分析完了所有问题，接下来要做的，就是要把所有问题层层出栈。只是，这样的思索是存在精确解的吗？如果没有，那这样的思索是无谓的吗？正如小黑不顾繁重的任务和压力，在深夜敲下废话连篇的意识流文章一样，此举意义何在？\n什么才能治好小黑的精神内耗？这个学期的经历似乎告诉小黑：不是忙碌，更不是让自己的身心投入到学习和工作中。\n那是什么？也许是慢慢地与留下太多遗憾地自己和解。但和解的过程，又何尝不是一个精神内耗的过程。\n小黑似乎又一次掉入了无限的递归之中。\n","date":"2022-08-04T01:09:43+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/%E5%B0%8F%E9%BB%91%E5%90%8E%E4%BC%A0/","title":"小黑后传"},{"content":" “我们前后找了几十年以来的毕业生，在十六万人中分析了五百人，得到了结论。结果发现很多结果出乎清华校方的意料。 比如，我们分析了成绩因素，按理说成绩好应该更成功吧？但结果是，成绩对以后的发展并没有影响，就是说，成绩好并不代表发展好，当然也不可能发展差对吧。这是很出乎校方意料的一点。 那社工呢？大家都认为如果做了什么学生会主席、社会工作可能对未来发展有所帮助，结果还是没有关系。没有关系意思是有没有这些优势都不影响以后发展。 那么家庭影响应该足够重要了吧，比如有的家长眼界很高，有先进的观念，培养孩子很有方法，这是个影响因素吗？不是。 地域呢？比如有的同学来自偏远地区，有的在大城市，这是影响因素吗？不是。这些都不是，那我们发现了什么呢？ 我们在二十多个因素里，唯一发现很多人都具有的一个共同点就是，他们都很早地开始思考自己的未来并且着手做准备。\n ","date":"2022-07-28T02:11:43+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/review/","title":"试错，还是错逝？"},{"content":"“失去的二十周” 2022.1.21  “泡沫的顶峰”\n 2022.3.1  “出校理由：北医三院” “原因玄学，大学生，压力大，作息不规律，有心理因素，正常”\n 2022.3.8  “这就是世一大的教学水平吗” 自己的“看家本领”到底是什么？？\n 2022.4.10  第一次大寄\n 2022.4.17  “所以，你们到底做了些什么？”\n 2022.4.23  “搞了半天，还是咱不够卷\u0026hellip;”\n 2022.4.26  第二次大寄 “一段噩梦的结束” “人是会在同一个地方摔两次跟头的”\n 2022.4.28  “今后还会像这般心想事成吗？”\n 2022.4.30  “至少到现在这个阶段我觉得，我看到一个很强的人时，我会这样想：‘我虽然崇拜他，但我并不会说去完全变成他的样子，而只是在想怎么做好自己’” “很遗憾，我目前还做不到这一点”\n 2022.5.2  “残酷な天使のように，少年よ神话になれ！”\n 2022.5.3  一个并不光彩的W\n 2022.5.4  “突然感觉，你清学子要花费很多时间在‘与自己和解’上”\n 2022.5.17  “大家并不是真的有多反感封校和防疫，他们只是想借此机会抒发各自心中的不快罢了”\n 2022.5.23  “千载难逢的机会，你不用吗？” “算了，我怕有后续影响”\n 2022.6.8  “好多人都在讲好大学平台有多好，但是鄙人感觉这个平台并不属于只会埋头干活的人。。”\n 2022.6.12  “有时候选择比努力更重要，以后在社会上更是这样，这也许是这魔幻的一学期带给我们的启示吧”\n 2022.6.14  C楼3层，一个全新的世界\n 2022.6.20  “在一个弱一点的985如果能一开始处于前列，那么你的学习动能是强于在清北挣扎的，是一个向上走的趋势，而且会进入大佬圈子，拿到本校顶尖的资源和信息共享；而清北学渣什么都没有，四年一直走下坡路。这一上一下差距会拉得很大。”\n 2022.6.25  “这他妈就是一个强者恒强的过程。。。”\n So where is the way? ","date":"2022-07-27T23:59:43+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/%E6%97%A5%E5%AF%84/","title":"日寄"},{"content":"Makefile \u0026amp; CMake 想象一下我们有如下C++程序hello.cpp：\n1 2 3 4 5  #include \u0026lt;iostream\u0026gt;int main() { std::cout \u0026lt;\u0026lt; \u0026#34;hello world!\u0026#34; \u0026lt;\u0026lt; std::endl; }   我们需要在终端输入以下指令：\n1  $ g++ hello.cpp -o hello   这时我们就可以生成可执行文件hello。但在实际应用场景中，我们可能会面临如下问题：\n 项目中的.h文件和.cpp文件十分繁多。 各.h文件、.cpp文件的依赖关系十分复杂。 多文件可能会出现重复编译的情况，拖慢编译速度。 \u0026hellip;  为了解决这些问题，makefile和CMake应运而生。\n本讲需要使用到的工具：g++，make，cmake。可以通过以下方式安装：\n1  $ sudo apt-get install g++ make cmake   Makefile makefile文件描述了C/C++工程的编译规则，可以用来指明源文件的编译顺序、依赖关系、是否需要重新编译等，自动化编译C/C++项目（实际上也不止局限于C/C++项目）。\n我们可以考虑以下实例：\n1 2 3 4 5  . ├── invsqrt.cpp ├── invsqrt.h ├── main.cpp └── makefile   makefile如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  CXXFLAGS = -std=c++17 -O2 main: main.o invsqrt.o $(CXX) $(CXXFLAGS) -o $@ $^ main.o: main.cpp invsqrt.h $(CXX) $(CXXFLAGS) -o $@ -c $\u0026lt; invsqrt.o: invsqrt.cpp invsqrt.h $(CXX) $(CXXFLAGS) -o $@ -c $\u0026lt; .PHONY: clean clean: rm main.o invsqrt.o main   我们不需要理解每一段代码的具体含义（之后我们可以看到，我们不需要手动编写makefile，而可以直接通过CMake工具生成makefile），我们只需要了解如何使用makefile：\n 在makefile的同目录下输入make，就可以按照makefile所指定的编译规则自动编译整个工程。 在makefile的同目录下输入make clean，可以删除编译生成的中间文件和可执行文件。 除此之外，你还可以为makefile添加更多指令，此处由于篇幅所限，不再展开介绍。  CMake makefile存在以下问题：\n 语法复杂，代码可读性差，难以维护。 跨平台性差。  在目前的C++工程中，我们多使用CMake来管理项目。\n什么是CMake？CMake是一种跨平台的编译工具，可以用较为简洁易读的语法描述C++项目的编译、链接、安装过程等，在现代C++项目上得到了广泛应用。\n1 第一个CMake项目 CMake的项目文件叫做CMakeLists.txt。其放置位置如下图所示：\n1 2  ├── CMakeLists.txt └── main.cpp   该项目的CMakeLists.txt中需要添加以下内容：\n1 2 3  cmake_minimum_required(VERSION 3.5)project (hello_world)add_executable(hello_world main.cpp)   语法总结1\n cmake_minimum_required(VERSION 3.5) CMake需要的最小版本。CMake的版本可以在命令行中输入cmake --version获取，一般无强制要求。 project(\u0026lt;project_name\u0026gt;) 指定工程名称。 add_executable(\u0026lt;executable_name\u0026gt; \u0026lt;cppfile_name\u0026gt;) 生成可执行文件。   cmake工具的使用一般分为两步 1) 使用CMakeLists.txt生成makefile。 2) 使用makefile自动化编译项目。\n 输入cmake CMakeLists.txt，目录下将会生成一个Makefile文件。 输入make，即可将源代码编译生成可执行文件。此处将会在与CMakeLists.txt相同目录的位置生成一个可执行文件hello_word，输入./hello_word即可运行该可执行文件。 此外，输入make help，你也可以查看使用当前的Makefile所能执行的所有指令，例如make clean（清楚生成的可执行文件和中间文件）。  2 多文件 在平时的程设小作业中，我们习惯将所有的代码都写在一个.cpp文件中。但在实际工程中，为了方便代码复用和运行维护，通常将所有的文件划分为头文件(.h)，模块文件(.cpp)和主程序文件(.cpp)。\n在本节中，我们将在头文件中声明一个计算平方根倒数的函数，在模块文件中实现其主体，然后在主函数中调用它。项目结构如下：\n1 2 3 4 5 6 7  . ├── CMakeLists.txt ├── include │ └── invsqrt.h └── src ├── invsqrt.cpp └── main.cpp    tips: 在C++工程中，我们通常在include/目录下放置头文件，在src/目录下放置源文件。\n 该项目的CMakeLists.txt中需要添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 12  # build part cmake_minimum_required(VERSION 3.5)project(invsqrt)set(SOURCES src/invsqrt.cpp src/main.cpp)add_executable(invsqrt ${SOURCES})target_include_directories(invsqrt PUBLIC ${PROJECT_SOURCE_DIR}/include)# debug part message(\u0026#34;CMAKE_SOURCE_DIR: ${CMAKE_SOURCE_DIR}\u0026#34;)message(\u0026#34;PROJECT_SOURCE_DIR: ${PROJECT_SOURCE_DIR}\u0026#34;)message(\u0026#34;SOURCES: ${SOURCES}\u0026#34;)   语法总结2\n set(\u0026lt;variable\u0026gt; \u0026lt;value\u0026gt;) 设置变量 target_include_directories(\u0026lt;project_name\u0026gt; \u0026lt;INTERFACE|PUBLIC|PRIVATE\u0026gt; \u0026lt;headfile_directory\u0026gt;) 指定所要包含的头文件。 message(\u0026quot;your message\u0026quot;) 在终端打印信息。   这里需要特别说明一下CMake中的变量使用。CMake中的变量分为两种：\n 显式变量：使用set指令定义的变量。 隐式变量：通过其它指令隐式生成的变量。如该项目中会隐式生成PROJECT_SOURCE_DIR变量，默认为CMakeLists.txt所在的文件夹。  CMake中有丰富的变量，用于定义工程目录、编译选项等，此处不做过多展开。想要了解更多，可以参考文末列出的参考文档。\n3 静态库和动态库 有些时候，出于方便复用、防止源码泄露等原因，我们需要将代码封装为静态库和动态库。CMake同样提供了生成静态库和动态库的功能。\n3.1 静态库 在此处，我们将上一小节中计算平方根倒数的程序封装为静态库。项目结构如下：\n1 2 3 4 5 6 7  . ├── CMakeLists.txt ├── include │ └── invsqrt.h └── src ├── invsqrt.cpp └── main.cpp   该项目的CMakeLists.txt中需要添加以下内容：\n1 2 3 4 5 6 7 8 9 10  cmake_minimum_required(VERSION 3.5)project(invsqrt)# create static library add_library(invsqrt_static STATIC src/invsqrt.cpp)target_include_directories(invsqrt_static PUBLIC ${PROJECT_SOURCE_DIR}/include)# create executable add_executable(invsqrt src/main.cpp)target_link_libraries(invsqrt PRIVATE invsqrt_static)   语法总结3\n add_library(\u0026lt;library_name\u0026gt; STATIC \u0026lt;cppfile_name\u0026gt;) 生成静态库 target_link_libraries(\u0026lt;executable\u0026gt; \u0026lt;INTERFACE|PUBLIC|PRIVATE\u0026gt; \u0026lt;library_name\u0026gt;) 指定所要链接的库。   此处我们使用一种更为优雅的生成方式——我们期望将生成的静态库、可执行文件输出到build文件夹里，而不是和主项目混杂在一起。为此我们需要输入以下指令：\n1 2 3 4  $ mkdir build $ cd build $ cmake .. # 使用的是上一层目录的CMakeLists.txt，因此需要输入\u0026#39;..\u0026#39; $ make   我们将会在build/目录下看到静态库libinvsqrt_static.a和可执行文件invsqrt。\n3.2 动态库 项目目录结构同静态库一节。\n该项目的CMakeLists.txt中需要添加以下内容：\n1 2 3 4 5 6 7 8 9 10  cmake_minimum_required(VERSION 3.5)project(invsqrt)# create shared library add_library(invsqrt_shared SHARED src/invsqrt.cpp)target_include_directories(invsqrt_shared PUBLIC ${PROJECT_SOURCE_DIR}/include)# create executable add_executable(invsqrt src/main.cpp)target_link_libraries(invsqrt PRIVATE invsqrt_shared)   语法总结4\n add_library(\u0026lt;library_name\u0026gt; SHARED \u0026lt;cppfile_name\u0026gt;) 生成动态库   同样按照上小节的方法生成项目。我们将会在build/目录下看到动态库libinvsqrt_shared.so和可执行文件invsqrt。\n4 使用第三方库 在实际的C++工程中，我们可能需要链接一些开源的第三方库。CMake也提供了相关的配置方式。我们以谷歌开发的单元测试框架googletest为例：\n googletest的安装方法：\n1 2 3 4 5 6 7 8 9  $ git clone https://github.com/google/googletest.git # or git clone git@github.com:google/googletest.git $ cd googletest $ mkdir build $ cd build $ cmake ../ $ make -j all $ make install # or sudo make install     在默认情况下，与该第三方库相关的头文件将会被放置在/usr/local/include目录下，与该第三方库相关的库文件将会被放置在/usr/local/lib目录下，与该第三方库相关的可执行文件将会被放置在/usr/local/bin目录下。\n 项目结构如下：\n1 2 3 4 5 6 7  . ├── CMakeLists.txt ├── include │ └── mysqrt.h └── src ├── mysqrt.cpp └── main.cpp   1 2 3 4 5 6 7 8 9 10 11 12 13 14  cmake_minimum_required(VERSION 2.6)project(cmake_with_gtest)set(SOURCES src/mysqrt.cpp src/main.cpp)find_package(GTest)message(\u0026#34;GTEST_LIBRARIES: ${GTEST_LIBRARIES}\u0026#34;)message(\u0026#34;GTEST_INCLUDE_DIRS: ${GTEST_INCLUDE_DIRS}\u0026#34;)include_directories(${GTEST_INCLUDE_DIRS} ${PROJECT_SOURCE_DIR}/include)add_executable(cmake_with_gtest ${SOURCES})target_link_libraries(cmake_with_gtest ${GTEST_LIBRARIES} pthread)   语法总结5\n find_package(\u0026lt;package_name\u0026gt;) 查询第三方库的位置。若查找成功，则初始化变量\u0026lt;package_name\u0026gt;_INCLUDE_DIR（第三方库的头文件目录）以及\u0026lt;package_name\u0026gt;_LIBRARIES（第三方库的静态/动态库目录）。   CMake支持的所有第三方库可以在https://cmake.org/cmake/help/latest/manual/cmake-modules.7.html中找到。\n写在最后 CMake还有很多强大的功能：\n 设置C++工程的语言标准、编译优化选项。 层级文件之间CMakeLists.txt的相互调用，以便应用于目录层级更加复杂的C++工程。 对生成的库、可执行文件等进行安装。 \u0026hellip;  略过上述内容不会对我们的教学产生太大影响。感兴趣的同学可以参考以下文章：\n CMake官方文档 cmake-examples 该GitHub仓库中有很多开箱即用的CMake实例。 为什么编译c/c++要用makefile，而不是直接用shell呢？ 这篇博文详细地阐述了使用makefile的动机和意义（\\xfgg/）。 跟我一起写Makefile Makefile教程。从中大家也可以看出Makefile的语法十分不友好\u0026hellip;  ","date":"2022-06-23T20:47:58+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/cmake/","title":"Makefile \u0026 CMake"},{"content":"目录 [TOC]\n前言 在程设课上，我们运行一个C++程序的步骤通常是这样的：打开Visual Studio 2008，在文件中写好程序，然后点击“开始调试”或者“开始执行（不调试）”，一个黑色的方框就会弹出来。\n实际上，从C++源代码文件到可执行文件的过程是十分复杂的，Visual Studio等现代化的IDE（Integrated Development Environment，集成开发环境）掩盖了程序构建的复杂流程。本节我们就以linux平台上的C++程序为例，简略介绍C++工程中的一些概念。\n为了获得更好的实验体验，建议大家使用linux操作系统（虚拟机或WSL）来运行本节的程序。\n本讲需要使用到的工具有：gcc，g++。可以通过以下方式安装：\n1  $ sudo apt-get install gcc g++   从.cpp到.exe —— C/C++程序的构建过程 C/C++程序生成一个可执行文件的过程可以分为4个步骤：预处理（Preprocessing）、编译（Compiling）、汇编（Assembly）和链接（Linking）。之后我们将通过演示实例介绍每一步发生的故事。\n编译工具 针对不同的应用场景和平台，各大厂家设计了不同的C++编译工具。\n  MSVC（Microsoft Visual C++）：MSVC是微软公司开发的C++开发工具，我们程设课上使用的Visual Studio就内置了MSVC。\n  GCC（GNU Compiler Collection）：GCC是由GNU（GNU\u0026rsquo;s Not Unix）开发的一套编译工具，支持C、C++、Fortran、Go等一系列语言。本教程中我们使用的编译工具就是GCC。\nGCC提供给用户的前端程序为gcc（针对C）和g++（针对C++）。它们的区别详见gcc vs g++。\n  此外还有Clang、NVCC等编译工具。不同的编译工具对C++的支持不尽然相同，此处不再赘述。\n  1 预处理 C++程序在预处理阶段会执行以下操作：宏的替换、头文件的插入、删除条件编译中不满足条件的部分。\n1  $ g++ –E invsqrt.cpp –o invsqrt.i   2 编译 C++程序在编译阶段会将C++文件转换为汇编文件。\n1 2 3 4  # from .i file $ g++ –S invsqrt.i –o invsqrt.s # from .cpp file $ g++ –S invsqrt.cpp –o invsqrt.s   3 汇编 汇编语言文件经过汇编，生成目标文件.o文件（二进制文件，机器码），每一个源文件都对应一个目标文件。\n1 2 3 4 5  # from .s file $ g++ –c invsqrt.s –o invsqrt.o # from .cpp file $ g++ –c invsqrt.cpp –o invsqrt.o $ g++ -c main.cpp -o main.o    生成的invsqrt.o和main.o文件不能直接打开，你可以使用readelf -a \u0026lt;object file\u0026gt;阅读其信息。\n 4 链接 每个源文件对应的目标.o文件被链接起来，就生成一个可执行程序文件。\n1  $ g++ invsqrt.o main.o -o main.exe   当然，如果想要使用.cpp文件一步到位生成可执行文件，可以使用以下指令：\n1  $ g++ invsqrt.cpp main.cpp -o main.exe    实际上在linux系统上，可执行文件一般是没有后缀名的。此处为了方便说明添加了.exe文件。\n  语法总结\ng++和gcc工具中使用的一些命令行参数：\n  -E 只进行预处理\n  -S 只进行编译\n  -c 只生成目标文件\n  -o \u0026lt;file\u0026gt; 指定输出文件的名称。我们约定：.i为预处理后的文件，.s为汇编文件，.o为目标文件。\n   静态库和动态库 出于便于复用、封装细节或防止源码泄露等原因，在实际应用过程中，我们需要把C++源码封装为库(library)。\n根据其行为不同，可以将库分为静态库(static library)和动态库(shared library)。\n静态库 静态库的代码在编译的过程中，会被直接载入到可执行文件中。这样做的好处是：可执行文件在执行时，不再需要静态库本身。但缺点也显而易见：生成的可执行文件的体积会比较大。\nlinux平台下静态库的后缀通常为.a，命名方式通常为libxxx.a;windows平台下静态库的后缀通常为.lib。\n在linux平台上生成静态库，并使用动态库链接形成可执行文件的方法为：\n1 2 3 4  # generate static lib $ ar crv libinvsqrt.a invsqrt.cpp # link to generate the executable file $ g++ -static main.cpp -L . -linvsqrt -o main_shared.exe   动态库 动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入。这就带来了一个明显的好处：不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例，减小了各个模块之间的耦合程度，也减小了可执行文件的体积。然而，这也要求用户的电脑上需要同时拥有可执行文件和动态库，也有可能因为版本不匹配等问题发生DLL Hell等问题。\nlinux平台下静态库的后缀通常为.so，命名方式通常为libxxx.so;windows平台下静态库的后缀通常为.dll。\n在linux平台上生成动态库，并使用动态库链接形成可执行文件的方法为：\n1 2 3 4  # generate shared lib $ g++ invsqrt.cpp -I ./ -fPIC -shared -o libinvsqrt.so # link to generate the executable file $ g++ main.cpp -L . -linvsqrt -o main_shared.exe   写在最后 由于时间所限，还有很多有趣的内容我们没有涉及：\n gcc/g++有着丰富的命令行参数设置，比如程序优化、C/C++语言标准设置等。 在本节中，我们只介绍了如何在linux平台上生成和使用静态库、动态库。实际上，利用Visual Studio也可以便捷地在windows平台上生成静态库、动态库。 \u0026hellip;  略过上述内容不会对我们的教学产生太大影响。感兴趣的同学可以参考以下文档：\n GCC官网 learn cpp 一份新手友好的C++入门文档。 演练：使用Visual Studio创建并使用静态库 演练：使用Visual Studio创建并使用动态库  ","date":"2022-06-23T20:47:58+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/compile/","title":"编译、链接、静态库、动态库"},{"content":"C# async 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118  using System; using System.Collections.Generic; using System.Threading.Tasks; namespace AsyncBreakfast { // These classes are intentionally empty for the purpose of this example. They are simply marker classes for the purpose of demonstration, contain no properties, and serve no other purpose. internal class Bacon { } internal class Coffee { } internal class Egg { } internal class Juice { } internal class Toast { } class Program { static async Task Main(string[] args) { Coffee cup = PourCoffee(); Console.WriteLine(\u0026#34;coffee is ready\u0026#34;); var eggsTask = FryEggsAsync(2); // save the task  var baconTask = FryBaconAsync(3); var toastTask = MakeToastWithButterAndJamAsync(2); var breakfastTasks = new List\u0026lt;Task\u0026gt; { eggsTask, baconTask, toastTask }; while (breakfastTasks.Count \u0026gt; 0) { Task finishedTask = await Task.WhenAny(breakfastTasks); // use this code to await an async process if (finishedTask == eggsTask) { Console.WriteLine(\u0026#34;eggs are ready\u0026#34;); } else if (finishedTask == baconTask) { Console.WriteLine(\u0026#34;bacon is ready\u0026#34;); } else if (finishedTask == toastTask) { Console.WriteLine(\u0026#34;toast is ready\u0026#34;); } breakfastTasks.Remove(finishedTask); } Juice oj = PourOJ(); Console.WriteLine(\u0026#34;oj is ready\u0026#34;); Console.WriteLine(\u0026#34;Breakfast is ready!\u0026#34;); } // combine an async task and a simple task static async Task\u0026lt;Toast\u0026gt; MakeToastWithButterAndJamAsync(int number) { var toast = await ToastBreadAsync(number); ApplyButter(toast); ApplyJam(toast); return toast; } private static Juice PourOJ() { Console.WriteLine(\u0026#34;Pouring orange juice\u0026#34;); return new Juice(); } private static void ApplyJam(Toast toast) =\u0026gt; Console.WriteLine(\u0026#34;Putting jam on the toast\u0026#34;); private static void ApplyButter(Toast toast) =\u0026gt; Console.WriteLine(\u0026#34;Putting butter on the toast\u0026#34;); private static async Task\u0026lt;Toast\u0026gt; ToastBreadAsync(int slices) { for (int slice = 0; slice \u0026lt; slices; slice++) { Console.WriteLine(\u0026#34;Putting a slice of bread in the toaster\u0026#34;); } Console.WriteLine(\u0026#34;Start toasting...\u0026#34;); await Task.Delay(3000); Console.WriteLine(\u0026#34;Remove toast from toaster\u0026#34;); return new Toast(); } private static async Task\u0026lt;Bacon\u0026gt; FryBaconAsync(int slices) { Console.WriteLine($\u0026#34;putting {slices} slices of bacon in the pan\u0026#34;); Console.WriteLine(\u0026#34;cooking first side of bacon...\u0026#34;); await Task.Delay(3000); for (int slice = 0; slice \u0026lt; slices; slice++) { Console.WriteLine(\u0026#34;flipping a slice of bacon\u0026#34;); } Console.WriteLine(\u0026#34;cooking the second side of bacon...\u0026#34;); await Task.Delay(3000); Console.WriteLine(\u0026#34;Put bacon on plate\u0026#34;); return new Bacon(); } private static async Task\u0026lt;Egg\u0026gt; FryEggsAsync(int howMany) { Console.WriteLine(\u0026#34;Warming the egg pan...\u0026#34;); await Task.Delay(3000); Console.WriteLine($\u0026#34;cracking {howMany} eggs\u0026#34;); Console.WriteLine(\u0026#34;cooking the eggs ...\u0026#34;); await Task.Delay(3000); Console.WriteLine(\u0026#34;Put eggs on plate\u0026#34;); return new Egg(); } private static Coffee PourCoffee() { Console.WriteLine(\u0026#34;Pouring coffee\u0026#34;); return new Coffee(); } } }   ","date":"2022-06-22T10:40:10+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/csharp_async/","title":"Csharp async"},{"content":"dotnet build 生成一个.NET工程。目录下必须有.sln文件或.csproj文件。\ndotnet publish 生成一个.NET工程的发布版本。目录下必须有.sln文件或.csproj文件。\ndotnet run 启动一个.NET工程。目录下必须有.csproj文件。\ndotnet clean 清除一个.NET工程的所有编译文件。目录下必须有.sln文件或.csproj文件。\ndotnet list package 查看一个.NET工程的所有依赖包。目录下必须有.sln文件或.csproj文件。\ndotnet list reference 列举项目的所有依赖项。目录下必须有.csproj文件。\ndotnet add package \u0026lt;PACKAGE_NAME\u0026gt; --version \u0026lt;VERSION\u0026gt; 添加一个依赖包。目录下必须有.csproj文件。\n publish命令和build命令最大的区别在于：publish命令将应用程序的依赖项，从NuGet缓存复制到输出文件夹中。\n ","date":"2022-06-21T17:59:46+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/csharp_on_linux/","title":"Csharp on Linux"},{"content":"Protobuf 目录 [TOC]\n数据的传输与解析——浅谈序列化与反序列化 在网络通信的过程中，服务器端和客户端之间常常需要进行对象的传输。对象中常常含有不同的变量：\n 整数 字符串 数组 数组对象 \u0026hellip;  那么我们如何正确地进行这种传递呢？要想实现对象的传输，在发送端我们需要使用一定的规则，将对象转换为具体的字节数组，这就是序列化(serialization)；而在接受端再以这种规则将字节数组还原为对象，这就是反序列化(deserialization)。\n常见的序列化-反序列化协议有XML、JSON、Protobuf。\n XML(eXtensible Markup Language，可扩展标记语言)使用标签\u0026lt;xx\u0026gt;和\u0026lt;/xx\u0026gt;来区隔不同的数据。 JSON(JavaScript Object Notation，JavaScript对象简谱)使用JavaScript构造对象的方法来存储、传输数据。 Protobuf(Protocol Buffers)是Google公司开源跨平台的序列化数据结构的协议。  我们通过一个实例说明三者的差异。我们不妨定义以下对象：\n1 2 3 4 5 6 7 8 9 10 11 12  #include \u0026lt;string\u0026gt; class Helloworld { int id; std::string name; } int main() { Helloworld helloworld(101, \u0026#34;hello\u0026#34;); }   使用XML序列化该对象：\n1 2 3 4  \u0026lt;helloworld\u0026gt; \u0026lt;id\u0026gt;101\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;hello\u0026lt;/name\u0026gt; \u0026lt;/helloworld\u0026gt;   使用JSON序列化该对象：\n1 2 3 4  { \u0026#34;id\u0026#34;: 101, \u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34; }   使用Protobuf序列化该对象（16进制格式）：\n1  08 65 12 06 48 65 6C 6C 6F 77   根据上述实例，我们可以用一张表格总结三者的差异：\n    XML JSON Protobuf     数据存储格式 文本 文本 二进制   可读性 好 较好 差   存储空间 大 较大 小   序列化/反序列速度 慢 慢 快   侧重点 数据结构化 数据结构化 数据序列化     本节我们将重点介绍Protobuf的使用方法。但XML及其各种变体（如HTML、XAML）和JSON也在软件部的后续开发中有着广泛应用。感兴趣的同学可以参考相关资料了解XML和JSON的更多使用方法。\n protobuf的安装 protobuf可以通过以下方式安装（参考自Protobuf C++ Installation）\n1 2 3 4 5 6 7 8 9 10 11 12  $ sudo apt-get install autoconf automake libtool curl make g++ unzip # 安装所需要的工具包 $ git clone https://github.com/protocolbuffers/protobuf.git # 若网络不佳，可以将指令换为 git clone https://gitee.com/mirrors/protobuf_source.git ./protobuf $ cd protobuf # (optional) git submodule update --init --recursive $ git checkout 3.20.x # 根据版本需求选择不同的分支 $ ./autogen.sh $ ./configure $ make -j$(nproc) $ sudo make install $ sudo ldconfig   以上操作会将protoc可执行文件（后续教程会介绍其使用方法）以及与protobuf相关的头文件、库安装至本机。在终端输入protoc，若输出提示信息，则表示安装成功。\nproto文件 基础使用 在使用protobuf时，我们首先需要在.proto文件中将需要被序列化的数据结构进行定义。\n一个.proto文件示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // import \u0026#34;other_protos.proto\u0026#34;; // 如果需要引用其它的protobuf文件，可以使用import语句。 syntax = \u0026#34;proto3\u0026#34;; // 指定protobuf遵循的语法格式是proto2还是proto3。在本教程和之后的开发中，我们都使用proto3语法格式。 package student; // 包名声明。如在本例中，proto文件生成的类都会被放在namespace student中，这一举措的意义在于防止命名冲突 enum Sex // 自定义枚举类型 { MALE = 0; FEMALE = 1;}message Course // protobuf中，使用message定义数据结构，类似于C中的结构体 { int32 credit = 1; string name = 2;}message StudentInfo{ // 变量声明格式 \u0026lt;限定修饰符\u0026gt; \u0026lt;数据类型\u0026gt; \u0026lt;变量名\u0026gt;=id  int32 age = 1; string name = 2; Sex sex = 3; repeated Course courses = 4; // repeated表示重复（数组），本例也表明message可以嵌套message }  protobuf语法标准 protobuf有两套语法标准：proto2和proto3，两套语法不完全兼容。我们可以使用syntax关键字指定protobuf遵循的语法标准。\npackage 为了防止命名冲突，protobuf文件中可以声明包名（package）。具体效果将在后续章节介绍。\n编号 消息定义中的每个字段都有一个唯一的编号，从1开始。这些字段号用于识别你在二进制格式消息中的信息。\n一个常见的约定是，我们会将经常使用的字段编号为1-15，不常用的字段编号为16以上的数字，因为1-15的编号编码仅需要1 byte，这样可以减小字节流的体积。\n数据类型 Protobuf中常见的基础数据类型与若干编程语言的对应关系如下：\n   proto Type C++ Type Python Type C# Type     double double float double   float float float float   int32 int32 int int   int64 int64 int/long long   uint32 uint32 int/long uint   uint64 uint64 int/long ulong   sint32 int32 int int   sint64 int64 int/long long   fixed32 uint32 int/long uint   fixed64 uint64 int/long ulong   sfixed32 int32 int int   sfixed64 int64 int/long long   bool bool bool bool   string string str/unicode string   bytes string str (Python 2) bytes (Python 3) ByteString    更多语言的对应关系参看Protobuf scalar types。\n此外，Protobuf还支持使用enum关键字定义枚举类型。每个枚举定义都必须包含一个映射到0的常量作为枚举的默认值。\n为了尽可能多地压缩数据，Protobuf对各数据类型地默认值做了以下处理：\n numeric types: 0 bool: false string: 空字符串 byte: 空字节 enum: 第一个定义的枚举值（0） message: 取决于目标编程语言  repeated repeated关键字可以定义重复多次的信息（即数组），其顺序是有序的。\n命名法 为了便于阅读，protobuf规定了一系列命名法：\n message、enum采用大驼峰命名法，如message StudentInfo。 字段采用下划线分割法，且全部小写，如string student_name。 枚举值采用下划线分割法，且全部大写，如FIRST_VALUE。  进阶使用 protobuf中还有一些高级语法：\noneof 如果你有一个信息，它可能包含若干种字段，并且最多只有一个字段会同时被设置（回忆C/C++中的联合体union），你可以使用oneof字段来节省空间。\noneof块中可以定义除了map字段（后续会讲到）和repeated字段外的所有类型字段。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  syntax = \u0026#34;proto3\u0026#34;;package oneof_demomessage MessageA{\tstring name_a = 1;}message MessageOneof{\toneof test_oneof\t{\tstring name = 1;\tMessageA message_a = 2;\t}}  map map字段可以定义关联映射类型（类似于Python中的字典dict()）。\nmap字段的定义方式如下：map\u0026lt;key_type, value_type\u0026gt; map_field = N;。其中，key_value可以为整数类型或字符串类型，value_type为除map类型的任意类型。\n1 2 3 4 5 6 7  syntax = \u0026#34;proto3\u0026#34;;package map_demomessage StudentInfo{\tmap\u0026lt;int32,string\u0026gt; id_name_pairs = 1;}  除此之外，protobuf中还有很多高阶语法：\n Any 保留字段（Reserved Values） 嵌套类型（Nested Types） \u0026hellip;  此处由于篇幅所限，我们不做过多展开。\n使用proto文件进行序列化和反序列化 生成目标语言文件 编写好的protobuf文件不能直接应用于工程中，我们需要使用protoc工具生成对应的文件（以C++和Csharp为例）：\n1 2 3  $ protoc --help # 查看使用方法 $ protoc test.proto --cpp_out=. # 在当前目录下生成.cpp文件和.h文件 $ protoc test.proto --csharp_out=. # 在当前目录下生成.cs文件   若使用--cpp_out选项，则会生成\u0026lt;protobuf_name\u0026gt;.pb.h文件和\u0026lt;protobuf_name\u0026gt;.pb.cc文件；若使用--csharp_out选项，则会生成\u0026lt;protobuf_name\u0026gt;.cs文件。生成的文件中会将proto文件中定义的message转换为对应的类，供目标语言程序使用。\nC++ 在C++程序中使用protobuf工具的例程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  #include \u0026lt;iostream\u0026gt;#include \u0026lt;fstream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;google/protobuf/message.h\u0026gt; // for protobuf #include \u0026#34;test.pb.h\u0026#34; // for protobuf source file int main() { // 可以看到，protobuf文件中的信息都被封装在namespace student中，这是之前protobuf中的`package`语法所规定的。  // 1. 如何实例化一个proto文件中定义的类  student::StudentInfo student1; // 2. 如何设置类的各个属性  // a. 添加单一字段：使用set_\u0026lt;xxx\u0026gt;()语句  student1.set_age(18); student1.set_name(\u0026#34;Alice\u0026#34;); student1.set_sex(student::Sex::female); // b. 添加repeated字段：使用add_\u0026lt;xxx\u0026gt;()语句  student::Course* course1 = student1.add_courses(); course1 -\u0026gt; set_name(\u0026#34;calculus\u0026#34;); course1 -\u0026gt; set_credit(5); student::Course* course2 = student1.add_courses(); course2 -\u0026gt; set_name(\u0026#34;Fundamentals of Electronic Circuits and System\u0026#34;); course2 -\u0026gt; set_credit(2); // 3. 如何使用类的各个属性：使用\u0026lt;xxx\u0026gt;()语句  std::cout \u0026lt;\u0026lt; \u0026#34;----------------student info----------------\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; \u0026#34;age: \u0026#34; \u0026lt;\u0026lt; student1.age() \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; \u0026#34;name: \u0026#34; \u0026lt;\u0026lt; student1.name() \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; \u0026#34;sex (0:male, 1:female): \u0026#34; \u0026lt;\u0026lt; (int)student1.sex() \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; \u0026#34;courses: \u0026#34; \u0026lt;\u0026lt; std::endl; for(int i = 0;i\u0026lt;student1.courses_size();i++) { std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;. \u0026#34; \u0026lt;\u0026lt; \u0026#34;name: \u0026#34; \u0026lt;\u0026lt; student1.courses(i).name() \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; \u0026#34;credit: \u0026#34; \u0026lt;\u0026lt; student1.courses(i).credit() \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026#34;--------------------------------------------\u0026#34; \u0026lt;\u0026lt; std::endl; // 4. 序列化  std::cout \u0026lt;\u0026lt; \u0026#34;serialize to file.\u0026#34; \u0026lt;\u0026lt; std::endl; std::fstream output(\u0026#34;./output\u0026#34;, std::ios::out | std::ios::binary ); student1.SerializeToOstream(\u0026amp;output); // 序列化为流  std::cout \u0026lt;\u0026lt; \u0026#34;serialize to array.\u0026#34; \u0026lt;\u0026lt; std::endl; size_t size = student1.ByteSizeLong(); unsigned char* data = new unsigned char [size]; student1.SerializeToArray(data, student1.ByteSizeLong()); // 序列化为数组  // 5. 反序列化和debug  std::cout \u0026lt;\u0026lt; \u0026#34;deserialize from array.\u0026#34; \u0026lt;\u0026lt; std::endl; student::StudentInfo studentInfoFromArray; std::cout \u0026lt;\u0026lt; std::endl; studentInfoFromArray.ParseFromArray(data, size); std::cout \u0026lt;\u0026lt; studentInfoFromArray.DebugString() \u0026lt;\u0026lt; std::endl; // 输出字符串化的信息 }   需要指出的是，想要成功生成可执行文件，需要链接protobuf的静态库和动态库。在linux系统上应用使用到protobuf的C++工程，最好的方法是使用CMake。在本例中，库的依赖关系由CMake工具处理。\nCsharp 在Csharp程序中使用protobuf工具的例程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  using System; using System.IO; using Google.Protobuf; using Student; namespace example { class Program { static void Main(string[] args) { // 1. 如何实例化一个proto文件中定义的类  var student1 = new StudentInfo(); // 2. 如何设置类的各个属性 // a. 添加单一字段（回忆Csharp一讲中的“字段”） student1.Age = 18; student1.Name = \u0026#34;Alice\u0026#34;; student1.Sex = Sex.Female; // b. 添加repeated字段（使用Add()方法） var course1 = new Course(); course1.Name = \u0026#34;calculus\u0026#34;; course1.Credit = 5; student1.Courses.Add(course1); var course2 = new Course(); course2.Name = \u0026#34;Fundamentals of Electronic Circuits and System\u0026#34;; course2.Credit = 2; student1.Courses.Add(course2); // 3. 如何使用类的各个属性（回忆Csharp一讲中的“字段”） Console.WriteLine(\u0026#34;----------------student info----------------\u0026#34;); Console.WriteLine($\u0026#34;age: {student1.Age}\u0026#34;); Console.WriteLine($\u0026#34;name: {student1.Name}\u0026#34;); Console.WriteLine($\u0026#34;sex (0:male, 1:female): {student1.Sex}\u0026#34;); Console.WriteLine($\u0026#34;courses: \u0026#34;); foreach (Course course in student1.Courses) { Console.WriteLine($\u0026#34;name: {course.Name} credit: {course.Credit}\u0026#34;); } // 4. 序列化 Console.WriteLine(\u0026#34;serialize to array.\u0026#34;); byte[] data = new byte[student1.CalculateSize()]; MemoryStream ostream = new MemoryStream(); using (CodedOutputStream output = new CodedOutputStream(ostream, true)) { student1.WriteTo(output); output.Flush(); } data = ostream.ToArray(); // 5. 反序列化和debug Console.WriteLine(\u0026#34;deserialize from array.\u0026#34;); var student2 = new StudentInfo(); MemoryStream istream = new MemoryStream(data); using (CodedInputStream input = new CodedInputStream(istream)) { student2?.MergeFrom(input); } Console.WriteLine(student2); } } }   在Csharp程序中，需要在NuGet程序包中搜索并下载Google.Protobuf安装包。\n 补充说明：如何在Visual Studio中使用NuGet为Csharp程序安装第三方库？\nNuGet是一个自由开源软件包管理系统，作为Visual Studio的一个扩展，可以简化在Visual Studio中添加、更新和删除库的操作。\n我们在开发Csharp程序时不可避免地要用到第三方库，NuGet是一种很好用的工具。以下将以protobuf为例简要介绍NuGet的使用。\n  右键项目，点击“管理NuGet程序包”。\n  点击“浏览”，搜索你想要安装的包名。可以根据项目所需要切换不同的版本。\n  点击安装。在编辑器内输入using Google.Protobuf，若无报错，说明安装成功。\n   写在最后 由于篇幅所限，我们仍然有许多内容没有展开：\n protobuf编码之varint/zigzag protobuf为什么可以获得如此高效的编码效果？这涉及到其底层算法——varint和zigzag算法。 proto2语法和proto3语法的区别。 \u0026hellip;  略去上述内容不会对我们的教学产生太大影响，感兴趣的同学可以参考Protobuf官方文档学习更多知识。\n","date":"2022-06-20T20:07:09+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/protobuf/","title":"Protobuf"},{"content":"Parameters Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator.\n简单来讲，nn.Parameters用于注册可供训练的参数。e.g:\n1 2 3 4 5 6  In [6]: a = torch.nn.Parameter(torch.Tensor([0])) In [7]: a Out[7]: Parameter containing: tensor([0.], requires_grad=True)   nn.Parameter默认requires_grad=True\n一些常用参量（用于更新参数等）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  \u0026gt;\u0026gt; class Net(torch.nn.Module): ...: def __init__(self): ...: super().__init__() ...: self.linear_1 = torch.nn.Linear(3,2) ...: self.linear_2 = torch.nn.Linear(2,1) \u0026gt;\u0026gt; net = Net() \u0026gt;\u0026gt; net.modules \u0026gt;\u0026gt; \u0026lt;bound method Module.modules of Net( (linear_1): Linear(in_features=3, out_features=2, bias=True) (linear_2): Linear(in_features=2, out_features=1, bias=True) )\u0026gt; \u0026gt;\u0026gt; net.named_modules \u0026gt;\u0026gt; \u0026lt;bound method Module.named_modules of Net( (linear_1): Linear(in_features=3, out_features=2, bias=True) (linear_2): Linear(in_features=2, out_features=1, bias=True) )\u0026gt; \u0026gt;\u0026gt; net.state_dict() \u0026gt;\u0026gt; OrderedDict([(\u0026#39;linear_1.weight\u0026#39;, tensor([[-0.5300, -0.5476, 0.0943], [-0.2907, 0.1068, 0.5465]])), (\u0026#39;linear_1.bias\u0026#39;, tensor([ 0.3375, -0.4314])), (\u0026#39;linear_2.weight\u0026#39;, tensor([[ 0.0114, -0.2185]])), (\u0026#39;linear_2.bias\u0026#39;, tensor([0.1354]))]) \u0026gt;\u0026gt; net.parameters \u0026gt;\u0026gt; \u0026lt;bound method Module.parameters of Net( (linear_1): Linear(in_features=3, out_features=2, bias=True) (linear_2): Linear(in_features=2, out_features=1, bias=True) )\u0026gt; \u0026gt;\u0026gt; net.lambda1 = torch.nn.Parameter(torch.randn((1,1))) \u0026gt;\u0026gt; net.lambda1 \u0026gt;\u0026gt; Parameter containing: tensor([[-0.0876]], requires_grad=True) \u0026gt;\u0026gt; net.state_dict() \u0026gt;\u0026gt; OrderedDict([(\u0026#39;lambda1\u0026#39;, tensor([[-0.0876]])), (\u0026#39;linear_1.weight\u0026#39;, tensor([[-0.5300, -0.5476, 0.0943], [-0.2907, 0.1068, 0.5465]])), (\u0026#39;linear_1.bias\u0026#39;, tensor([ 0.3375, -0.4314])), (\u0026#39;linear_2.weight\u0026#39;, tensor([[ 0.0114, -0.2185]])), (\u0026#39;linear_2.bias\u0026#39;, tensor([0.1354]))])   optimizer 1 2 3 4 5 6 7 8 9 10 11  \u0026gt;\u0026gt; optimizer = torch.optim.Adam(net.parameters(), lr=0.01) \u0026gt;\u0026gt; optimizer.state_dict() \u0026gt;\u0026gt; {\u0026#39;state\u0026#39;: {}, \u0026#39;param_groups\u0026#39;: [{\u0026#39;lr\u0026#39;: 0.01, \u0026#39;betas\u0026#39;: (0.9, 0.999), \u0026#39;eps\u0026#39;: 1e-08, \u0026#39;weight_decay\u0026#39;: 0, \u0026#39;amsgrad\u0026#39;: False, \u0026#39;params\u0026#39;: [0, 1, 2, 3, 4]}]} \u0026gt;\u0026gt; optimizer.state \u0026gt;\u0026gt; defaultdict(dict, {})   ","date":"2022-06-20T20:07:09+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/pytorch/","title":"Protobuf"},{"content":"面向对象程序设计基础 目录 [TOC]\n在大一的程设课上，我们系统学习了C++的语法，掌握了一些编写小型程序的技能。实际上，要想写出一个可读性好、可复用、鲁棒性强的程序，掌握一些基本的设计原则是十分必要的。\n本讲的内容并不针对具体的某一语言，而且相比之前的一些内容，本讲的知识更需要在长期的实践中“内化”；与此同时，与软件工程相关的理论博大精深，本讲仅仅挑选一些代表性的原则，只能带领大家入门，想要了解更多还需要仔细阅读文末提供的书单~\nKISS KISS代表着“Keep It Simple and Stupid”。KISS原则指出，简单性应该是软件开发的主要目标，应该避免不必要的复杂性。\n不过，如何界定“简单”？KISS原则指出，为了保证代码的灵活性和可扩展性，我们可能不得不增加代码的复杂度。但除此之外，在这种问题固有复杂性的基础之上增加自制的复杂性，是十分不明智的做法——程序并非程序员炫技的场所，而应该是一件简约的艺术品。\n一言以概之：如无必要，勿增实体。\nLoose Coupling⭐ Loose Coupling，即松耦合原则。这一原则指出：模块与模块之间的耦合（即相互关联的程度）应该越小越好，或者说，它们应该尽可能少地感知到对方的存在。\n举一个例子吧（本例选自 Clean C++ 一书）：\n考虑你有一台电灯，和一个用于控制电灯的开关：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  class Lamp { public: void on() { } void off() { } } class Switch { public: Switch(Lamp\u0026amp; lamp): lamp(lamp) {} void toggle() { if (state) { state = false; lamp.off(); } else { state = true; lamp.on(); } } }   在这样的设计方法下，开关可以工作，但可能会带来一个问题：Switch类中包含了Lamp类的引用，Switch类与Lamp类之间存在着强耦合关系——Switch类可以感知到Lamp类的存在。\n这种写法不仅不符合常理，而且不便于维护和扩展：试想，如果我们想要用开关控制电扇、充电器等其它电器该怎么办？难道我们需要分别设计SwitchForLamp、SwitchForFan、SwitchForCharger类吗？\n如何解决这类耦合问题？一个方法是：将两个类之间相关联的部分抽象成一个接口（interface），第二个类此时不需要包含第一个类的实例或引用，而只需要对接口负责，从而降低耦合度，提高程序的可扩展性。\n以上程序可以改写如下（在C++中，接口可以使用虚基类实现）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  #include \u0026lt;iostream\u0026gt; class Switchable { public: virtual void on() = 0; virtual void off() = 0; }; class Switch { public: Switch(Switchable\u0026amp; switchable) : Switchable(switchable) {} void toggle() { if (state) { state = false; switchable.off(); } else { state = true; switchable.on(); } } private: Switchable\u0026amp; switchable; bool state {false}; }; class Lamp: public Switchable { public: void on() override { std::cout \u0026lt;\u0026lt; \u0026#34;Lamp is on!\u0026#34; \u0026lt;\u0026lt; std::endl; } void off() override { std::cout \u0026lt;\u0026lt; \u0026#34;Lamp is off!\u0026#34; \u0026lt;\u0026lt; std::endl; } }; class Fan: public Switchable { public: void on() override { std::cout \u0026lt;\u0026lt; \u0026#34;Fan is on!\u0026#34; \u0026lt;\u0026lt; std::endl; } void off() override { std::cout \u0026lt;\u0026lt; \u0026#34;Fan is off!\u0026#34; \u0026lt;\u0026lt; std::endl; } }; int main() { Lamp lamp; Switch switch1(lamp); switch1.toggle(); switch1.toggle(); Fan fan; Switch switch2(fan); switch2.toggle(); switch2.toggle(); }   在以上更改中，开关与其它电器耦合的部分被抽象为一个接口Switchable，开关只需要对这一接口进行操作，避免了开关与具体电器类的耦合。\nSOLID⭐ SOLID是以下五大面向对象设计原则的缩写：\n 单一功能原则（Single Responsibility Principle，SRP） 开闭原则（Open Closed Principle，OCP） 里氏替换原则（Liskov Substitution Principle，LSP） 接口隔离原则（Interface Segregation Principle，ISP） 依赖反转原则（Dependency Inversion Principle，DIP）。  单一功能原则 单一功能原则指出，每个软件单元（类、函数等），应该只有一个单一的、定义明确的责任。\n如何界定单一责任？一个比较普适的定义是，改变该软件单元只能有一个原因。如果有多个原因，那么该单元就应该拆分。\n开闭原则 开闭原则指出，软件单元（类、函数等）应该对于扩展是开放的，但是对于修改是封闭的。\n具体来讲，如果我们需要给一个软件添加新的功能，我们通常不建议修改源码，而更加建议通过继承的方式。\n里氏替换原则⭐ 里氏原则指出，派生类（子类）对象可以在程序中代替其基类（超类）对象。\n换句话说，一个软件实体如果使用的是一个父类，那么也一定适用于其子类——把一个软件里面的父类都替换为它的子类，程序的行为是不会发生变化的。\n利用这一原则，我们可以判断类与类之间的继承关系是否合适。\n举个例子，假设我们拥有一个矩形类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  class Rectangle { public: Rectangle(int width, int height) : width(width), height(height) {} void setWidth(int width) { this-\u0026gt;width = width; } void setHeight(int height) { this-\u0026gt;height = height; } void setEdges(int width, int height) { this-\u0026gt;width = width; this-\u0026gt;height = height; } private: int width; int height; };   我们想要再新建立一个正方形类。根据初中几何知识：正方形是一种特殊的矩形——因此一种直观的想法是：让正方形类去继承矩形类：\n1 2 3 4  class Square: public Rectangle { // ... };   但如果站在里氏替换原则的角度来看，这一设计是不科学的！比如我们考虑以下操作：\n1 2 3  Rectangle rectangle; rectangle.setHeight(20); rectangle.setEdges(10, 5);   根据里氏替换原则，派生类对象（Square）一定可以替换基类对象（Rectangle），假如我们进行这一替换：\n1 2 3  Square square; square.setHeight(20); square.setEdges(10, 5);   这时就出现了问题：\n 第一个操作会产生歧义：该操作是只改变正方形的宽（这样会违背正方形的定义），还是同时改变正方形的长和宽（这样违背函数的字面意思）。 第二个操作则会直接违背正方形的定义。  可以看到，派生类对象在此处替换基类对象会产生很多问题，这一继承是不科学的！\n接口隔离原则⭐ 接口隔离原则指出，程序员在设计接口时应当将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法——使用多个专门的接口比使用单一的总接口要好。\n换句话讲，接口约束了类的行为，是一种减轻代码耦合程度的好方法。但如果一个接口太过宽泛，可能会带来一些不必要的麻烦。举例说明：\n我们想要定义一个“鸟”接口：\n1 2 3 4 5 6 7  class Bird { public: virtual void eat() = 0; virtual void breathe() = 0; virtual void fly() = 0; };   在此基础上实现一个鸽子类，现在一切看上去都正常：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Pigeon: public Bird { public: virtual void eat() override { // ...  } virtual void breathe() override { // ...  } virtual void fly() override { // ...  } };   我们再实现一个企鹅类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Penguin: public Bird { public: virtual void eat() override { // ...  } virtual void breathe() override { // ...  } virtual void fly() override { // ???  } };   问题发生了。我们在一开始设计“鸟”这一接口时，想当然地以为所有地鸟类都会飞，却忽略了企鹅不会飞这一特例。\n为了避免这样的情况发生，我们需要小心地将接口拆分：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  class Lifeform { public: virtual void eat() = 0; virtual void breathe() = 0; }; class Flyable { public: virtual void fly() = 0; }; class Pigeon: public Lifeform, public Flyable { public: void eat() override { // ...  } void breathe() override { // ...  } void fly() override { // ...  } }; class Penguin: public Lifeform { public: void eat() override { // ...  } void breathe() override { // ...  } };   如上文所示，所有的鸟类都需要呼吸和进食，我们可以大胆地将其封装为Lifeform接口，而并非所有鸟类都会飞，所以需要将其单独提取出来作为Flyable接口。在实现不同的鸟类时，我们将这些接口进行筛选组合即可。\n依赖倒转原则⭐ 依赖倒转原则指出，在实际的开发场景中，类与类之间的依赖关系是十分复杂，在设计依赖关系时，高层模块不应该依赖低层模块，二者都应该依赖其抽象。\n什么意思呢？考虑以下实例，一个用户在某在线网络平台上拥有一个账户，而这个账户又存储着该用户的信息。由此，两者不可避免地产生了下列的循环依赖关系——你中有我，我中有你：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  class Account; class Customer { public: // ...  void setAccount(Account *account) { customerAccount = account; } // ... private: Account *customerAccount; }; class Account { public: void setOwner(Customer *customer) { owner = customer; } private: Customer *owner; }; int main() { Account* account = new Account { }; Customer* customer = new Customer { }; account-\u0026gt;setOwner(customer); customer-\u0026gt;setAccount(account); }   这会导致很严重的问题：首先代码的可读性由于循环依赖下降，而且两者的生命周期不相互独立——如果Account对象的生命周期先于Customer对象结束，Customer对象中将会产生一个空指针，调用Customer对象中的成员函数可能会导致程序崩溃。\n而依赖倒转原则为解决此类问题提供了一套流程：\n 不允许两个类中的其中一个直接访问另一个类，要想进行这种访问操作，需要通过接口。 实现这个接口。  在本例中，我们不再使得Account类中包含有Customer类的指针，所有Account类需要访问Customer类的行为，都被定义进一个叫做Owner的接口中，而后，Customer类需要实现这个接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u0026lt;iostream\u0026gt;#include \u0026lt;string\u0026gt; class Owner { public: virtual std::string getName() = 0; }; class Account; class Customer : public Owner { public: void setAccount(Account* account) { customerAccount = account; } virtual std::string getName() override { // return the Customer\u0026#39;s name here...  } // ... private: Account* customerAccount; // ... }; class Account { public: void setOwner(Owner* owner) { this-\u0026gt;owner = owner; } //... private: Owner* owner; };   经过修改之后，Account类将不依赖于Customer类。\n设计模式⭐ C++、C#、Python等语言为实现继承、多态等面向对象特性提供了丰富的语法。那么在具体的软件工程中，又该如何使用这些特性呢？这就是设计模式。设计模式是上述SOLID原则在软件工程中的具体体现。\n设计模式共计分为3大类22小类：\n  创建型模式提供创建对象的机制， 增加已有代码的灵活性和可复用性。\n  结构型模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。\n  行为模式负责对象间的高效沟通和职责委派。\n  不同的设计模式之间有着相似的理念和重叠之处。合理利用设计模式可以让代码更加规范、更容易维护，但盲目使用设计模式也不是明智之举。\n本讲将介绍一个难度较大，而且应用较为广泛的设计模式——桥接模式（属于结构型模式）。\n桥接模式的定义如下：桥接模式是将类抽象部分与实现部分分离，使它们都可以独立地变化。\n什么是抽象部分？什么是实现部分？让我们先考虑以下场景：一家奶茶店售卖不同种类的奶茶，奶茶既有不同的容量，也有不同的口味。如果我们只需要改变奶茶的容量，可以做出如下设计：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  class IMilkTea // 通用接口 { virtual void order() = 0; }; class MilkTeaSmallCup: public IMilkTea { void order() override { std::cout \u0026lt;\u0026lt; \u0026#34;order info:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size: small cup\u0026#34; \u0026lt;\u0026lt; std::endl; } }; class MilkTeaMediumCup: public IMilkTea { void order() override { std::cout \u0026lt;\u0026lt; \u0026#34;order info:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size: medium cup\u0026#34; \u0026lt;\u0026lt; std::endl; } }; class MilkTeaLargeCup: public IMilkTea { void order() override { std::cout \u0026lt;\u0026lt; \u0026#34;order info:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size: large cup\u0026#34; \u0026lt;\u0026lt; std::endl; } };   当类的变化只有一个维度时，继承的思路是比较直接而简单的。但当我们将“口味”也加入继承体系中，也就是当类的变化有两个维度时，沿用上面的思路将会使得类的数量急剧增长：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  class MilkTeaSmallCupFairyGrass: public IMilkTea { void order() override { std::cout \u0026lt;\u0026lt; \u0026#34;order info:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size: small cup\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;flavor: fairy grass\u0026#34; \u0026lt;\u0026lt; std::endl; } }; class MilkTeaSmallCupPearl: public IMilkTea { void order() override { std::cout \u0026lt;\u0026lt; \u0026#34;order info:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;size: small cup\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;flavor: pearl\u0026#34; \u0026lt;\u0026lt; std::endl; } }; // class MilkTeaMediumCupPearl, class MilkTeaLargeCupFairyGrass, ...   问题的根源在于，我们试图在两个独立的维度（“容量”和“口味”）上扩展奶茶类。这时候，桥接模式就派上了用场：我们将容量视为抽象部分，将口味视为实现部分，并将两者桥接。\n “抽象部分”和“实现部分”所承担的角色：\n 抽象部分：抽象化给出的定义，只提供高层控制逻辑，依赖于完成底层实际工作的实现对象。抽象部分保存一个对实现化对象的引用（指针）。 实现部分：给出实现化角色的通用接口，抽象部分仅能通过在这里声明的方法与实现对象交互。   例如在本例中，可以做如下修改：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  // 实现化部分 class IMilkTeaFlavorBase { public: virtual void GetFlavor() = 0; }; class MilkTeaPearl: public IMilkTeaFlavorBase { public: void GetFlavor() override { std::cout \u0026lt;\u0026lt; \u0026#34;flavor: pearl\u0026#34; \u0026lt;\u0026lt; std::endl; } }; class MilkTeaFairyGrass: public IMilkTeaFlavorBase { public: void GetFlavor() override { std::cout \u0026lt;\u0026lt; \u0026#34;flavor: fairy grass\u0026#34; \u0026lt;\u0026lt; std::endl; } }; // 抽象化部分 class IMilkTeaSizeBase { public: virtual void SetFlavor(std::shared_ptr\u0026lt;IMilkTeaFlavorBase\u0026gt; flavorBase) { this-\u0026gt;flavorBase = flavorBase; } virtual void Order() = 0; protected: std::shared_ptr\u0026lt;IMilkTeaFlavorBase\u0026gt; flavorBase; }; class MilkTeaSmall: public IMilkTeaSizeBase { public: void Order() override { std::cout \u0026lt;\u0026lt; \u0026#34;size: small\u0026#34; \u0026lt;\u0026lt; std::endl; flavorBase-\u0026gt;GetFlavor(); } }; class MilkTeaMedium: public IMilkTeaSizeBase { public: void Order() override { std::cout \u0026lt;\u0026lt; \u0026#34;size: medium\u0026#34; \u0026lt;\u0026lt; std::endl; flavorBase-\u0026gt;GetFlavor(); } }; class MilkTeaLarge: public IMilkTeaSizeBase { public: void Order() override { std::cout \u0026lt;\u0026lt; \u0026#34;size: large\u0026#34; \u0026lt;\u0026lt; std::endl; flavorBase-\u0026gt;GetFlavor(); } }; // 使用方法 int main() { // 大杯烧仙草  std::shared_ptr\u0026lt;MilkTeaFairyGrass\u0026gt; milkTeaFairyGrass = std::make_shared\u0026lt;MilkTeaFairyGrass\u0026gt;(); std::shared_ptr\u0026lt;MilkTeaLarge\u0026gt; milkTeaLargeWithFairyGrass = std::make_shared\u0026lt;MilkTeaLarge\u0026gt;(); milkTeaLargeWithFairyGrass-\u0026gt;SetFlavor(milkTeaFairyGrass); milkTeaLargeWithFairyGrass-\u0026gt;Order(); }   可以在上述示例中看到：抽象部分各类中，都含有一个实现部分的指针。如果需要访问实现部分的方法，可以通过该指针进行访问。这样，我们就通过桥接的方式分离了两个不同的维度，使得类的可扩展性更好。\n由于篇幅所限，我们在此处不能对设计模式进行一一介绍，感兴趣的同学可以参考文末给出的阅读清单进行学习。\n参考文献和荐读清单 Refactoring.Guru 该网站详细介绍了各设计模式的特点，并提供了不同编程语言的实例。\nClean C++ 这本书的侧重点不在介绍C++语法，而侧重于使用C++语言介绍如何写出可读性强、符合面向对象规范的程序，强推！\n","date":"2022-06-20T20:07:09+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/oop/","title":"面向对象程序设计基础"},{"content":"gRPC 无03 王与进\n目录 [TOC]\n前言 在介绍gRPC之前，我们需要先介绍几个在通信中需要用到的概念。\nClient-Server model:star: Client-Server结构是一种经典的通信模型。它通常采取两层结构：\n 服务器（Server）负责数据的处理。它有以下特征：  等待来自客户端的请求 处理请求并传回结果   客户端（Client）负责完成与用户的交互任务。它有以下特征：  发送请求 等待直到收到响应    THUAI5就是一个应用了Client-Server model的典型实例：\n在游戏中，玩家通过在Client端编写C++代码来制定游戏策略，而Server端由Csharp语言写成，用于分析处理游戏逻辑。编译生成的Client端可执行文件将向Server端发送请求，请求处理完毕后Server端再向Client端发送处理后的结果，这样Client端就可以接受到游戏实况，以供下一步决策。\nIP Address IP Address(Internet Protocol address，网际协议地址)，是网际协议中用于标识发送或接受数据报的设备的一串数字。\n当设备连接网络后，设备将被分配一个IP地址，对于一个具体的设备而言，IP地址是独一无二的。IP地址有两个主要的功能：标识主机（用户在互联网上可以识别）和网络寻址（允许计算机通过互联网发送和接受数据）。\n常见的IP地址分为IPv4和IPv6两大类：\n  IPv4：32位长，通常书写时以四组十进制数字组成，并以点分割，例如：172.16.254.1。\n  IPv6：128位长，通常书写时以八组十六进制数字组成，并以冒号分割，例如：\n2001:db8:0:1234:0:567:8:1。\n  我们可以使用如下方法查询本机的IP地址：\n windows：ipconfig linux：ifconfig（可能需要使用sudo apt-get install net-tools进行安装）   一个特殊的IP地址：127.0.0.1\n尽管现在有大量可用的 IP 地址，但为了防止编程冲突的特定目的，刻意保留一些地址，甚至是地址范围是很方便的。\n127.0.0.1就是其中一个。它表示的是主机环回地址，表示的是任何数据包都不应该离开计算机，计算机本身即为接收者。\n当我们需要在本地测试一些网站服务，或者只想在本地设备上运行只有本地设备可以访问的服务，就可以使用127.0.0.1。\n Port Port(端口)在电脑网络中是一种经过软件创建的服务，在一个电脑的操作系统中扮演通信的端点。\n什么意思呢？利用IP地址，可以实现不同计算机之间的通信。但实际上，计算机中是运行着多个进程的——当不同的信息被传入计算机后，计算机需要一种手段来区分信息的接收者，以将不同进程的处理结果正确地发送给接收者。\n这个时候，端口就派上了用场。如果我们在通信时不仅指定IP地址，而且指定端口，计算机就可以正确地将不同的请求交给正确的进程处理。\n特定的服务一般对应于特定的端口，详见端口列表。\n我们可以使用如下方法查看本机的端口使用情况：\n windows：netstat -ano| findstr \u0026quot;\u0026lt;port\u0026gt;\u0026quot; linux：netstat -tunlp | grep \u0026lt;port\u0026gt;或lsof -i:\u0026lt;port\u0026gt;  gRPC概况 gRPC的全称是gRPC Remote Procedure Calls。其中“Remote Procedure Calls”翻译为“远程过程调用”。“远程过程调用”指的是客户端（Client）可以像调用本地对象一样直接调用服务端（Server）应用的方法。具体过程如下：\n 定义若干服务（Service），指定其能够被远程调用的方法（包含参数和返回类型）。这些定义都写在.proto文件里。 在服务端（Server）实现这个接口（内部处理逻辑），并运行gRPC服务器，来处理客户端的调用。 在客户端（Client）建立一个存根（stub），提供与服务端相同的方法。  下面的图形象地展示了gRPC的使用过程：\n这样一来，用户在使用gRPC构建的应用程序时，不需要关心调用方法的内部逻辑（被封装在Server中），只需要调用Client端提供的方法向Server端提供请求，等待Server端返回结果即可——看上去就和在Client端本地调用方法一样。\ngRPC有诸多优点：\n 速度快：gRPC使用protobuf进行Server/Client之间数据的序列化和反序列化，保证了通信的高效。 跨语言：构建Server端和Client端程序的源语言无需一致。 跨平台：Server端和Client端的平台无需一致。  我们仍然THUAI5为例，阐述gRPC在构建具体项目中的意义（注：虽然THUIA5中使用的通信方法并非gRPC，但gRPC对我们的设计仍然有着重大的借鉴意义）：\n Server端需要实现复杂的游戏逻辑，而且需要支持Unity，如果使用C++语言可能会导致开发效率太低，因此需要使用Csharp语言进行开发。 Client端需要提供选手接口供选手编写AI代码，因此需要使用C++语言开发。  两者使用语言不同，如何使得两者建立联系？我们可以使用gRPC的思路：\n 在.proto文件中定义选手可以调用的游戏方法（如人物操作和获取物品信息）。 在Server端实现这些接口的内部逻辑。 在Client端提供用户需要直接调用的方法，而无需关心其具体实现。  于是我们就实现了Server和Client的解耦。在此基础上，我们甚至可以提供不同种类语言的用户接口——你可以使用Python、Java或其它语言来编写你的游戏策略。\ngRPC安装 C++ 安装gRPC C++相关的库需要手动编译其源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13  $ git clone -b v1.46.3 --depth 1 --shallow-submodules https://github.com/grpc/grpc # 如果网络不佳，可以将网址换为 https://gitee.com/mirrors/grpc.git $ cd grpc $ git submodule update --init --recursive # 在grpc的原文档中没有submodule该步，但笔者实测，如果没有这一步grpc将无法安装。 $ mkdir -p cmake/build $ pushd cmake/build $ cmake -DgRPC_INSTALL=ON \\  -DgRPC_BUILD_TESTS=OFF \\  ../.. $ make -j $ make install # 或 sudo make install $ popd    需要指出的是，由于网络等问题，git submodule update --init --recursive一步往往无法正常运行。为此可以点击此处下载third_party.tar.gz，并将git submodule..一步替换为以下操作：\n1 2 3 4  $ rm -rf third_party $ mv \u0026lt;tar_gz_path\u0026gt; . $ tar -zxvf third_party.tar.gz $ cd ..    Csharp Csharp中，我们可以使用NuGet程序包安装gRPC库（图中第一项）。\ngRPC服务:star: grpc默认使用protobuf作为接口定义语言。定义方式见下例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // The greeter service definition. service Greeter { // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply);}// The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1;}// The response message containing the greetings message HelloReply { string reply = 1;}  定义服务使用了service和rpc关键字。粗略地来讲，在本例中，gRPC服务接受一条含有name字段的HelloRequest message，发送给服务端处理后，返回一条含有reply字段的HelloRequest message。\ngRPC可以定义以下4种服务：\n 单一RPC（Unary RPCs），客户端向服务器发送一个请求，并得到一个响应，就像一个正常的函数调用。简单来讲就是一个请求对象对应一个返回对象。  1  rpc SayHello(HelloRequest) returns (HelloResponse);   服务器流式RPC（Server streaming RPCs），客户端向服务器发送请求，并获得一个流来读回一连串的消息。客户端从返回的流中读取信息，直到没有更多的信息。gRPC保证在单个RPC调用中的信息排序。简单来讲就是发送一个请求对象，服务端可以传回多个结果对象。  1  rpc LotsOfReplies(HelloRequest) returns (stream HelloResponse);   客户端流式RPC（Client streaming RPCs），客户端写了一串消息并将它们发送给服务器，同样使用一个提供的流。一旦客户端完成了消息的写入，它就等待服务器读取它们并返回其响应。gRPC再次保证了单个RPC调用中的消息排序。简单来讲就是客户端传入多个请求对象，服务端返回一个响应结果。  1  rpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse);   双向流RPC（Bidirectional streaming RPCs），双方使用读写流发送一连串的消息。这两个流独立运行，因此客户和服务器可以按照他们喜欢的顺序进行读写：例如，服务器可以等待收到所有客户的消息，然后再写它的响应，或者它可以交替地读一个消息，然后再写一个消息，或者其他一些读和写的组合。每个流中的消息的顺序被保留下来。简单来讲就是结合客户端流式rpc和服务端流式rpc，可以传入多个对象，返回多个响应对象。  1  rpc BidiHello(stream HelloRequest) returns (stream HelloResponse);  接下来我们将结合一些实例进一步了解它们的使用方法和区别。\ngRPC使用:star: 在本例中，我们将使用Csharp语言实现一个简单的Client-Server模型——Client端提供两个数和一个操作符，而Server端则进行具体的运算过程并将计算结果返回给Client端。\nproto 我们不妨考虑以下服务场景：\n 客户端发送一个包含两个操作数和一个运算符的元组，服务端返回一个结果：该场景符合单一RPC。 客户端发送一个包含两个操作数和一个运算符的元组，服务端返回计算结果，并将该结果重复多次：该场景符合服务器流式RPC。 客户端发送若干个包含两个操作数和一个运算符的元组，服务端返回计算结果之和：该场景符合客户端流式RPC。 客户端发送若干个包含两个操作数和一个运算符的元组，服务端分别返回每一对元组的计算结果之和：该场景符合双向流RPC。  我们需要在Message.proto文件中定义需要提供的服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  syntax = \u0026#34;proto3\u0026#34;;package hello;enum Operator { NONE_OP = 0; ADD = 1; SUB = 2; MUL = 3;}message Operand { int32 op1 = 1; int32 op2 = 2; Operator opr = 3;}message Result { int32 val = 1;}service Calculator { // Unary  rpc UnaryCall (Operand) returns (Result); // Server streaming  rpc StreamingFromServer (Operand) returns (stream Result); // Client streaming  rpc StreamingFromClient (stream Operand) returns (Result); // Bi-directional streaming  rpc StreamingBothWays (stream Operand) returns (stream Result);}  之后就可以使用该文件生成对应的CSharp文件以供使用。\nServer Server端有两个任务：\n 实现我们服务定义的生成的服务接口：做我们的服务的实际的“工作”。 运行一个 gRPC 服务器，监听来自客户端的请求并返回服务的响应。  为了实现这些目的，我们需要在Server端定义一个CalculatorImpl类，并继承Calculator.CalculatorBase类，以实现所有的服务方法。\n 对于Calculator.CalculatorBase类的解释：Base class for server-side implementations of Calculator。可见它是专供Server端使用的一个基类。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93  class CalculatorImpl : Calculator.CalculatorBase { public override Task\u0026lt;Result\u0026gt; UnaryCall(Operand operand, ServerCallContext context) { var res = new Result(); switch (operand.Opr) { case Operator.Add: res.Val = operand.Op1 + operand.Op2; break; case Operator.Sub: res.Val = operand.Op1 - operand.Op2; break; case Operator.Mul: res.Val = operand.Op1 * operand.Op2; break; default: break; } return Task.FromResult(res); } public override async Task StreamingFromServer(Operand operand, IServerStreamWriter\u0026lt;Result\u0026gt; result_stream, ServerCallContext context) { var res = new Result(); switch (operand.Opr) { case Operator.Add: res.Val = operand.Op1 + operand.Op2; break; case Operator.Sub: res.Val = operand.Op1 - operand.Op2; break; case Operator.Mul: res.Val = operand.Op1 * operand.Op2; break; default: break; } for (var i = 0; i \u0026lt; 3; i++) { await result_stream.WriteAsync(res); } } public override async Task\u0026lt;Result\u0026gt; StreamingFromClient(IAsyncStreamReader\u0026lt;Operand\u0026gt; operand_stream, ServerCallContext context) { var res = new Result(); while (await operand_stream.MoveNext()) { var operand = operand_stream.Current; switch (operand.Opr) { case Operator.Add: res.Val += operand.Op1 + operand.Op2; break; case Operator.Sub: res.Val += operand.Op1 - operand.Op2; break; case Operator.Mul: res.Val += operand.Op1 * operand.Op2; break; default: break; } } return res; } public override async Task StreamingBothWays(IAsyncStreamReader\u0026lt;Operand\u0026gt; operand_stream, IServerStreamWriter\u0026lt;Result\u0026gt; result_stream, ServerCallContext context) { while (await operand_stream.MoveNext()) { Operand operand = operand_stream.Current; var res = new Result(); switch (operand.Opr) { case Operator.Add: res.Val = operand.Op1 + operand.Op2; break; case Operator.Sub: res.Val = operand.Op1 - operand.Op2; break; case Operator.Mul: res.Val = operand.Op1 * operand.Op2; break; default: break; } await result_stream.WriteAsync(res); } } }   我们来看上方的代码的特点：\n 为了允许任务的异步执行，我们在返回值中使用Task关键字。 在服务器流式RPC中，我们需要使用异步方法WriteAsync将服务器的响应写入异步流IServerStreamWriter中。 在客户端流式RPC中，我们需要使用异步流IAsyncStreamReader逐个读出请求并进行运算。 在双向流式RPC中，我们需要同时使用IAsyncStreamReader和IServerStreamWriter。  而启用gRPC服务器的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  public static void Main() { try { // 禁止复用端口！！！（SoReuseport 置为 0） Grpc.Core.Server server = new Grpc.Core.Server(new[] { new ChannelOption(ChannelOptions.SoReuseport, 0) }) { Services = { Calculator.BindService(new CalculatorImpl()) }, Ports = { new ServerPort(\u0026#34;127.0.0.1\u0026#34;, 8888, ServerCredentials.Insecure) } }; // 建立监听特定IP地址和端口Server的模板代码 server.Start(); Console.WriteLine(\u0026#34;Server begins to listen!\u0026#34;); Console.WriteLine(\u0026#34;Press any key to stop the server...\u0026#34;); Console.ReadKey(); Console.WriteLine(\u0026#34;Server end!\u0026#34;); server.ShutdownAsync().Wait(); } catch (Exception ex) { Console.WriteLine(ex.ToString()); } }   我们总结一下创建客户端的步骤：\n 创建 Grpc.Core.Server 的一个实例。 创建我们的服务实现类 CalculatorImpl 的一个实例。 通过在 Services 集合中添加服务的定义注册我们的服务实现。 指定想要接受客户端请求的地址和监听的端口。通过往 Ports 集合中添加 ServerPort 即可完成。 在服务器实例上调用 Start 为我们的服务启动一个 RPC 服务器。  Client 首先，我们需要建立一个Client对象：\n1 2 3  Channel channel = new Channel(\u0026#34;127.0.0.1:8888\u0026#34;, ChannelCredentials.Insecure); var client = new Calculator.CalculatorClient(channel); // 建立一个连接到特定host的client // ... Client 的调用操作   在调用单一RPC服务时，我们像调用本地方法那样调用远程方法（UnaryCall），如果RPC成功完成，则返回响应值。\n1 2 3 4 5  // case 1: unary call（单一RPC） Console.WriteLine(\u0026#34;case 1:\u0026#34;); var unaryCall = client.UnaryCall(operand0); //  var unaryCallVal = unaryCall.Val; Console.WriteLine(unaryCallVal);   在调用服务器流式RPC服务时，由于得到的响应是流式的，所以我们需要使用MoveNext方法逐个读取其值。\n1 2 3 4 5 6 7 8  // case 2: streaming from server（服务器流式RPC） Console.WriteLine(\u0026#34;case 2:\u0026#34;); var streamingFromServer = client.StreamingFromServer(operand0); while(await streamingFromServer.ResponseStream.MoveNext()) { var streamingFromServerVal = streamingFromServer.ResponseStream.Current.Val; Console.WriteLine(streamingFromServerVal); }   在调用客户端流式RPC服务时，我们需要使用WriteAsync方法逐个写入请求值，最终使用CompleteAsync方法表示不再请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // case 3: streaming from client（客户端流式RPC） Console.WriteLine(\u0026#34;case 3:\u0026#34;); var streamingFromClient = client.StreamingFromClient(); Tuple\u0026lt;int, int, Operator\u0026gt;[] tups = { new(1, 1, Operator.Add), new(5, 6, Operator.Mul), new(3, 4, Operator.Sub), new(0, 0, Operator.NoneOp) }; foreach (var tup in tups) { Operand operand = new Operand(); operand.Op1 = tup.Item1; operand.Op2 = tup.Item2; operand.Opr = tup.Item3; await streamingFromClient.RequestStream.WriteAsync(operand); } await streamingFromClient.RequestStream.CompleteAsync(); var streamingFromClientVal = streamingFromClient.ResponseAsync.Result.Val; Console.WriteLine(streamingFromClientVal);   在调用双向流RPC服务时，我们将请求写入RequestStream，使用ResponseStream获取响应。两者是相互独立的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // case 4: streaming both ways（双向流RPC） Console.WriteLine(\u0026#34;case 4:\u0026#34;); var streamingBothWays = client.StreamingBothWays(); foreach (var tup in tups) { Operand operand = new Operand(); operand.Op1 = tup.Item1; operand.Op2 = tup.Item2; operand.Opr = tup.Item3; _ = streamingBothWays.RequestStream.WriteAsync(operand); if (!await streamingBothWays.ResponseStream.MoveNext()) { break; } var streamingBothWaysVal = streamingBothWays.ResponseStream.Current.Val; Console.WriteLine(streamingBothWaysVal); }   运行结果如下：\n参考与荐读 由于时间所限，有很多有趣的内容我们没有涉及：\n 计算机网络模型 RPC的生命周期 在gRPC中使用安全认证和通讯协议 \u0026hellip;  略过上述内容不会对我们的教学产生太大影响，感兴趣的同学可以参考以下文档和资源：\n 计算机网络——自顶向下方法 Stanford CS144 gRPC 官方文档  ","date":"2022-06-20T19:55:32+08:00","permalink":"https://ther-nullptr.github.io/posts/programming/grpc/","title":"grpc"},{"content":"chapter 1 是谁把这些星星撒过天空\n像闪光的尘埃\n像发光的云\n他们将乳白色的光芒\n倒入深黑色的碗中\nWho spilled those stars across the sky\nlike sparkling dust\nlike clouds of light?\nThey pour their milky shine\ninto the deep black bowl\nchapter 2 我是我所行走的世界\n我所见所闻所感都来自我自己\nI was the world in which I walked\nand what I saw or heard or felt\ncame not but from myself\nchapter 3 大理石永久幻化成一个灵魂\n孤独地航行在陌生的\n思想海洋中\nThe marble index of a mind forever\nVoyaging through strange seas of thought, alone\nchapter 4 现在我用沉静的目光见到\n恰是那台机器脉冲的颤跳\nAnd now I see with eye serene\nThe very pulse of the machine\n","date":"2022-06-19T20:55:58+08:00","image":"https://ther-nullptr.github.io/posts/ldr/LDR_hu8eb093704b9dd0a9c9f2f554fd593af3_2854661_120x120_fill_q75_box_smart1.jpg","permalink":"https://ther-nullptr.github.io/posts/ldr/","title":"Love Death Robots"},{"content":"  打开wsl，输入以下指令：\n1  $ VSCODE_WSL_DEBUG_INFO=true code .   此时terminal会输出调试信息：\n此时所执行的脚本路径为~//.vscode//extensions//ms-vscode-remote.remote-wsl-0.xx.x//scripts//wslDownload.sh(On Windows)。定位程序发生卡死的位置：\n将该程序的第110行（或临近位置，带tar的那行）删掉：\n手动下载，并解压该文件，将其名称改为其SHA-1值。然后移动到~/.vscode-server/bin/中：\n成功解决！\n  ","date":"2022-06-19T19:10:59+08:00","permalink":"https://ther-nullptr.github.io/posts/awesome_toolkits/wsl_remote/","title":"解决VSCode更新后无法启动Remote WSL的问题"},{"content":"Q 现实世界或许不坏，可是我讨厌自己。\nA 你认为现实不好，所以心生厌恶。\nA 是你的内心把现实和真实对调了。\nA 只要看现实的角度和切入点稍有不同，心里就会有很大的变化。\nA 有多少种人就有多少种真实。\nA 但是，你自己的真实只有一个。它由狭义的世界观而生，是为了保护自己而修改过的信息。是被歪曲过的真实。\nA 不过仅有一个人所持的世界观是微不足道的。\nA 可是人只能用自己渺小的基准来测量事物，只愿意用别人给予的真实去看待事物。\nA 晴天使人快乐，雨天使人忧郁，被人这样教导之后就深信不疑。\nA 雨天也会有快乐的事，接受的方式不同就会产生完全不同的结果。人内心的真实，还真是脆弱啊。\nA 人的真实不过仅此而已，所以才会想要知道更深层的真实。\nA 只是，你不习惯被人喜欢罢了。\nA 所以你没有必要总是看别人的脸色。\nQ 但，大家不是都讨厌我吗？\nA 你是笨蛋吗？这只是你自己钻牛角尖而已。\nQ 可是，我讨厌自己。\nA 讨厌自己的人是无法喜欢和信赖他人的。\nQ 我悲怯，胆小，狡猾，懦弱。\nA 理解自己后，就可以对自己温柔一点了吧。\nQ 我讨厌自己，不过，也有可能会喜欢上自己。或许，我可以呆在这里。没错，我就是我自己。我就是我，我想做自己，我想呆在这里，我可以呆在这里！\n","date":"2022-06-19T19:09:43+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/eva2/","title":"每日一遍"},{"content":" 给您跪了。\n 自己的负面情绪，真的已经外溢到如此严重的地步了吗？\n或者说，这种网抑云式的生活状态，本身就是不可理喻的吗？\n","date":"2022-06-15T23:59:43+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/%E6%97%A0%E9%A2%98/","title":"外溢"},{"content":"优秀stack站点记录 zhixuan\u0026rsquo;s Blog\n相忘于江湖\nHome\n一笼虾饺有四个\n","date":"2022-06-15T00:24:11+08:00","permalink":"https://ther-nullptr.github.io/posts/awesome_toolkits/websites/","title":"Websites"},{"content":"多周期CPU 多周期数据通路     R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; ALUOut\u0026lt;=PC+(signext(IR[15:0]\u0026laquo;2)) * * *   EX ALUOut \u0026lt;= A op B ALUOut\u0026lt;=A+sign-ext(IR[15:0]) if (A=B)PC\u0026lt;=ALUOut PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   MEM Reg[IR[15:11]]\u0026lt;=ALUOut Lw:MDR\u0026lt;=MemData[ALUOut];Sw:MemData[ALUout] \u0026lt;=B     WB  Reg[IR[20:16]]\u0026lt;=MDR      重点控制信号的周期：\n IRWrite：在Instruction fench阶段被置为1，但在lw的Memory access阶段被置为0，因为在lw的Register writeback阶段，Instruction Register不能再更新，如果更新的话，会被读出的数据覆盖。 MemRead：在Instruction fench阶段被置为1（用于读取指令），在Instruction decode阶段为0，在lw的Memory access阶段被置为1。 MemWrite：默认0，在sw的Memory access阶段被置为1。 ALUSrc1：在Instruction fench阶段为00（PC），在Execution阶段根据实际情况修改。 ALUSrc2：在Instruction fench阶段为01（4），在Instruction decode阶段为11（imm\u0026laquo;2），在Execution阶段根据实际情况修改。 PCWrite：在Instruction fench阶段为1（PC\u0026lt;=PC+4），在Instruction decode阶段为0，此后若执行跳转类指令则被置为1。 PCSource：默认为00（PC\u0026lt;=PC+4），在Execution阶段视情况改变。 IorD：默认0（取指令），在Memory access阶段被置为1。 PCWriteCond：在beq的Execution阶段被置为1。 MemtoReg：默认00。 RegDst：默认00。 RegWrite：默认0，在jal和jalr以及Register writeback中被置为1。 ExtOp：在Instruction decode阶段被置为1，因为需要算跳转地址。 ALUOp：在Instruction fench和Instruction decode均为00，因为需要执行加法操作；R type被置为10，beq被置为01。  多周期异常和中断处理 异常指内部不可预知事件（溢出，同步），中断指外部不可预知事件（I/O，异步）。\n简单起见，假设我们需要处理两种异常：1)未定义指令的执行 2)算术溢出，异常处理程序的入口为0x80000180。\n我们需要添加2个寄存器：\n EPC Register：保存受影响的指令的地址。注意写入EPC的地址应该为PC-4。 Cause Register：记录产生异常事件原因。此处0为未定义指令，1为算术溢出。  4个控制信号：\n EPCWrite：EPC写入使能，在触发异常/中断时被置为1。 CauseWrite：Cause写入使能。 PCSource：增加0x80000180。 IntCause：异常原因选择信号。  异常处理的步骤大致如下：\n  异常检测。通过检测 ALU 的溢出信号，判断是否发生溢出异常。\n  保存现场。在异常程序计数器（Exception Program Counter，EPC）中保存出错的指令地址。\n  跳转到异常处理程序。通过修改程序计数器（PC）的值，使得处理器进入异常处理程序。\n  异常/中断处理程序采取操作，比如可以执行对溢出情况实现定义的一些操作，或者终止程序运行并报告。在异常处理完成后，异常处理程序可以选择终止程序，也可以根据 EPC 存储的指令地址恢复并继续执行程序。\n  ","date":"2022-06-04T11:43:15+08:00","permalink":"https://ther-nullptr.github.io/posts/digital_logic_and_processors/multicycle_cpu/","title":"Multicycle CPU"},{"content":"控制信号常见答疑 About Extop https://stackoverflow.com/questions/55290060/what-does-extend-immediate-to-32-bits-mean-in-mips\nI-type instructions with 16-bit immediates are different.\n addi / addiu immediates are sign-extended (by duplicating the top/sign bit of the immediate to all higher bits). https://en.wikipedia.org/wiki/Two%27s_complement#Sign_extension This allows 2\u0026rsquo;s complement numbers from -2^15 .. +2^15-1 to be encoded. (0xFFFF8000 to 0x00007FFF) ori/andi/xori boolean immediates are zero-extended (by setting all higher bits to zero) This allows unsigned / 2\u0026rsquo;s complement numbers from 0 .. 2^16-1 to be encoded. (0x00000000 to 0x0000FFFF)  For other instructions see this [instruction-set reference](https://web.cse.ohio-state.edu/~crawfis.3/cse675-02/Slides/MIPS Instruction Set.pdf) which breaks down each instruction showing 016 || [I15..0] for zero-extension or [I15]16 || [I15..0] for sign-extension.\nAnd usually you don\u0026rsquo;t want to raise an exception on signed overflow, so normally compilers use addu / addiu even on signed integers. addiu is badly named: it\u0026rsquo;s not \u0026ldquo;for unsigned integers\u0026rdquo;, it\u0026rsquo;s just a wrapping-allowed / never-faulting version of add/addi. It sort of makes sense if you think of C, where signed overflow is undefined behaviour (and thus could use add and raise an exception in that case if the compiler wanted to implement it that way), but unsigned integers have well-defined overflow behaviour: base 2 wraparound.\n可以为X的控制信号  控制数据来源的信号（RegDst，MemtoReg）等，不需要用到时可以为X。 控制是否执行某操作（RegWrite）等，必须为0/1。  ","date":"2022-06-04T10:34:25+08:00","permalink":"https://ther-nullptr.github.io/posts/digital_logic_and_processors/control_signal/","title":"CPU中的控制信号"},{"content":"处理器 处理器架构  普林斯顿架构：存储器同时存储指令和其他数据 哈佛架构：数据存储和指令存储分开  处理器性能 执行时间 = 指令数 x CPI x 时钟周期\n或： $$ CPI = CPI_1\\times p_1+\u0026hellip;+CPI_n\\times p_n $$ 性能提升方法：\n 优化编译技术（减少指令数） 快速电路技术或更为先进的电路结构（减少时钟周期）  寄存器 VS 存储器  寄存器：以编号进行访问，可同时访问不同寄存器。 存储器：以地址进行访问，不可同时访问不同地址，相邻数据的地址相差4字节。  数据单位约定 在32位MIPS中，1 word = 4 bytes = 32 bits，相邻数据的地址相差4字节。\n1 2 3 4 5 6 7 8 9 10 11  #include\u0026lt;stdio.h\u0026gt;int main() { printf(\u0026#34;%d\\n\u0026#34;,sizeof(int)); // 4  printf(\u0026#34;%d\\n\u0026#34;,sizeof(char)); // 1  printf(\u0026#34;%d\\n\u0026#34;,sizeof(unsigned int)); // 4  printf(\u0026#34;%d\\n\u0026#34;,sizeof(long int)); // 8  printf(\u0026#34;%d\\n\u0026#34;,sizeof(long long int)); // 8  printf(\u0026#34;%d\\n\u0026#34;,sizeof(float)); // 4  printf(\u0026#34;%d\\n\u0026#34;,sizeof(double)); // 8 }   MIPS汇编指令 汇编优化相关问题   算数\u0026amp;逻辑指令11bit冗余能否利用起来？\n额外11bit用于移位量\u0026amp;功能码。好处：寄存器算术操作只占用一种操作码，指令集可以使用其他操作码支持更多种指令。\n  分支可能的地址范围有32位，如何用16bit表示？\n采用基址+偏移地址的寻址方式。\n addr \u0026laquo; 2：将直接地址转换为字节地址。\n   带立即数的分支指令立即数如何编码？\nMIPS没有带立即数的分支指令，使用比较指令(slti、sltiu等）+ 分支指令组合实现。\n  访存可能的地址有32位，如何用21bit表示？\n采用基址+偏移地址的寻址方式进行访存（5 bit：寄存器 16 bit：立即数）。\n  跳转可能的地址有32位，如何用26bit表示？\nj小范围，jr大范围。\n  寻址方式  寄存器寻址：找到对应的寄存器，从寄存器中取数/写数。如：R type(add)。 立即数寻址：指令中的立即数可以被直接使用。如：I type(addi) 基址寻址：目标地址=基址（存储于寄存器中）+ 立即数。如：lw,sw PC相对寻址：PC+立即数。如：beq 伪直接寻址：固定PC的高4位不变。如：j  MIPS过程调用   Preserved（子程序不改变这些寄存器的数据，如果子程序要用，需要子程序维护好）\n  Not Preserved（子程序可以改变这些寄存器的数据，如果主程序要用，需要主程序维护好）\n  考虑以下汇编：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  fact: addi $sp, $sp, -8 sw $ra, 4($sp) sw $a0, 0($sp) slti $t0, $a0, 1 beq $t0, $zero, L1 addi $v0, $zero, 1 addi $sp, $sp, 8 jr $ra L1: addi $a0, $a0, -1 jal fact lw $a0, 0($sp) lw $ra, 4($sp) addi $sp, $sp, 8 mul $v0, $a0, $v0 jr $ra   对应：\n1 2 3 4 5  int fact(int n) { if (n \u0026lt; 1) return 1; else return (n * fact(n-1)); }   ","date":"2022-06-03T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/digital_logic_and_processors/assembly/","title":"MIPS汇编语言"},{"content":"Memory RAM random access是指该存储器的所有内容都可以被读取和写入，且与时间和位置无关。\n    SRAM DRAM     中文名 静态随机访问存储器 动态随机访问存储器   速度 快 慢（需要读取内容后刷新、定期刷新）   结构 由MOS管组成的锁存器（6T） 由MOS管和电容实现（1T1C）   容量 小 大   应用 缓存 主存   访问时间 1~10ns 10~100ns    常见问答：\n  DRAM电容量C的权衡：\n 大电容容量C的优势：提供更长的数据保持时间 大电容容量C的劣势：工艺实现难度加大，密度降低    RAS和CAS均为低电平有效，其中CAS可以作为输出使能信号。\n  cache 原理 存储系统满足局部性原理：\n 时间局部性——最近的将来要用到的信息很可能就是现在正在使用的信息。主要由循环造成。 空间局部性——最近的将来要用到的信息很可能与现在正在使用的信息在空间上是邻近的。主要由顺序执行和数据的聚集存放造成。  cache的访问时间 一级cache（$r$为访问时间比，$e$为访问效率）： $$ T_A=T_{A1}+(1-H)T_{A2}\\ r=\\frac{T_{A2}}{T_{A1}}\\ e=\\frac{T_{A1}}{T_A} = \\frac{1}{1+(1-H)r} $$ 二级cache： $$ T_A=T_{A1}+(1-H_1)[T_{A2}+(1-H_2)T_{A3}] $$\ncache的基本结构 需要解决两个问题：\n 数据是否在cache中？ 如果在cache中，如何找到数据？  首先需要指出主存存储和cache存储的格式：\n 主存存储 主存块号+行内地址 cache存储 （标签）+cache行号+行内地址  每个主存块和每个cache行储存的数据相同，两者的数据传输以块/行为单位。其中每个cache行的组成如下：数据+标签+有效位。\ncache地址映像方式 三种地址映像方式：\n 直接映像: 主存块只可能在cache的某个特定行中 全相联映像: 主存块可以放在cache的任意行中 组相联映像: 主存块可以放在cache中的n个特定行中，n一般在2到8之间，将这n个行称为一个Cache组。  直接映像 主存标号$i$，cache标号$j$满足以下关系： $$ j = i \\mod N \\ i = j+Nk(k=0,1..) $$ 可以直接写成AB、AC，但是只适用于直接映像。\n对于32位内存系统，假设cache每行有$x$byte，cache共$y$行，则低$\\log_2x$位为块内地址，中间$\\log_2y$位为cache行号，剩下$(32-\\log_2x-\\log_2y)$位为主存标签。\n在这种情况下，由于每行cache的数据有$4x$bit，还有1 bit有效位，所以cache的实际总位数为： $$ y[4x+1+(32-\\log_2x-\\log_2y)] $$ 注意到cache每行至少储存1个word，所以$x$一定是4的倍数，地址的后两位一定是0。载入cache数据时仍然以word为单位进行载入。\n全相联映像 对于32位内存系统，假设cache每行有$x$byte，则低$\\log_2x$位为块内地址，剩下$(32-\\log_2x)$位作为主存标签。\n组相联映像 对于32位内存系统，假设cache每行有$x$byte，cache共$y$行，$z$路组相连，则低$\\log_2x$位为块内地址，中间$\\log_2(\\frac{y}{z})$位作为cache组号，剩下$(32-\\log_2x-\\log_2(\\frac{y}{z}))$位作为主存标签。\n直接映像为$z=1$，全相联映像为$z=y$。\ncache数据替换 每个cache行有一个有效标志位v，表明“这一行的主存数据副本是有效的”。v=1时cache行才能被命中。\n 复位、刚上电或清空cache时，所有行的v=0。 cache行刚刚被替换时，对应行的v=1。  当访问一个地址发现其不在cache中时，复制主存块到缓存的行中；当主存块对应的cache行均被占用（有效位为1）时，需要选择一个cache行进行替换。\n 直接映像 只有一行可以被替换，不用选 组相联/全相联  随机法(Random)：在cache中随机选择一个主存块 先进先出法(FIFO, First-In First-Out)：选择一个集合中最先进入cache中的主存块(即存在时间最长的块)，类似于数据结构中的队列。 最近最少使用法(LRU, Least recently used)：替换cache中最近最少被使用的主存块    cache数据更新 当需要向层次结构存储器写入数据时，需要考虑将数据存入cache还是主存的问题。\nwrite through 指高速缓存和主存都写入。\nwrite back 先将数据写入Cache中，之后在该块要被替换出Cache时才将数据写到主存储器中。\n在cache中会添加一个脏位（dirty bit）。脏位为1意味着cache里面的数据是更新后的，而主存储器里面的数据是过时的。不一致的数据在被替换时一定要写回主存中。\n    Write Through Write Back      既写到cache同时也更新主存储器 只写cache，当数据被替换出cache时才将写回到主存储器    慢 快   被替换是否会导致写操作 No Yes   重复的写操作是否重复写主存 Yes No    cache性能评估  强迫性缺失：第一次访问主存储器中的某一个数据块，只能先从主存储器将数据加载到cache中。 容量缺失：由于cache容纳不了程序所需的所有主存块而引起的缺失。 冲突缺失：在组相联或者直接映像中，多个的主存块竞争同一个cache组时引起的缺失，也称碰撞缺失。  ","date":"2022-05-31T19:22:17+08:00","permalink":"https://ther-nullptr.github.io/posts/digital_logic_and_processors/memory/","title":"Cache"},{"content":"流水线计算公式 $n$为指令数，$k$为流水线级数，级间延时$\\Delta t$。\n实际吞吐率（单位时间内流水线处理的指令数）： $$ TP=\\frac{n}{(k+n-1)\\Delta t} $$ 最大吞吐率： $$ TP_{max} = \\frac{1}{\\Delta t} $$ 实际加速比： $$ S = \\frac{kn}{k+n-1} $$ 最大加速比： $$ S_{max}=k $$\n流水线中控制信号的流动 控制信号在IF之后的ID/RF阶段产生。\n ID/RF：Extop EX：ALUSrc、ALUOp、RegDst？？ MEM：MemWrite、Branch WB：MemToReg、RegWrite  流水线中的冒险 首先列出未冒险时流水线CPU在各步中进行的操作：\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC] * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; ALUOut\u0026lt;=PC+(signext(IR[15:0]\u0026lt;\u0026lt;2)) * * PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B ALUOut\u0026lt;=A+sign-ext(IR[15:0]) if (A=B)PC\u0026lt;=ALUOut    MEM Reg[IR[15:11]]\u0026lt;=ALUOut Lw:MDR\u0026lt;=MemData[ALUOut];Sw:MemData[ALUout] \u0026lt;=B     WB  Reg[IR[20:16]] \u0026lt;=MDR      结构冒险 problem 流水线处理器中直接取消了InstMemory和DataMemory混用的做法，因此不必担心存储器访问的冲突。\n ALU使用冲突。考虑下列指令：  1 2  add $t0,$t1,$t2 beq $a0,$a1,label   当add指令执行完EX时，beq指令执行完ID，ALU产生冲突的结果。\n寄存器堆写入冲突。考虑下列指令：  1 2  lw $a0,0($t0) add $t0,$t1,$t2   当add指令执行完MEM时，lw指令执行完WB，寄存器写入数据产生冲突的结果。\nsolution   ALU使用冲突：\nbeq指令原先需要进行两次计算操作：1)在ID阶段计算分支地址 2)在EX阶段作差比较，更新PC。\n现在需要将计算分支地址移动到EX阶段，把ALUOut计算分解为ALU和PCAdd（一个周期进行两次计算），在MEM阶段更新PC。\n  寄存器堆写入冲突：\n将R型的Write back移动到WB阶段。\n  更新后的操作如下：\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; * * PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B ALUOut\u0026lt;=A+sign-ext(IR[15:0]) ALUOut\u0026lt;=A-B; PCAdd\u0026lt;=PC+(signext(IR[15:0]\u0026lt;\u0026lt;2))    MEM  Lw:MDR\u0026lt;=MemData[ALUOut]; Sw:MemData[ALUout] \u0026lt;=B if(Zero) PC\u0026lt;=PCAdd    WB Reg[IR[15:11]]\u0026lt;=ALUOut Reg[IR[20:16]] \u0026lt;=MDR      数据冒险 无法得到所需的数据而导致不能执行后续指令。数据冒险面对的是操作数是否已经更新的问题。\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; * * PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B ALUOut\u0026lt;=A+sign-ext(IR[15:0]) ALUOut\u0026lt;=A-B; PCAdd\u0026lt;=PC+(signext(IR[15:0]\u0026lt;\u0026lt;2))    MEM  Lw:MDR\u0026lt;=MemData[ALUOut]; Sw:MemData[ALUout] \u0026lt;=B if(Zero) PC\u0026lt;=PCAdd    WB Reg[IR[15:11]]\u0026lt;= ALUOut Reg[IR[20:16]] \u0026lt;= MDR      Read after write data hazards(RAW) 硬件优化 考虑以下指令：\n1 2  add $r1,$r2,$r3 add $r2,$r1,$r3   before:\n               add $r1,$r2,$r3 IF ID EX MEM WB     nop          nop          add $r2,$r1,$r3    IF ID EX MEM    注意到第一条指令在WB阶段将结果写回寄存器（注意已经不是MEM阶段了！），而第二条指令在ID阶段读取寄存器。可以不修改数据通路，但是需要保证寄存器先写后读（否则需要阻塞3个周期）。\n 实现方法：\n 流水线上的寄存器在上升沿时写入，在时钟的下降沿写入寄存器堆。 比较写入地址和读取地址，当两者相同且要写入寄存器堆时，读取端的数据直接选择为写入端的数据而不从寄存器堆中读取。   Forward(转发) 考虑以下指令：\n1 2 3  add $r1,$r2,$r3 sub $r4,$r1,$r5 and $r6,$r1,$r7   after:\n               add $r1,$r2,$r3 IF ID EX MEM WB     sub $r4,$r1,$r5  IF ID EX MEM WB    and $r6,$r1,$r7   IF ID EX MEM WB    指令1在WB阶段写入r1寄存器，但指令2、3在ID阶段就要用到r1。不过实际上在指令的EX阶段该数值就已经计算完毕，需要在指令2、3的EX阶段用到。\n对于指令2，EX的操作数来源于EX/MEM.ALUOut；对于指令3，EX的操作数来源于MEM/WB.ALUOut；\nLoad-use data hazard(lw-calculate type) Forward 考虑以下指令：\n1 2 3  lw $r1,100($r2) sub $r4,$r1,$r5 and $r6,$r1,$r7   before:\n               lw $r1,100($r2) IF ID EX MEM WB     nop          nop          sub $r4,$r1,$r5    IF ID EX MEM   and $r6,$r1,$r7     IF ID EX    after:\n               lw $r1,100($r2) IF ID EX MEM WB               sub $r4,$r1,$r5   IF ID EX MEM WB   and $r6,$r1,$r7    IF ID EX MEM    lw后r1的新值在MEM阶段后产生，随后被转发至MDR（注意此处的MDR与多周期中的MDR不同，此处的MDR应该是MEM/WB的一部分）中，在下个周期供sub的EX阶段使用。\n上述方法必须使用一次stall，可以重排指令，在lw之后运行一条不依赖r1寄存器的指令。\n更新后的操作如下：\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; * * PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B; (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR) ALUOut\u0026lt;=A+sign-ext(IR[15:0]); (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR) ALUOut\u0026lt;=A-B; PCAdd\u0026lt;=PC+(signext(IR[15:0]\u0026laquo;2))    MEM  Lw:MDR\u0026lt;=MemData[ALUOut]; Sw:MemData[ALUout] \u0026lt;=B if(Zero) PC\u0026lt;=PCAdd    WB Reg[IR[15:11]]\u0026lt;=ALUOut Reg[IR[20:16]] \u0026lt;=MDR      Load-use data hazard(lw-sw type) 考虑以下指令：\n1 2  lw $a0,10($a1) sw $a0,10($a2)   before\n               lw $a0,10($a1) IF ID EX MEM WB     nop          sw $a0,10($a2)   IF ID EX MEM WB    after\n               lw $a0,10($a1) IF ID EX MEM WB     sw $a0,10($a2)  IF ID EX MEM WB     lw在MEM阶段结束后更新a0的值，此时可以直接转发给sw，以供在下一时钟周期存入存储器。\n 注意与以下情景做区分：\n1 2  lw $t4, 0($t0) sw $t0, 0($t4)   这就不是lw-sw type了，解决方案见lw-calculate type。\n 控制冒险 取到的指令可能不是所需要的，导致指令不能在预定的时钟周期内执行。控制冒险面对的是下一条指令的PC是多少的问题。\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; * * PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B; (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR) ALUOut\u0026lt;=A+sign-ext(IR[15:0]); (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR) ALUOut\u0026lt;=A-B; PCAdd\u0026lt;=PC+(signext(IR[15:0]\u0026laquo;2))    MEM  Lw:MDR\u0026lt;=MemData[ALUOut]; Sw:MemData[ALUout] \u0026lt;=B(B,MEM/WB.MemReadData) if(Zero) PC\u0026lt;=PCAdd    WB Reg[IR[15:11]]\u0026lt;=ALUOut Reg[IR[20:16]] \u0026lt;=MDR      beq hazard 考虑如下指令：\n1 2  beq $t1,$t2,0x10 add $a3,$a2,$a1   before:\n               beq $t1,$t2,0x10 IF ID EX MEM WB     nop          nop          nop          add $a3,$a2,$a1     IF ID EX    beq在MEM阶段才执行跳转，在WB阶段将目标地址写入PC，写入PC后下一条指令在IF阶段取用PC，默认情况下需要stall 3个周期，否则下一条指令执行可能会发生错误。\n但实际上判断是否需要跳转的所有条件在EX阶段执行后就可以全部掌握，可以将ALUOut转发的结果转发至IF，这样只需要stall 2个周期。\nafter(v1):\n               beq $t1,$t2,0x10 IF ID EX MEM WB     nop          nop          add $a3,$a2,$a1    IF ID EX MEM    实际上，也可以将分支判断移动到ID阶段：\n    R lw or sw beq J     IF PC\u0026lt;=PC+4; IR \u0026lt;=MemInst[PC]; * * *   ID opcode\u0026lt;=IR[31:26]; A\u0026lt;=Reg[IR[25:21]]; B\u0026lt;=Reg[IR[20:16]]; * if(A==B) PC\u0026lt;=PC+(signext(IR[15:0]\u0026laquo;2)) PC \u0026lt;= {PC[31:28],IR[25:0],2’b00}   EX ALUOut \u0026lt;= A op B; (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR) ALUOut\u0026lt;=A+sign-ext(IR[15:0]); (Register,EX/MEM.ALUOut,MEM/WB.ALUOut,MDR)     MEM  Lw:MDR\u0026lt;=MemData[ALUOut]; Sw:MemData[ALUout] \u0026lt;=B(B,MEM/WB.MemReadData)     WB Reg[IR[15:11]] \u0026lt;=ALUOut Reg[IR[20:16]] \u0026lt;=MDR      但是可能会带来一些新的问题：\n  在EX阶段我们使用ALU比较两个操作数是否相等，使用PCAdder计算分支地址，所以现在需要在ID阶段额外引入比较器和PC计算器。注意到数据可能来自旁路。\n  在ID阶段判断beq，等价于将beq的EX阶段提前进行，这样就会产生3种情况：\n beq前1条指令为R type指令，stall 1个周期（1EX-\u0026gt;2ID）                 add $a3,$a2,$a1 IF ID EX MEM WB     nop          beq $t0,$a3,0x10   IF ID EX MEM WB     beq前1条指令为lw指令，stall 2个周期（1MEM-\u0026gt;2ID）                 lw $a3,0($a2) IF ID EX MEM WB     nop          nop          beq $t0,$a3,0x10    IF ID EX MEM     beq前2条指令为lw指令，stall 1个周期（1MEM-\u0026gt;3ID）                 lw $a3,0($a2) IF ID EX MEM WB     add $t3,$t2,$t1  IF ID EX MEM WB    nop          beq $t0,$a3,0x10    IF ID EX MEM      延迟槽技术 即使将beq的判断前移到ID阶段，在beq之后也必须stall一个周期。可以在stall周期内执行一些必定要执行的指令，这就是延迟槽技术。\n无特殊说明使用延迟槽技术的情况下，即使beq后面的指令必定执行，也必须要stall。\n预测  静态预测：总预测分支不执行或者执行，错误则撤销指令。若预测失误会导致不必要的流水线重置。 动态预测：在IF阶段进行分支预测缓存，可以用PC（或者PC的低位地址）为索引，记录过去是否跳转。   实现过程：\n  clock 1(IF):\n在第一次执行beq指令时，建立BHT和BTB，并存下1)指令地址 2)最终是否跳转 3)跳转目标地址；在之后执行beq指令时，查询指令地址是否在BHT和BTB中，若不存在，建立新的条目；若存在，根据历史记录判断是否跳转；若跳转，取出目标地址作为下一条指令的IF地址。\n  clock 2(ID):\n根据预测目标地址取出指令。\n   j hazard 考虑如下指令：\n1 2  j label add $t3,$t1,$t2                  j label IF ID EX MEM WB     stall          add $a3,$a2,$a1   IF ID EX MEM WB    j在ID阶段完成目标地址的计算，需要stall一个周期。\n**但这种做法是错误的！**因为流水线直到ID阶段才能知道取出的指令为j，而此时下一条指令只能已经从指令存储中取出，只能延后执行，而不能不执行。\n于是我们不进行硬件阻塞。j指令执行完ID阶段时，可以判断出执行的是j指令，此时下一条指令（实际上不会执行）也执行完IF阶段，但之后的步骤都会被flush掉，ID阶段生成的新地址也被用于载入下下一条指令。从时间上看还是stall了一个周期。\n               j label  IF ID EX MEM WB     add $a3,$a2,$a1(next)  IF x x x x    add $a3,$a2,$a1(jump target)   IF ID EX MEM WB     转发方式总结\n 数据冒险   A2-\u0026gt;B1 R型的前前条为R型 A3-\u0026gt;B1 R型的前条为R型 A4-\u0026gt;B2 lw-sw转发  此外，A4-\u0026gt;B1无法转发，对应于lw-R型必须stall一个周期。\n 控制冒险(beq)   A2-\u0026gt;B0 beq的前前条为R型，可以使用转发解决 A3-\u0026gt;B0 beq的前前条为lw，需要stall 2个周期 A4-\u0026gt;B0 beq的前条为R型，需要stall 1个周期 A5-\u0026gt;B0 beq的前条为lw型，需要stall 2个周期  注意在转发时不能跨时钟周期转发，比如上上条指令ALU的结果输出不能直接转发，必须要经过MEM/WB寄存器。\n 考虑冒险的数据通路设计 Forward Unit R-R type(1) 若R型的前条为R型，则可能需要从EX_MEM寄存器中转发至EX阶段：\n1 2 3 4 5 6 7 8  if (EX_MEM.RegWrite // 需要写入寄存器  and (EX_MEM.RegWrAddr != 0) // 不能使用0寄存器  and (EX_MEM.RegWrAddr == ID_EX.RegisterRs)) // 触发转发条件（如果不使用转发就会冒险）  ForwardA = 10; if (EX_MEM.RegWrite and (EX_MEM.RegWrAddr != 0) and (EX_MEM.RegWrAddr == ID_EX.RegisterRt)) ForwardB = 10;   R-R type(2) 若R型的前前条为R型，则可能需要从MEM_WB寄存器中转发至EX阶段：\n1 2 3 4 5 6 7 8 9 10 11 12  if (MEM_WB.RegWrite and (MEM_WB.RegWrAddr != 0) and (MEM_WB.RegWrAddr == ID_EX.RegisterRs) and (EX_MEM.RegWrAddr != ID_EX.RegisterRs || ~ EX_MEM.RegWrite) // 从前条转发的条件不满足 ) ForwardA = 01; if (MEM_WB.RegWrite and (MEM_WB.RegWrAddr != 0) and (MEM_WB.RegWrAddr == ID_EX.RegisterRt) and (EX_MEM.RegWrAddr != ID_EX.RegisterRt || ~ EX_MEM.RegWrite) // 从前条转发的条件不满足 ) ForwardB = 01;   最后一个判断条件是为了避免以下情况（前两条指令都是以关联寄存器为目的寄存器的时候，需要转发最新的数据，其数据来源是前一条，而不是前前条）：\n1 2 3  add $1,$1,$2 add $1,$1,$3 add $1,$1,$4   综上可知，EX阶段的forward单元可以描述为（以操作数1为例）：\n1 2 3 4 5 6 7 8  always @(*) begin case(ForwardA) 00:ALU_op1 \u0026lt;= ID_EX.op1; 01:ALU_op1 \u0026lt;= MEM_WB.wb_res; 10:ALU_op1 \u0026lt;= EX_MEM.alu_res; default:ALU_op1 \u0026lt;= ID_EX.op1; endcase end   lw-sw type 若sw的前条为lw，则可能需要从MEM/WB寄存器中转发至MEM阶段：\n1 2 3 4 5 6  if(EX_MEM.RegWrAddr != 0 // 不能使用0寄存器  and EX_MEM.MemWrite // 需要写入存储器(sw)  and MEM_WB.MemRead // 需要读取存储器(lw)  and EX_MEM.RegWrAddr == MEM_WB.RegWrAddr // 转发源和转发目标的寄存器编号一致 ) ForwardA = 1;   综上可知，MEM阶段的forward单元可以描述为：\n1 2 3 4 5 6  always @(*) begin case(Forward) 0:Mem_Write_Data \u0026lt;= EX_MEM.op2; 1:Mem_Write_Data \u0026lt;= MEM_WB.Mem_Read_Data; endcase end   beq 若beq的前前一条指令为R type，则需要将EX_MEM中的ALU计算结果转发至ID阶段：\n1 2 3 4  if(EX_MEM.RegWrite // 需要写入寄存器  and EX_MEM.RegisterRd == IF_ID.RegisterRs // 需要用到寄存器中的结果  ) Forward = 1;   综上可知，ID阶段的forward单元可以描述为：\n1 2 3 4 5 6  always @(*) begin case(Forward) 0:compare_op1 \u0026lt;= regA; 1:compare_op1 \u0026lt;= EX_MEM.alu_res; endcase end   Hazard Unit lw-R type 1 2 3 4  if (ID/EX.MemRead // 是否为load指令  and ((ID/EX.RegisterRd == IF/ID.RegisterRs) or (ID/EX.RegisterRd == IF/ID.RegisterRt)) ) // EX级的装载指令的目的寄存器是否与在ID级指令的某一个源寄存器相匹配（可能会发生冒险）  stall = 5\u0026#39;b11000; // stall IF and ID   beq 1 2 3 4 5  if(ID.branch // 分支指令  and ((ID_EX.RegWrite and (ID_EX.RegisterRd == IF_ID.RegisterRs or ID_EX.RegisterRd == IF_ID.RegisterRt))) // R type  and ((EX_MEM.MemRead and (EX_MEM.RegisterRd == IF_ID.RegisterRs or EX_MEM.RegisterRd == IF_ID.RegisterRt))) // lw  ) stall = 5\u0026#39;b11000; // stall IF and ID    当 beq 前一条指令为 lw 指令时，我们阻塞流水线一个周期，在 lw 和 beq 中间插入一个气泡。此时这一情况自动退化为前前一条指令为 lw 的情况，会被上述逻辑再次处理，因此最终还是会完成 2 个周期的阻塞。\n Flush Unit 使用flush[i]清除第i级流水线上执行的指令（对应4级流水寄存器以及最终写回的寄存器堆）：\n1 2 3  if(flush[i]) begin stage_reg[i] \u0026lt;= 0; end   流水线CPU异常处理 假设有3种异常：badop，IRQ（外部中断），ALUExp。\nException被放置在EX阶段，因此badop要经过一次ID/EX寄存器。\n触发异常时，该单元会flush掉当前指令的EX/MEM寄存器，下条指令的ID/EX寄存器，下下条指令的IF/ID寄存器。（注意flush由hazard和exception单元共同控制）\n扩展技术 指令级并行 超级流水线 用于缩短时钟周期。\n多发射 用于降低CPI。\n  超长指令字 静态决定让哪些指令同时执行（在编译阶段由编译器决定）。\n  超标量 动态决定哪些指令同时执行 （在运行时由硬件决定）。\n IOI-IOC,IOI-OOC,OOI-OOC\n   线程级并行 超线程 多核处理器 $a$为并行部分的比例，$n$为并行部分的加速比。 $$ S=\\frac{1}{(1-a)+\\frac{a}{n}} $$\n异构计算 单指令流多数据流，具有更大的并行度，设计相对比较简单。\n","date":"2022-05-31T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/digital_logic_and_processors/pipeline_cpu/","title":"Pipeline CPU"},{"content":"冰菓 今夜与君\n爱梦相会\n务必觅得\n微眠之约\n萌生此等惬意\n实乃徒增彷徨\n予君之意\n君不知乎\n察其目光\n未萦他意\n宛如待友\n毫无二致\n许愿星逝于晓\n夜现晨湮之法\n愿君得察\n今夜与君\n爱梦相会\n务必觅得\n微眠之约\n","date":"2022-05-29T16:24:16+08:00","permalink":"https://ther-nullptr.github.io/posts/small_talk/ice-cream/","title":"Ice Cream"},{"content":"Hugo commands  create a article  1  $ hugo new post/first.md    start hugo server  1 2  $ hugo server # simple debug $ hugo server --theme=Mainroad --buildDrafts # debug with theme    watch environment info  1  $ hugo env   ","date":"2022-05-29T00:00:00Z","permalink":"https://ther-nullptr.github.io/posts/awesome_toolkits/command/","title":"Hugo Commands"},{"content":"Fourier Transform:\n1  FourierTransform[HeavisideTheta[x],x,w]  Inverse Fourier Transform:\n1  InverseFourierTransform[1/(1+I*w),w,x]  Fourier coefficient:\n1 2 3  FourierSinCoefficient[SquareWave[x],x,n]FourierCosCoefficient[SquareWave[x],x,n]FourierCoefficient[SquareWave[x],x,n]  Convolution:\n1 2  Convolve[UnitBox[x],UnitBox[x],x,x]Convolve[Exp[-a*t]*HeavisideTheta[t],Sin[t]*HeavisideTheta[t],t,x]  DTFT:\n1  FourierSequenceTransform[HeavisideTheta[n]*a^n,n,w]  IDTFT:\n1  InverseFourierSequenceTransform[1,n,w]  Laplace:\n1  LaplaceTransform[t^4Sin[t],t,s]  InverseLaplace:\n1  LaplaceTransform[E^(-t),t,s]  ","date":"2022-04-16T00:14:35+08:00","permalink":"https://ther-nullptr.github.io/posts/awesome_toolkits/ss_mathematica/","title":"Mathematica in Signal \u0026 System"},{"content":"快速排序与归并排序的思想 起源是递归法——将序列分成两部分处理，或者说，用二叉树遍历的逻辑处理。二叉树的遍历基本有三种方式——前序、中序、后序。\n  归并是一种后序遍历：算法到手有两个已经处理好的序列，然后把这两个序列处理成最终的结果。处理的最后一步，当然是完成对序列的排序，因此，算法的内容就是：将两个排序好的序列排成一个新的序列——这就是归并。\n  快排是一种前序遍历：对一个序列进行处理，然后处理其子列。处理有这样的要求：将两个子列都完全完成处理以后，整个数列就排好了。因此：“划分点”是唯一的符合条件的处理方法。\n  那么有没有中序遍历？手里拿着一个序列：一半已经排好，另一半还是完全乱的，这一步处理的目的就是：这一步执行完成之后，再对右边进行排序，整个序列就排好了。所以，这一个步骤的内容就应该是：维持左边的序列完好，保证右边所有的元素都大于左边的所有元素。\n  ","date":"2021-12-03T00:13:05+08:00","permalink":"https://ther-nullptr.github.io/posts/quotes/%E4%B8%87%E7%A5%9E/","title":"快速排序与归并排序的思想"},{"content":"计算机美化指南 前言 一个美观的开发界面，对于调试程序、管理代码版本、提升编程体验等有着至关重要的作用。本文介绍了windows平台下命令行界面的美化方法，以期让读者拥有更好的编程体验。\n在图形化用户界面（GUI）大规模普及之前，命令行界面（CLI）一直是电脑界的主流。CLI开销小、运行快速，但是非专业用户使用不方便。如今，不从事开发的电脑用户接触到命令行的机会已经很少了（非计算机系学习C，可能只会在“命令行参数”一节接触到命令行），但如果从事软件开发，使用git、gcc等工具，熟练掌握命令行的使用还是有必要的。\ncmd 点击win+R，输入cmd，就会弹出windows下最基本的命令行终端——cmd。它的初始界面长这样：\n且不说白+黑的配色毫无生机，字体看上去也十分违和。这种不美观的界面可能的确劝退了不少人学习它的欲望。\n 科普：什么样的字体才能称之为好看？\n  serif：衬线字体，字体边缘具有明显的艺术修饰效果，如 宋体（simsun）、Times new roman。\n这种字体适合做艺术字，但若用作代码字体，则会显得节外生枝，影响呈现效果。\n  sans-serif：非衬线字体，字体比划一般粗细均匀、清晰，如 微软雅黑（Arial）。这种字体一般用于正文写作。\n  monospace：等宽字体，指每个英文字符（字母、数字、标点）宽度一致的字体。如 Consolas、Courier New。\n这种字体由于呈现效果较好，被广泛地用于编程。\n绝大多数开发工具都会有使用等宽字体的建议（如VS 2019）：\n   我们试图给cmd换一个monospace的字体（右键边框，点击“属性”）。遗憾的是，cmd字体的选择十分匮乏，找不到合适的monospace字体。\npowershell 我们看看windows上另一款更加强大的命令行界面：powershell。在windows搜索框中键入powershell，打开。\n遗憾的是，除了黑色界面变成蓝色界面，字体的呈现效果并没有什么改观。而且，powershell也没有提供一种较为美观的monospace字体。\nwindows terminal 长期以来，windows都没有像mac、Linux那样，为开发者提供一个较为美观的命令行界面。这种情况一直到2019年windows terminal的推出才有所改观。你可以在Microsoft Store中直接安装它。\n安装完毕后，启动效果如下：\n打开“设置-power shell图标-外观”，可以看到现在终端的字体是Cascadia Mono，可以查证这是一种等宽字体。windows powershell的字体的选择十分丰富，你可以根据自己的喜好任意挑选。\noh-my-posh 还可以实现更加美观的效果吗？当然可以！我们需要借助oh-my-posh插件，先看下最终效果吧:\n可以看到，该插件不仅加入了彩色的图标、操作时间等元素，而且对文件夹的git仓库状态等也有较好的显示。\n预安装要求：Windows terminal、git（后台回复：git，领取git安装程序，安装时只需一路点OK）。\n  下载oh-my-posh和posh-git插件\n由于一些众所周知的原因，网络上所展示的传统的下载途径可能需要一些特殊的手段。对此，小编准备了插件资源（后台回复：terminal，提取插件）。资源中有一个Modules文件夹和一个Microsoft.PowerShell_profile.ps1文件。\n下载完毕后，在你的电脑中找到C:\\Users\\用户名\\Documents(或文档)\\WindowsPowerShell文件夹（也有可能是其它的D盘或E盘，因人而异）。此时的文件夹中应该有一个Scripts文件夹。将Modules文件夹和一个Microsoft.PowerShell_profile.ps1文件按照如下方式放置：\n启动windows terminal，会看到以下场景：\n这些方块是什么？是乱码。这是因为系统自带的字体不能渲染oh-my-posh的一些特定符号。我们需要下载对应的字体。\n  终端后续配置\n为渲染这些符号，我们需要下载名为Nerd系列的字体。网址如下：https://www.nerdfonts.com/\n（若网址打不开，也可后台回复：fonts，领取Nerd字体）。解压文件夹后，打开其中的.ttf文件，点击安装，即可使用字体。\n重启windows terminal，选择刚才安装的字体，即可呈现出正确的效果。\n还可以设置终端背景、终端透明度等，让你的命令行界面更加出彩。\n至此，命令行界面美化完成！\n  ","date":"2021-08-19T23:53:42+08:00","permalink":"https://ther-nullptr.github.io/posts/awesome_toolkits/oh-my-posh/","title":"Oh My Posh"}]