<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>SSL model finetune :: Ther&#39;s Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="structure of finetune models 首先有必要记录一下finetune model的结构（以wav2vec2、hubert、data2vec）为例：
wav2vec2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  Wav2VecCtc( (w2v_encoder): Wav2VecEncoder( (w2v_model): Wav2Vec2Model( (feature_extractor): ConvFeatureExtractionModel( (conv_layers): ModuleList( (0): Sequential( (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False) (1): Dropout(p=0." />
<meta name="keywords" content="research" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://ther-nullptr.github.io/posts/research/ssl-model-finetune/" />




<link rel="stylesheet" href="https://ther-nullptr.github.io/assets/style.css">

  <link rel="stylesheet" href="assets/%25!s%28%3cnil%3e%29.css">






<link rel="apple-touch-icon" href="https://ther-nullptr.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://ther-nullptr.github.io/">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="SSL model finetune">
<meta property="og:description" content="structure of finetune models 首先有必要记录一下finetune model的结构（以wav2vec2、hubert、data2vec）为例：
wav2vec2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  Wav2VecCtc( (w2v_encoder): Wav2VecEncoder( (w2v_model): Wav2Vec2Model( (feature_extractor): ConvFeatureExtractionModel( (conv_layers): ModuleList( (0): Sequential( (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False) (1): Dropout(p=0." />
<meta property="og:url" content="https://ther-nullptr.github.io/posts/research/ssl-model-finetune/" />
<meta property="og:site_name" content="Ther&#39;s Blog" />

  
    <meta property="og:image" content="https://ther-nullptr.github.io/">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

  <meta property="article:section" content="research" />


  <meta property="article:published_time" content="2022-08-12 00:13:05 &#43;0800 CST" />












</head>
<body class="">


<div class="container headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="https://ther-nullptr.github.io/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/posts/">Home</a></li>
        
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="/archives/">Archives</a></li>
        
      
        
          <li><a href="/search/">Search</a></li>
        
      
        
          <li><a href="/links/">Links</a></li>
        
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/posts/">Home</a></li>
      
    
      
        <li><a href="/about/">About</a></li>
      
    
      
        <li><a href="/archives/">Archives</a></li>
      
    
      
        <li><a href="/search/">Search</a></li>
      
    
      
        <li><a href="/links/">Links</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://ther-nullptr.github.io/posts/research/ssl-model-finetune/">SSL model finetune</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2022-08-12
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://ther-nullptr.github.io/tags/research/">research</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <h2 id="structure-of-finetune-models">structure of finetune models<a href="#structure-of-finetune-models" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>首先有必要记录一下finetune model的结构（以wav2vec2、hubert、data2vec）为例：</p>
<h3 id="wav2vec2">wav2vec2<a href="#wav2vec2" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Wav2VecCtc(
</span></span><span class="line"><span class="cl">  (w2v_encoder): Wav2VecEncoder(
</span></span><span class="line"><span class="cl">    (w2v_model): Wav2Vec2Model(
</span></span><span class="line"><span class="cl">      (feature_extractor): ConvFeatureExtractionModel(
</span></span><span class="line"><span class="cl">        (conv_layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (1): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (2): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (3): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (4): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (5): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (6): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">      (dropout_input): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">      (dropout_features): Dropout(p=0.1, inplace=False)
</span></span><span class="line"><span class="cl">      (quantizer): None
</span></span><span class="line"><span class="cl">      (project_q): None
</span></span><span class="line"><span class="cl">      (encoder): TransformerEncoder(
</span></span><span class="line"><span class="cl">        (pos_conv): Sequential(
</span></span><span class="line"><span class="cl">          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
</span></span><span class="line"><span class="cl">          (1): SamePad()
</span></span><span class="line"><span class="cl">          (2): GELU()
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): TransformerSentenceEncoderLayer(
</span></span><span class="line"><span class="cl">            (self_attn): MultiheadAttention(
</span></span><span class="line"><span class="cl">              (dropout_module): FairseqDropout()
</span></span><span class="line"><span class="cl">              (k_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (v_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (q_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (out_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (dropout1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout2): Dropout(p=0.1, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout3): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">            (fc1): Linear(in_features=768, out_features=3072, bias=True)
</span></span><span class="line"><span class="cl">            (fc2): Linear(in_features=3072, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          ...
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      (final_proj): None
</span></span><span class="line"><span class="cl">    )
</span></span><span class="line"><span class="cl">    (final_dropout): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">    (proj): Linear(in_features=768, out_features=32, bias=True)
</span></span><span class="line"><span class="cl">  )
</span></span><span class="line"><span class="cl">)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="hubert">hubert<a href="#hubert" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">HubertCtc(
</span></span><span class="line"><span class="cl">  (w2v_encoder): HubertEncoder(
</span></span><span class="line"><span class="cl">    (w2v_model): HubertModel(
</span></span><span class="line"><span class="cl">      (feature_extractor): ConvFeatureExtractionModel(
</span></span><span class="line"><span class="cl">        (conv_layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (1): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (2): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (3): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (4): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (5): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (6): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">      (dropout_input): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">      (dropout_features): Dropout(p=0.1, inplace=False)
</span></span><span class="line"><span class="cl">      (encoder): TransformerEncoder(
</span></span><span class="line"><span class="cl">        (pos_conv): Sequential(
</span></span><span class="line"><span class="cl">          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
</span></span><span class="line"><span class="cl">          (1): SamePad()
</span></span><span class="line"><span class="cl">          (2): GELU()
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): TransformerSentenceEncoderLayer(
</span></span><span class="line"><span class="cl">            (self_attn): MultiheadAttention(
</span></span><span class="line"><span class="cl">              (dropout_module): FairseqDropout()
</span></span><span class="line"><span class="cl">              (k_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (v_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (q_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (out_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (dropout1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout2): Dropout(p=0.1, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout3): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">            (fc1): Linear(in_features=768, out_features=3072, bias=True)
</span></span><span class="line"><span class="cl">            (fc2): Linear(in_features=3072, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        ...
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      (final_proj): None
</span></span><span class="line"><span class="cl">    )
</span></span><span class="line"><span class="cl">    (final_dropout): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">    (proj): Linear(in_features=768, out_features=32, bias=True)
</span></span><span class="line"><span class="cl">  )
</span></span><span class="line"><span class="cl">)
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="data2vec">data2vec<a href="#data2vec" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Wav2VecCtc(
</span></span><span class="line"><span class="cl">  (w2v_encoder): Wav2VecEncoder(
</span></span><span class="line"><span class="cl">    (w2v_model): Data2VecAudioModel(
</span></span><span class="line"><span class="cl">      (feature_extractor): ConvFeatureExtractionModel(
</span></span><span class="line"><span class="cl">        (conv_layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (1): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (2): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (3): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (4): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (5): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (6): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
</span></span><span class="line"><span class="cl">            (1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (2): Sequential(
</span></span><span class="line"><span class="cl">              (0): TransposeLast()
</span></span><span class="line"><span class="cl">              (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">              (2): TransposeLast()
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (3): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">      (dropout_input): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">      (dropout_features): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">      (encoder): TransformerEncoder(
</span></span><span class="line"><span class="cl">        (pos_conv): Sequential(
</span></span><span class="line"><span class="cl">          (0): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">            (1): SamePad()
</span></span><span class="line"><span class="cl">            (2): TransposeLast()
</span></span><span class="line"><span class="cl">            (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">            (4): TransposeLast()
</span></span><span class="line"><span class="cl">            (5): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (1): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">            (1): SamePad()
</span></span><span class="line"><span class="cl">            (2): TransposeLast()
</span></span><span class="line"><span class="cl">            (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">            (4): TransposeLast()
</span></span><span class="line"><span class="cl">            (5): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (2): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">            (1): SamePad()
</span></span><span class="line"><span class="cl">            (2): TransposeLast()
</span></span><span class="line"><span class="cl">            (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">            (4): TransposeLast()
</span></span><span class="line"><span class="cl">            (5): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (3): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">            (1): SamePad()
</span></span><span class="line"><span class="cl">            (2): TransposeLast()
</span></span><span class="line"><span class="cl">            (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">            (4): TransposeLast()
</span></span><span class="line"><span class="cl">            (5): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">          (4): Sequential(
</span></span><span class="line"><span class="cl">            (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">            (1): SamePad()
</span></span><span class="line"><span class="cl">            (2): TransposeLast()
</span></span><span class="line"><span class="cl">            (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">            (4): TransposeLast()
</span></span><span class="line"><span class="cl">            (5): GELU()
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layers): ModuleList(
</span></span><span class="line"><span class="cl">          (0): TransformerSentenceEncoderLayer(
</span></span><span class="line"><span class="cl">            (self_attn): MultiheadAttention(
</span></span><span class="line"><span class="cl">              (dropout_module): FairseqDropout()
</span></span><span class="line"><span class="cl">              (k_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (v_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (q_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">              (out_proj): Linear(in_features=768, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            )
</span></span><span class="line"><span class="cl">            (dropout1): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout2): Dropout(p=0.1, inplace=False)
</span></span><span class="line"><span class="cl">            (dropout3): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">            (fc1): Linear(in_features=768, out_features=3072, bias=True)
</span></span><span class="line"><span class="cl">            (fc2): Linear(in_features=3072, out_features=768, bias=True)
</span></span><span class="line"><span class="cl">            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">          )
</span></span><span class="line"><span class="cl">        ...
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      )
</span></span><span class="line"><span class="cl">      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">      (final_proj): None
</span></span><span class="line"><span class="cl">    )
</span></span><span class="line"><span class="cl">    (final_dropout): Dropout(p=0.0, inplace=False)
</span></span><span class="line"><span class="cl">    (proj): Linear(in_features=768, out_features=32, bias=True)
</span></span><span class="line"><span class="cl">  )
</span></span><span class="line"><span class="cl">)
</span></span></code></pre></td></tr></table>
</div>
</div><p>对比可以看出，wav2vec2和hubert的结构基本类似，而与data2vec则有以下区别：</p>
<ol>
<li>
<p>hubert的feature extractor只有第一层有normalization，而data2vec的feature extractor每一层都有normalization，而且一个是groupnorm，一个是layernorm。</p>
<p>hubert:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
</span></span></code></pre></td></tr></table>
</div>
</div><p>data2vec:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(2): Sequential(
</span></span><span class="line"><span class="cl">           (0): TransposeLast()
</span></span><span class="line"><span class="cl">           (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)
</span></span><span class="line"><span class="cl">           (2): TransposeLast()
</span></span><span class="line"><span class="cl">         )
</span></span></code></pre></td></tr></table>
</div>
</div><p>不过两者的区别应该不本质，因为wav2vec2原论文就是layernorm。</p>
</li>
<li>
<p>hubert的pos conv只有1层，而data2vec的pos conv有5层，且架构略有不同：
hubert:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(pos_conv): Sequential(
</span></span><span class="line"><span class="cl">       (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
</span></span><span class="line"><span class="cl">       (1): SamePad()
</span></span><span class="line"><span class="cl">       (2): GELU()
</span></span><span class="line"><span class="cl">     )
</span></span></code></pre></td></tr></table>
</div>
</div><p>data2vec:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">(0): Sequential(
</span></span><span class="line"><span class="cl">         (0): Conv1d(768, 768, kernel_size=(19,), stride=(1,), padding=(9,), groups=16)
</span></span><span class="line"><span class="cl">         (1): SamePad()
</span></span><span class="line"><span class="cl">         (2): TransposeLast()
</span></span><span class="line"><span class="cl">         (3): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
</span></span><span class="line"><span class="cl">         (4): TransposeLast()
</span></span><span class="line"><span class="cl">         (5): GELU()
</span></span><span class="line"><span class="cl">       )
</span></span></code></pre></td></tr></table>
</div>
</div><p>此外，论文中特别强调<strong>wav input和每一层conv的输出全是要经过normalization的</strong>。</p>
<p>两者所起到的作用都是relative positional embedding。</p>
</li>
</ol>
<h2 id="config-for-finetune">config for finetune<a href="#config-for-finetune" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>以下均以base model为例：</p>
<table>
<thead>
<tr>
<th></th>
<th>wav2vec2</th>
<th>hubert</th>
<th>data2vec</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>80000</td>
<td>80000</td>
<td>-</td>
</tr>
<tr>
<td>lr(max)</td>
<td>2e-5/3e-5(librilight),1e-4(librispeech)</td>
<td>5e-4</td>
<td>-</td>
</tr>
<tr>
<td>normalization</td>
<td>false</td>
<td>false</td>
<td>true</td>
</tr>
</tbody>
</table>

      </div></div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2022 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://ther-nullptr.github.io/assets/main.js"></script>
<script src="https://ther-nullptr.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
