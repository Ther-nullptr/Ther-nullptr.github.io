<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>knowledge distillation papers :: Ther&#39;s Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="以下对于distill model的研究将不只局限于ASR领域，还包括CV和NLP领域等。
loss algorithms 首先需要借助PyTorch平台简要介绍知识蒸馏中经常需要用到的一些loss api。
KL loss KL div的计算方式如下：
其中，PyTorch API为：torch.nn.KLDivLoss。
KL div衡量的是给定任意分布偏离真实分布的程度。从公式中可以看出，$p(x_i)$ 有更高概率的匹配区域比低 $p(x_i)$ 概率的匹配区域更加重要。
实际上，我们有：
等式右边第一项为交叉熵，由于$H(X)$（某一事件的熵）是固定的，所以KL散度的含义就是，相对于最优的编码，使用错误的编码浪费的比特数。
cos embedding loss PyTorch API为：torch.nn.CosineEmbeddingLoss。
contrastive loss BCE loss BCEloss解决的是二分类问题，或者可以视作一种简化版的交叉熵损失函数——将所有的其它例子都视作负例。
InfoNCE loss 同样可以使用温度参数$\tau$进行调节。
$l1$/$l2$ loss l1: torch.nn.L1Loss
l2: torch.nn.MSELoss
distill基础思想 distill最初提出于论文Distilling the Knowledge in a Neural Network。
文章在最初指出了混合模型在提高精度方面的作用，但混合模型具有开销较大的特点，一个较好的办法是将其蒸馏至一个student model中。在传统的认知中，我们认为知识蕴含在形式化的模型参数中，因此很难想象在变换模型形式的基础上保留学到的知识。
该论文指出对知识的正确认识应该是“it is a learned mapping from input vectors to output vectors”。更具体地讲，teacher model具有良好的泛化能力，蒸馏的目的就是要让student model学会teacher model的泛化能力，从而起到比直接训练student model更好的结果。
如何学习这种泛化能力？蒸馏的基础思想是使用teacher model的soft target（model预测的概率分布）训练student model（对应的，hard target指的是model预测的ground truth）。使用soft target训练有两个好处：" />
<meta name="keywords" content="research" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://ther-nullptr.github.io/posts/research/knowledge-distillation-papers/" />




<link rel="stylesheet" href="https://ther-nullptr.github.io/assets/style.css">

  <link rel="stylesheet" href="assets/%25!s%28%3cnil%3e%29.css">






<link rel="apple-touch-icon" href="https://ther-nullptr.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://ther-nullptr.github.io/">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="knowledge distillation papers">
<meta property="og:description" content="以下对于distill model的研究将不只局限于ASR领域，还包括CV和NLP领域等。
loss algorithms 首先需要借助PyTorch平台简要介绍知识蒸馏中经常需要用到的一些loss api。
KL loss KL div的计算方式如下：
其中，PyTorch API为：torch.nn.KLDivLoss。
KL div衡量的是给定任意分布偏离真实分布的程度。从公式中可以看出，$p(x_i)$ 有更高概率的匹配区域比低 $p(x_i)$ 概率的匹配区域更加重要。
实际上，我们有：
等式右边第一项为交叉熵，由于$H(X)$（某一事件的熵）是固定的，所以KL散度的含义就是，相对于最优的编码，使用错误的编码浪费的比特数。
cos embedding loss PyTorch API为：torch.nn.CosineEmbeddingLoss。
contrastive loss BCE loss BCEloss解决的是二分类问题，或者可以视作一种简化版的交叉熵损失函数——将所有的其它例子都视作负例。
InfoNCE loss 同样可以使用温度参数$\tau$进行调节。
$l1$/$l2$ loss l1: torch.nn.L1Loss
l2: torch.nn.MSELoss
distill基础思想 distill最初提出于论文Distilling the Knowledge in a Neural Network。
文章在最初指出了混合模型在提高精度方面的作用，但混合模型具有开销较大的特点，一个较好的办法是将其蒸馏至一个student model中。在传统的认知中，我们认为知识蕴含在形式化的模型参数中，因此很难想象在变换模型形式的基础上保留学到的知识。
该论文指出对知识的正确认识应该是“it is a learned mapping from input vectors to output vectors”。更具体地讲，teacher model具有良好的泛化能力，蒸馏的目的就是要让student model学会teacher model的泛化能力，从而起到比直接训练student model更好的结果。
如何学习这种泛化能力？蒸馏的基础思想是使用teacher model的soft target（model预测的概率分布）训练student model（对应的，hard target指的是model预测的ground truth）。使用soft target训练有两个好处：" />
<meta property="og:url" content="https://ther-nullptr.github.io/posts/research/knowledge-distillation-papers/" />
<meta property="og:site_name" content="Ther&#39;s Blog" />

  
    <meta property="og:image" content="https://ther-nullptr.github.io/">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

  <meta property="article:section" content="research" />


  <meta property="article:published_time" content="2022-08-17 08:22:05 &#43;0800 CST" />












</head>
<body class="">


<div class="container headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="https://ther-nullptr.github.io/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/posts/">Home</a></li>
        
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="/archives/">Archives</a></li>
        
      
        
          <li><a href="/search/">Search</a></li>
        
      
        
          <li><a href="/links/">Links</a></li>
        
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/posts/">Home</a></li>
      
    
      
        <li><a href="/about/">About</a></li>
      
    
      
        <li><a href="/archives/">Archives</a></li>
      
    
      
        <li><a href="/search/">Search</a></li>
      
    
      
        <li><a href="/links/">Links</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://ther-nullptr.github.io/posts/research/knowledge-distillation-papers/">knowledge distillation papers</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2022-08-17
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://ther-nullptr.github.io/tags/research/">research</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>以下对于distill model的研究将不只局限于ASR领域，还包括CV和NLP领域等。</p>
<h2 id="loss-algorithms">loss algorithms<a href="#loss-algorithms" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>首先需要借助PyTorch平台简要介绍知识蒸馏中经常需要用到的一些loss api。</p>
<h3 id="kl-loss">KL loss<a href="#kl-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>KL div的计算方式如下：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818174343816.png" alt="KL div"></p>
<p>其中，PyTorch API为：<code>torch.nn.KLDivLoss</code>。</p>
<p>KL div衡量的是给定任意分布偏离真实分布的程度。从公式中可以看出，$p(x_i)$ 有更高概率的匹配区域比低 $p(x_i)$ 概率的匹配区域更加重要。</p>
<p>实际上，我们有：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818180040162.png" alt="KL vs CE"></p>
<p>等式右边第一项为交叉熵，由于$H(X)$（某一事件的熵）是固定的，所以KL散度的含义就是，相对于最优的编码，使用错误的编码浪费的比特数。</p>
<h3 id="cos-embedding-loss">cos embedding loss<a href="#cos-embedding-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>PyTorch API为：<code>torch.nn.CosineEmbeddingLoss</code>。</p>
<p><img src="https://github.com/Ther-nullptr/ImageRepository/main/img/image-20220818195200570.png" alt="cos loss"></p>
<h3 id="contrastive-loss">contrastive loss<a href="#contrastive-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><img src="https://github.com/Ther-nullptr/ImageRepository/tree/main/img/image-20220819175604937.png" alt="contrastive loss"></p>
<h3 id="bce-loss">BCE loss<a href="#bce-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819180642611.png?token=AQ7WYGHG27FMO2KKIL7ASQTDATBB2" alt="BCE loss"></p>
<p>BCEloss解决的是二分类问题，或者可以视作一种简化版的交叉熵损失函数——将所有的其它例子都视作负例。</p>
<h3 id="infonce-loss">InfoNCE loss<a href="#infonce-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819181156002.png?token=AQ7WYGDHPDPQAAM64ZEL4TDDATBCC" alt="info nce loss"></p>
<p>同样可以使用温度参数$\tau$进行调节。</p>
<h3 id="l1l2-loss">$l1$/$l2$ loss<a href="#l1l2-loss" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>l1: <code>torch.nn.L1Loss</code></p>
<p>l2: <code>torch.nn.MSELoss</code></p>
<h2 id="distill基础思想">distill基础思想<a href="#distill基础思想" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>distill最初提出于论文<em>Distilling the Knowledge in a Neural Network</em>。</p>
<p>文章在最初指出了混合模型在提高精度方面的作用，但混合模型具有开销较大的特点，一个较好的办法是将其蒸馏至一个student model中。在传统的认知中，我们认为知识蕴含在形式化的模型参数中，因此很难想象在变换模型形式的基础上保留学到的知识。</p>
<p>该论文指出对知识的正确认识应该是“it is a learned mapping from input vectors to output vectors”。更具体地讲，teacher model具有良好的泛化能力，蒸馏的目的就是要让student model学会teacher model的泛化能力，从而起到比直接训练student model更好的结果。</p>
<p>如何学习这种泛化能力？蒸馏的基础思想是使用teacher model的soft target（model预测的概率分布）训练student model（对应的，hard target指的是model预测的ground truth）。使用soft target训练有两个好处：</p>
<ul>
<li>相比hard target有更多信息——soft target中可以携带很多有用的信息，而这些信息不可能用一个hard target来编码。相当于提高了label的质量，允许model使用更少的参数。</li>
<li>梯度差异小，lr可以更高。</li>
</ul>
<p>但使用soft target存在一个问题：有些概率分布很小，在计算loss时可以忽略不计，但其中蕴含的一些关键信息会丢失。前人的解决方案是使用logits（softmax的input）而不是probablity（实际上，后文指出，这是一种特殊情况的distillation）。本文中提出了一种叫做“distillation”的方法，用于提高softmax结果的“温度”。直到繁琐的模型产生一套合适的软目标。然后我们在训练小模型时使用同样的高温来匹配这些软目标。</p>
<p>“温度”的使用方法如下。一般来说，$T$越大，产生的概率分布越柔和。值得注意的是，student model在训练时使用$T&gt;1$，但在推理时使用$T=1$。</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818122556744.png?token=AQ7WYGCS6YFA5GO2ZYUKWNLDATBEY" alt="temperature in softmax"></p>
<p>从以上分析可以看出，蒸馏实际上是一个自监督过程，我们也可以引入监督过程——使用两个不同目标函数的加权平均，第一个目标函数是与软目标的交叉熵，这个交叉熵的计算方法是使用蒸馏模型的softmax中的高温，就像从繁琐的模型中生成软目标时一样，第二个目标函数是与正确标签的交叉熵（通常使用较低的权重）。</p>
<blockquote>
<p>由于软目标产生的梯度的大小为$\frac{1}{T^2}$，因此在使用硬目标和软目标时，必须将其乘以$T^2$。这可以确保在试验元参数时，如果改变蒸馏所用的温度，硬目标和软目标的相对贡献大致保持不变。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818160749384.png?token=AQ7WYGEDSMCV2Y7LYNWLO4LDATBFM" alt="conclusion"></p>
<h2 id="distill-model联合训练">distill model联合训练<a href="#distill-model联合训练" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>参考自论文<em>Apprentice: Using KD Techniques to Improve Low-Precision Network Accuracy</em>。</p>
<p>该论文主要针对低精度的学生网络而言。论文中一共提供了3种训练方法：第一种方案（方案A）联合训练两个网络&ndash;全精度的教师和低精度的学生网络。第二种方案（方案B）只训练低精度的学生网络，但在整个训练过程中从训练好的全精度教师网络中提取知识。第三种方案（方案C）从训练好的全精度教师和全精度学生网络开始，但在降低了学生网络的精度后对其进行微调。第二种相比第一种收敛更快，第三种准确率最高。在以上三种结构中，学生网络具有与教师网络类似的拓扑结构，只是学生网络具有低精度的神经元，而教师网络的神经元则以全精度运行。</p>
<p>其训练方式可用下图概括：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818172450825.png?token=AQ7WYGEW3Z24FVBD666IU3LDATBGM" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818172538944.png?token=AQ7WYGHX45AHE4QKJF6FACDDATBG6" alt=""></p>
<p>其中$\alpha=1,\beta=0.5,\gamma=0.5$。</p>
<h2 id="kl-div的应用">KL div的应用<a href="#kl-div的应用" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>总结一下，之前在蒸馏时计算loss的函数为：
$$
\mathtt {loss} = \alpha T^2 \mathtt{CEloss}(Q_s^\tau,Q_t^\tau)+(1-\alpha) \mathtt{CEloss}(Q_s,y)
$$
现在我们将其改写为：
$$
\mathtt {loss} = \alpha T^2 \mathtt{KLdiv}(Q_s^\tau,Q_t^\tau)+(1-\alpha) \mathtt{CEloss}(Q_s,y)
$$
两者的本质是一样的。</p>
<h2 id="distill-teacher的hidden-states">distill teacher的hidden states<a href="#distill-teacher的hidden-states" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>参考自论文<em>FITNETS: HINTS FOR THIN DEEP NETS</em>：</p>
<p>论文指出，使用教师学到的中间表征作为提示，以改善训练过程和学生的最终表现。因为学生的中间隐藏层一般会比教师的中间隐藏层小，所以要引入额外的参数，将学生的隐藏层映射到教师隐藏层的预测上。</p>
<p>论文指出了传统知识蒸馏模型的一个局限性：以前所有与卷积神经网络相关的工作都集中在将教师网络或网络集合压缩到宽度和深度相似的网络中，或者压缩到较浅和较宽的网络中；而没有利用深度的优势。因此本文指出了一种新的distill方法：使用同等深度但更窄的网络，从教师的隐藏层中引入中间层次的提示来指导学生的训练过程。</p>
<p>在传统的知识蒸馏模型中，如果加深student model的深度，KDLoss的效果会变差。因此，此处引入了“提示层”和“引导层”的概念。一个提示被定义为负责指导学生学习过程的教师隐藏层的输出。类似地，我们选择FitNet的一个隐藏层，即引导层，从教师的提示层学习。我们希望引导层能够预测提示层的输出。我们选择提示层为教师网络的中间层。同样地，我们选择引导层作为学生网络的中间层。</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818184502630.png?token=AQ7WYGBEZ3R7LTFZX6N77C3DATBHS" alt=""></p>
<p>其中$\mathbf{W_r}$指的是回归器。为了减小参数量，回归器一般使用卷积层而不是全连接层。</p>
<p>训练分为两个阶段：第一个阶段利用Hint-based loss诱导学生网络达到一个合适的初始化状态（只更新$\mathbf{W_Guided}$与$\mathbf{W_r}$）；第二个阶段利用教师网络的soft label指导整个学生网络的训练（即知识蒸馏），且Total loss中Soft target相关部分所占比重逐渐降低，从而让学生网络能够全面辨别简单样本与困难样本（教师网络能够有效辨别简单样本，而困难样本则需要借助真实标注，即Hard target）。</p>
<h2 id="distill在nlp领域的应用distilbert和tinybert">distill在NLP领域的应用——DistilBERT和TinyBERT<a href="#distill在nlp领域的应用distilbert和tinybert" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="distilbert">DistilBERT<a href="#distilbert" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>参考自论文<em>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</em>和<em>TinyBERT: Distilling BERT for Natural Language Understanding</em>。</p>
<p>DistillBERT用到了3种loss——除了传统无监督的$L_{CE}$和有监督的$L_{mlm}$，还引入了一个cos embedding loss $L_{cos}$。它将倾向于对齐学生和教师隐藏状态向量的方向。权重为5:2:1。</p>
<p>DistillBERT每隔两层就抽取一层进行初始化，缩减了一半的层数。</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818200933588.png?token=AQ7WYGFARJJHV6WPZY7TZ7TDATBIA" alt="steps"></p>
<h3 id="tinybert">TinyBERT<a href="#tinybert" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>TinyBERT的蒸馏分为两个阶段：在一般蒸馏阶段，没有微调的原始BERT作为教师模型。学生的TinyBERT通过提议的Transformer蒸馏法在一般领域的语料库上模仿教师的行为。之后，我们得到一个一般的TinyBERT，作为学生模型的初始化，用于进一步的蒸馏。在特定任务的蒸馏阶段，我们首先进行数据增强，然后使用经过微调的BERT作为教师模型在增强后的数据集上进行蒸馏。应该指出的是，这两个阶段对于提高TinyBERT的性能和泛化能力至关重要。</p>
<p>一般蒸馏帮助TinyBERT学习嵌入在预训练的BERT中的丰富知识，这对提高TinyBERT的泛化能力起到了重要作用。特定任务蒸馏进一步向TinyBERT传授来自微调的BERT的知识。</p>
<p>需要指出的是，在finetune阶段进行蒸馏时，需要对数据进行增强。</p>
<p>而模型的loss来源有3种：1）嵌入层的输出；2）来自Transformer层的隐藏状态和注意矩阵；3）预测层输出的logits。</p>
<p>具体来讲，其计算方法如下：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818230118223.png?token=AQ7WYGE4ASQ4WV73M3T5NU3DATBIM" alt=""></p>
<p>其中，teacher model有$N$层，student model有$M$层，$m=0$为embedding层，$m=M+1$为logits。</p>
<p>在计算时，我们需要使用attention和hidden states，两者的区别如下：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818231839284.png?token=AQ7WYGHUYUWUVLQIJDODGPLDATBIY" alt=""></p>
<p>$\mathcal{L_layer}$的定义如下：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818232212961.png?token=AQ7WYGAC5LNPEIDSREZMO23DATBJC" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818232117006.png?token=AQ7WYGHW6OLWRJUIGIVTHTTDATBJM" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818232130513.png?token=AQ7WYGHVIP5FG7POY5JUFCTDATBJ4" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818232150529.png?token=AQ7WYGBGGMI6ELWYDYV5MU3DATBKG" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220818232201396.png?token=AQ7WYGHBE6Z4A2JHQIKG7S3DATBKS" alt=""></p>
<h2 id="distill在asr领域的应用distilhubert">distill在ASR领域的应用——DistilHubert<a href="#distill在asr领域的应用distilhubert" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>distilhubert在计算loss时，并不是直接使用层的特征，而是使用student model第二层的输出映射过的特征与teacher model进行计算loss。</p>
<p>我们可以与tinybert做一个对比：distilhubert在训练时没有使用student各层的信息，而是使用末端output的信息；但tinybert的计算则是基于层的，而且分为attention metrics和hidden state。当然一个可能的考量是distilhubert层数较少而tinybert层数较多。</p>
<h2 id="distill与对比学习">distill与对比学习<a href="#distill与对比学习" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>参考自论文<em>From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression</em>。</p>
<p>大多数研究只注意在剪枝过程中对下游任务的特定知识，而忽略了在剪枝模型中是否很好地保持了原PLM的任务诊断性知识。这会导致严重的灾难问题；此外在极高的稀疏度下，修剪后的模型与原始密集模型相比性能急剧下降。</p>
<p>我们有三种修剪模块：PrC、SnC和FiC。</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819002709910.png?token=AQ7WYGC4UUGOQMZ65NTA2D3DATBLC" alt=""></p>
<p>其loss分为两部分：无监督和监督。其中监督数据的正例只需要其对应的标签相同。</p>
<ul>
<li><strong>PrC</strong> Contrastive Learning with Pre-trained Model。在向特定的下游任务转移学习时，原始PLM中与任务无关的知识倾向于丢失，这可能导致灾难性的遗忘问题。因此我们需要引入一个PrC模块来维护这种基于对比学习的通用语言知识。</li>
<li><strong>SnC</strong> 该模型使用了迭代修剪的算法，这些中间模型会被保存。当前的剪枝模型能够基于对比学习从这些快照中学习。</li>
<li><strong>FiC</strong> 在finetune model上的学习。</li>
</ul>
<p>文章提供了两种剪枝方式：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819003822771.png?token=AQ7WYGA7NXZIIVYY6XN7GYDDATBLM" alt=""></p>
<p>最后将loss相加即可：</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819003855452.png?token=AQ7WYGB4OGAOA22KG2L5DWTDATBL2" alt=""></p>
<h2 id="无监督少量数据的distill">无监督少量数据的distill<a href="#无监督少量数据的distill" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>参考自论文<em>From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression</em>。</p>
<p>该方法适用于只有少量的无标签数据。</p>
<p>(1)通过压缩教师网获得学生网；(2)在学生网的每个区块末尾增加一个1×1的卷积层，并通过最小二乘回归估计1×1卷积层的参数来对齐教师和学生；(3)将增加的1×1卷积层合并到先前的卷积层中，获得最终的学生网。</p>
<p><img src="https://raw.githubusercontent.com/Ther-nullptr/ImageRepository/main/img/image-20220819110039500.png?token=AQ7WYGGBEC7PDMCKXBX65KLDATBMC" alt=""></p>
<h2 id="reference">reference<a href="#reference" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>

      </div></div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2022 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://ther-nullptr.github.io/assets/main.js"></script>
<script src="https://ther-nullptr.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
