<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>fairseq functions :: Ther&#39;s Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="本文旨在列出fairseq中一些常见的自定义函数：
virtual functions task setup  setup_task  builders  build_model build_dictionary build_dictionary build_generator(usually useless)  loaders  load_dictionary load_datasets  steps   train_step
1 2 3 4 5 6 7 8 9 10 11  def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=False): model.train() model.set_num_updates(update_num) with torch.autograd.profiler.record_function(&amp;#34;forward&amp;#34;): with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))): loss, sample_size, logging_output = criterion(model, sample) if ignore_grad: loss *= 0 with torch.autograd.profiler.record_function(&amp;#34;backward&amp;#34;): optimizer.backward(loss) return loss, sample_size, logging_output     valid_step" />
<meta name="keywords" content="research" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://ther-nullptr.github.io/posts/research/fairseq-functions/" />




<link rel="stylesheet" href="https://ther-nullptr.github.io/assets/style.css">

  <link rel="stylesheet" href="assets/%25!s%28%3cnil%3e%29.css">






<link rel="apple-touch-icon" href="https://ther-nullptr.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://ther-nullptr.github.io/">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="fairseq functions">
<meta property="og:description" content="本文旨在列出fairseq中一些常见的自定义函数：
virtual functions task setup  setup_task  builders  build_model build_dictionary build_dictionary build_generator(usually useless)  loaders  load_dictionary load_datasets  steps   train_step
1 2 3 4 5 6 7 8 9 10 11  def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=False): model.train() model.set_num_updates(update_num) with torch.autograd.profiler.record_function(&amp;#34;forward&amp;#34;): with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))): loss, sample_size, logging_output = criterion(model, sample) if ignore_grad: loss *= 0 with torch.autograd.profiler.record_function(&amp;#34;backward&amp;#34;): optimizer.backward(loss) return loss, sample_size, logging_output     valid_step" />
<meta property="og:url" content="https://ther-nullptr.github.io/posts/research/fairseq-functions/" />
<meta property="og:site_name" content="Ther&#39;s Blog" />

  
    <meta property="og:image" content="https://ther-nullptr.github.io/">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

  <meta property="article:section" content="research" />


  <meta property="article:published_time" content="2022-07-18 00:13:05 &#43;0800 CST" />












</head>
<body class="">


<div class="container headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="https://ther-nullptr.github.io/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/posts/">Home</a></li>
        
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="/archives/">Archives</a></li>
        
      
        
          <li><a href="/search/">Search</a></li>
        
      
        
          <li><a href="/links/">Links</a></li>
        
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/posts/">Home</a></li>
      
    
      
        <li><a href="/about/">About</a></li>
      
    
      
        <li><a href="/archives/">Archives</a></li>
      
    
      
        <li><a href="/search/">Search</a></li>
      
    
      
        <li><a href="/links/">Links</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://ther-nullptr.github.io/posts/research/fairseq-functions/">fairseq functions</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2022-07-18
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://ther-nullptr.github.io/tags/research/">research</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>本文旨在列出fairseq中一些常见的自定义函数：</p>
<h2 id="virtual-functions">virtual functions<a href="#virtual-functions" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h3 id="task">task<a href="#task" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<h4 id="setup">setup<a href="#setup" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li><code>setup_task</code></li>
</ul>
<h4 id="builders">builders<a href="#builders" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li><code>build_model</code></li>
<li><code>build_dictionary</code></li>
<li><code>build_dictionary</code></li>
<li><code>build_generator</code>(usually useless)</li>
</ul>
<h4 id="loaders">loaders<a href="#loaders" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li><code>load_dictionary</code></li>
<li><code>load_datasets</code></li>
</ul>
<h4 id="steps">steps<a href="#steps" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li>
<p><code>train_step</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">update_num</span><span class="p">,</span> <span class="n">ignore_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">set_num_updates</span><span class="p">(</span><span class="n">update_num</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&#34;forward&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">AMPOptimizer</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">ignore_grad</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">*=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&#34;backward&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>valid_step</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">valid_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>optimizer_step</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">update_num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>inference_step</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">inference_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">prefix_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">models</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">prefix_tokens</span><span class="o">=</span><span class="n">prefix_tokens</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h4 id="hook-functions">hook functions<a href="#hook-functions" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<ul>
<li><code>begin_epoch</code></li>
<li><code>begin_validation_epoch</code></li>
</ul>
<h3 id="model">model<a href="#model" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<ul>
<li>
<p><code>build_model</code></p>
<p>除此之外，基本都需要自定义函数。</p>
</li>
<li>
<p><code>forward</code></p>
</li>
</ul>
<h3 id="criterion">criterion<a href="#criterion" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<ul>
<li>
<p><code>build_criterion</code></p>
<p>除此之外，基本都需要自定义函数。</p>
</li>
<li>
<p><code>forward</code></p>
</li>
</ul>
<h2 id="optimizers">optimizers<a href="#optimizers" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>fairseq中的optimizer是如何工作的？我们首先需要看一下pytorch API中默认的optimizer：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于模型参数中不同的部分，我们也可以使用不同的优化策略：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
</span></span><span class="line"><span class="cl">                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们再看看fairseq中的optimizer：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@register_optimizer</span><span class="p">(</span><span class="s2">&#34;adam&#34;</span><span class="p">,</span> <span class="n">dataclass</span><span class="o">=</span><span class="n">FairseqAdamConfig</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">FairseqAdam</span><span class="p">(</span><span class="n">FairseqOptimizer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">FairseqAdamConfig</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">fused_adam_cls</span> <span class="o">=</span> <span class="n">get_fused_adam_class</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_fused_adam</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&#34;use_old_adam&#34;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">fused_adam_cls</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&#34;tpu&#34;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">fp16_adam_stats</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&#34;--fp16-adam-stats is only supported on GPU&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># on TPUs we use the Adam defined here, since it</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># automatically casts gradients to FP32</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">use_fused_adam</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&#34;using FusedAdam&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">fused_adam_cls</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">params</span><span class="p">,</span> <span class="n">use_fp16_stats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">fp16_adam_stats</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_config</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">fp16_adam_stats</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;--fp16-adam-stats is only supported with FusedAdamV1&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>fairseq的optimizer在传入参数时需要将model的params进行传入。那么这个传入的过程在哪里呢？在train_step中，需要传入所有的主要模块：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">update_num</span><span class="p">,</span> <span class="n">ignore_grad</span><span class="o">=</span><span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">set_num_updates</span><span class="p">(</span><span class="n">update_num</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&#34;forward&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">AMPOptimizer</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">ignore_grad</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">*=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&#34;backward&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">logging_output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>值得注意的是，在fairseq中，反向传播的过程是这么写的（见<code>fairseq_task.py</code>）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">optimizer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其内部原理如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="s2">&#34;&#34;&#34;Computes the sum of gradients of the given tensor w.r.t. graph leaves.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>那么何时执行<code>optimizer.step()</code>操作呢？在<code>fairseq_task.py</code>中：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">update_num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后在<code>trainer.py</code>中调用这个函数：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">record_function</span><span class="p">(</span><span class="s2">&#34;optimizer&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="c1"># take an optimization step</span>
</span></span><span class="line"><span class="cl">     <span class="bp">self</span><span class="o">.</span><span class="n">task</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">           <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">update_num</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_num_updates</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">     <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>而在常规的pytorch程序中，反向传播过程是这么写的，可见有很大的不同：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div>
      </div></div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2022 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://ther-nullptr.github.io/assets/main.js"></script>
<script src="https://ther-nullptr.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
